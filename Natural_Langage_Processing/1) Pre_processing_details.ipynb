{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE - PROCESSING TUTORIAL (beginner friendly) \n",
    "based on the courses of CodeCademy\n",
    "\n",
    "### This tutorial will help to becomme familiar with NLP by explaining what are the principals things. I added many comments, explanations and examples to make things easier for your understanding. \n",
    "\n",
    "Pre-processing is:\n",
    "- Cleaning and preparing text data for use in a specific context (ultimate goal is to reduce the text to only the words that you need for your NLP goals)\n",
    "- Removing noise from data.\n",
    "\n",
    "\n",
    "While this list is not exhaustive, we will cover a few common approaches for cleaning and processing text data.   \n",
    "\n",
    "They include:\n",
    "\n",
    "    . Using Regex & NLTK libraries\n",
    "    . Removing unnecessary characters and formatting\n",
    "    . Tokenization – break multi-word strings into smaller components\n",
    "    . Normalization – a catch-all term for processing data; this includes stemming\n",
    "      and lemmatization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MENU:\n",
    "\n",
    "1) Noise Removal\n",
    "\n",
    "2) Tokenization\n",
    "\n",
    "3) Normalization\n",
    "\n",
    "4) Stemming\n",
    "\n",
    "5) Lemmatization\n",
    "* lemmatization 5.1\n",
    "* lemmatization 5.2\n",
    "      \n",
    "6) Review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Noise Removal  \n",
    "[More details abour Regular Expressions (re)](https://github.com/santinon/big_data/blob/master/Natural_Langage_Processing/Regular%20Expressions%20(RE)%2C%20regex%20or%20regexp.ipynb)\n",
    "\n",
    ".sub() method in Python’s regular expression (re) library for most of your noise removal needs\n",
    "\n",
    "\n",
    "The .sub() method has three required arguments:\n",
    "- *pattern*:  a regular expression that is searched for in the input string. There must be an *r* preceding the string to indicate it is a raw string, which treats backslashes as literal characters.\n",
    "- *replacement_text* : text that replaces all matches in the input string\n",
    "- *input* : the input string that will be edited by the *.sub()* method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    This is a paragraph\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "text = \"<p>    This is a paragraph</p>\"\n",
    "# removes the HTML p signs (replaces the tags with an empty string '')\n",
    "result = re.sub(r'<.?p>', '', text)  # \".?p\" = whatever there is , if there is something (?), before the p\n",
    "\n",
    "print(result) # see the spaces in-front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a paragraph\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "text = \"    This is a paragraph\" # The whitespace consists of four spaces.\n",
    "# removes the spaces (replaces each by an empty string '')\n",
    "result = re.sub(r'\\s{4}', '', text)\n",
    "\n",
    "print(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nation's Top Pseudoscientists Harness High-Energy Quartz Crystal Capable Of Reversing Effects Of Being Gemini \n",
      "\n",
      "@fat_meats, veggies are better than you think.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "headline_one = '<h1>Nation\\'s Top Pseudoscientists Harness High-Energy Quartz Crystal Capable Of Reversing Effects Of Being Gemini</h1>'\n",
    "\n",
    "tweet = '@fat_meats, veggies are better than you think.'\n",
    "\n",
    "headline_no_tag = re.sub(r'<.?h1>', '', headline_one) # \".?h1\" = whatever (.)there is , if there is something (?), before the h1\n",
    "tweet_no_at = re.sub(r'@', '', tweet)\n",
    "\n",
    "print(headline_no_tag, \"\\n\") # \"\\n\" = empty-line\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Tokenization\n",
    "\n",
    "The method for breaking text into smaller components is called tokenization and the individual components are called tokens   \n",
    "= separates text words-by-words (tokens)\n",
    "\n",
    "A few common operations that require tokenization include:\n",
    "\n",
    "    - Finding how many words or sentences appear in text\n",
    "    - Determining how many times a specific word or phrase exists\n",
    "    - Accounting for which terms are likely to co-occur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenize', 'this', 'text']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# words-tokenization\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Tokenize this text\"\n",
    "tokenized = word_tokenize(text)\n",
    "\n",
    "print(tokenized)\n",
    "print(type(tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenize this sentence.', 'Also, tokenize this sentence.']\n"
     ]
    }
   ],
   "source": [
    "# sentences-tokenization\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Tokenize this sentence. Also, tokenize this sentence.\"\n",
    "tokenized = sent_tokenize(text)\n",
    "\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['An', 'electrocardiogram', 'is', 'used', 'to', 'record', 'the', 'electrical', 'conduction', 'through', 'a', 'person', \"'s\", 'heart', '.', 'The', 'readings', 'can', 'be', 'used', 'to', 'diagnose', 'cardiac', 'arrhythmias', '.'] \n",
      "\n",
      "[\"An electrocardiogram is used to record the electrical conduction through a person's heart.\", 'The readings can be used to diagnose cardiac arrhythmias.']\n"
     ]
    }
   ],
   "source": [
    "# both tokenization occur here part below\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "ecg_text = 'An electrocardiogram is used to record the electrical conduction through a person\\'s heart. \\\n",
    "            The readings can be used to diagnose cardiac arrhythmias.'\n",
    "\n",
    "tokenized_by_word = word_tokenize(ecg_text)\n",
    "tokenized_by_sentence = sent_tokenize(ecg_text)\n",
    "\n",
    "print(tokenized_by_word, \"\\n\") # \"\\n\" = empty-line\n",
    "print(tokenized_by_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Normalization\n",
    "\n",
    "Text normalization is a catch-all term for various text pre-processing tasks. In the next few exercises, we’ll cover a few of them:\n",
    "\n",
    "* Upper or lowercasing\n",
    "\n",
    "\n",
    "* _Stopword_ removal: \n",
    "   * They include words such as “a”, “an”, and “the”. NLTK provides a built-in library with these words\n",
    "\n",
    "\n",
    "* **Stemming**: bluntly removing prefixes and suffixes from a word\n",
    "\n",
    "\n",
    "* **Lemmatization**: replacing a single-word token with its root\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THIS HAS A MIX OF CASES OH SHIT\n",
      "this has a mix of cases oh shit \n",
      "\n",
      "salvation ira, ymca, boys & girls club of ireland, feuk. brexit\n",
      "SALVATION IRA, YMCA, BOYS & GIRLS CLUB OF IRELAND, FEUK. BREXIT\n"
     ]
    }
   ],
   "source": [
    "# here,  Python’s built-in String methods to make a string all uppercase or lowercase\n",
    "\n",
    "my_string = 'tHiS HaS a MiX oF cAsEs OH shiT'\n",
    "\n",
    "print(my_string.upper())\n",
    "\n",
    "print(my_string.lower(), \"\\n\")\n",
    "\n",
    "\n",
    "#other example : \n",
    "brands = 'Salvation IRA, YMCA, Boys & Girls Club of Ireland, feuk. Brexit'\n",
    "\n",
    "brands_lower = brands.lower()\n",
    "print(brands_lower)\n",
    "brands_upper = brands.upper()\n",
    "print(brands_upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens with stopwords: \n",
      " ['NBC', 'was', 'founded', 'in', '1926', 'making', 'it', 'the', 'oldest', 'major', 'broadcast', 'network', 'in', 'the', 'USA']\n",
      "\n",
      " tokens without stopwords: \n",
      " ['NBC', 'founded', '1926', 'making', 'oldest', 'major', 'broadcast', 'network', 'USA']\n",
      "\n",
      "\n",
      " 2nd example: \n",
      "\n",
      " tokens with stopwords: \n",
      " ['A', 'YouGov', 'study', 'found', 'that', 'American', \"'s\", 'like', 'Italian', 'food', 'more', 'than', 'any', 'other', 'country', \"'s\", 'cuisine', '.']\n",
      "\n",
      " tokens without stopwords: \n",
      " ['A', 'YouGov', 'study', 'found', 'American', \"'s\", 'like', 'Italian', 'food', 'country', \"'s\", 'cuisine', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stopword removal\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english')) # creates a set (the type) of stopwords\n",
    "\n",
    "sentence = \"NBC was founded in 1926 making it the oldest major broadcast network in the USA\"\n",
    "\n",
    "# tokenize nbc_statement\n",
    "word_tokens = word_tokenize(sentence) \n",
    "\n",
    "# list comprehension to remove them from a sentence\n",
    "statement_no_stop = [word for word in word_tokens if word not in stop_words]\n",
    "\n",
    "# check here\n",
    "print(\"tokens with stopwords: \\n\", word_tokens)\n",
    "print(\"\\n tokens without stopwords: \\n\",statement_no_stop)\n",
    "\n",
    "\n",
    "print(\"\\n\\n 2nd example: \")\n",
    "# Other example:\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "survey_text = 'A YouGov study found that American\\'s like Italian food more \\\n",
    "               than any other country\\'s cuisine.'\n",
    "\n",
    "tokenized_survey = word_tokenize(survey_text)\n",
    "\n",
    "text_no_stops = [word for word in tokenized_survey if word not in stop_words]\n",
    "\n",
    "print(\"\\n tokens with stopwords: \\n\", tokenized_survey)\n",
    "print(\"\\n tokens without stopwords: \\n\", text_no_stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is the text preprocessing normalization task concerned with bluntly **removing word affixes (prefixes and suffixes)**. \n",
    "> ex: stemming would cast the word “going” to “go”.   \n",
    "(this is a common method used by search engines to improve matching between user input and website hits).\n",
    "\n",
    "NLTK has a built-in stemmer called _PorterStemmer_. You can use it with a _list comprehension_ to stem each word in a tokenized list of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nbc', 'wa', 'found', 'in', '1926', '.', 'thi', 'make', 'nbc', 'the', 'oldest', 'major', 'broadcast', 'network', '.']\n"
     ]
    }
   ],
   "source": [
    "# import and initialize the stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# apply stemmer to each word in a list using a list comprehension\n",
    "tokenized = ['NBC', 'was', 'founded', 'in', '1926', '.', 'This', 'makes', 'NBC', 'the',\n",
    "             'oldest', 'major', 'broadcast', 'network', '.']\n",
    "\n",
    "stemmed = [stemmer.stem(token) for token in tokenized]\n",
    "\n",
    "# check\n",
    "print(stemmed)\n",
    "\n",
    "# Notice, the words like ‘was’ and ‘founded’ became ‘wa’ and ‘found’, respectively.  \n",
    "# We you need to be careful when stemming strings.  \n",
    "# Words can often be converted to something unrecognizable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tokenized sentence: \n",
      " ['Java', 'is', 'an', 'Indonesian', 'island', 'in', 'the', 'Pacific', 'Ocean', '.', 'It', 'is', 'the', 'most', 'populated', 'island', 'in', 'the', 'world', ',', 'with', 'over', '140', 'million', 'people', '.'] \n",
      "\n",
      "\"stemmed sentence\" : \n",
      " ['java', 'is', 'an', 'indonesian', 'island', 'in', 'the', 'pacif', 'ocean', '.', 'It', 'is', 'the', 'most', 'popul', 'island', 'in', 'the', 'world', ',', 'with', 'over', '140', 'million', 'peopl', '.']\n"
     ]
    }
   ],
   "source": [
    "#  import and initialize the stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# import tokenization thing\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize  \n",
    "\n",
    "populated_island = 'Java is an Indonesian island in the Pacific Ocean. It is the most populated \\\n",
    "                    island in the world, with over 140 million people.'\n",
    "\n",
    "# \"populated_island\" tokenization first\n",
    "island_tokenized = word_tokenize(populated_island)\n",
    "print(\" tokenized sentence: \\n\", island_tokenized, \"\\n\")\n",
    "\n",
    "# stemming use list-comprehension \n",
    "stemmed = [stemmer.stem(token) for token in island_tokenized]\n",
    "print('\"stemmed sentence\" : \\n', stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is a method for casting words to their **root forms**.   \n",
    "(this is a more involved process than stemming, because it requires the method to know the _part-of-speech_ for _each word_).   \n",
    "\n",
    "Since lemmatization requires the _part-of-speech_, it is a less efficient approach than stemming.\n",
    "\n",
    "###### To take advantage of the power of lemmatization, we need to tag each word in our text with the most likely _part of speech_.   We’ll do that in the next part lemmatization 5.2\n",
    "\n",
    "#### lemmatization 5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NBC', 'wa', 'founded', 'in', '1926']\n"
     ]
    }
   ],
   "source": [
    "# import NLTK’s WordNetLemmatizer to lemmatize text\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() # initializes instance of \"WordNetLemmatizer\"\n",
    "\n",
    "# a list comprehension to apply the lemmatize operation to each word in a list\n",
    "tokenized = [\"NBC\", \"was\", \"founded\", \"in\", \"1926\"]\n",
    "\n",
    "lemmatized = [lemmatizer.lemmatize(token) for token in tokenized]\n",
    "\n",
    "# check it\n",
    "print(lemmatized)\n",
    "\n",
    "# The result, saved to lemmatized contains 'wa', while the rest of the words remain the same. Not too useful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tokenized sentence: \n",
      " ['Indonesia', 'was', 'founded', 'in', '1945', '.', 'It', 'contains', 'the', 'most', 'populated', 'island', 'in', 'the', 'world', ',', 'Java', ',', 'with', 'over', '140', 'million', 'people', '.'] \n",
      "\n",
      "lemmatized sentence: \n",
      " ['Indonesia', 'wa', 'founded', 'in', '1945', '.', 'It', 'contains', 'the', 'most', 'populated', 'island', 'in', 'the', 'world', ',', 'Java', ',', 'with', 'over', '140', 'million', 'people', '.']\n"
     ]
    }
   ],
   "source": [
    "# import tokenization library\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# import lemmatization library\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() # initializes instance of \"WordNetLemmatizer\"\n",
    "\n",
    "# text\n",
    "populated_island = 'Indonesia was founded in 1945. It contains the most populated island in the world, Java, with over 140 million people.'\n",
    "\n",
    "# tokenization\n",
    "tokenized_string = word_tokenize(populated_island)\n",
    "print(\" tokenized sentence: \\n\", tokenized_string, \"\\n\")\n",
    "\n",
    "# lemmatization\n",
    "lemmatized_words = [lemmatizer.lemmatize(token) for token in tokenized_string]\n",
    "\n",
    "# check\n",
    "print(\"lemmatized sentence: \\n\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lemmatization 5.2\n",
    "#### _Part-of-Speech_ Tagging\n",
    "To improve lemmatization performance, we need to find the part of speech for each word in our string.\n",
    "\n",
    ">ex: for the 8 major parts of speech in English grammar:  noun, pronoun, verb, adverb, adjective, conjunction, preposition, and interjection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detailed code : print the prints to understand the different steps (4 different)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatization POS based: \n",
      " ['Indonesia', 'be', 'found', 'in', '1945', '.', 'It', 'contain', 'the', 'most', 'populate', 'island', 'in', 'the', 'world', ',', 'Java', ',', 'with', 'over', '140', 'million', 'people', '.']\n"
     ]
    }
   ],
   "source": [
    "# 1/ Import libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Import wordnet and Counter\n",
    "# wordnet:  database use for contextualizing words\n",
    "# Counter: container that stores elements as dictionary keys\n",
    "from nltk.corpus import wordnet # To get words in dictionary with their parts of speech\n",
    "from collections import Counter #lemmatizes word based on it's parts of speech\n",
    "\n",
    "\n",
    "# 2/ Get synonyms (via a function here)\n",
    "# function to get the synonyms and then the POS for each word (print the prints to understand)\n",
    "def get_part_of_speech(word):\n",
    "    probable_part_of_speech = wordnet.synsets(word) # wordnet.synsets(): function to get a set of synonyms for the word = set with all synonyms\n",
    "    #print(\"- probable_part_of_speech: \",word, \"\\n\", probable_part_of_speech, \"   type:  \", type(probable_part_of_speech))\n",
    "\n",
    "    pos_counts = Counter() # container: stores elements as dictionary keys\n",
    "    #print(\"- pos_counts BEFORE: \\n\", pos_counts, \"   type:  \", type(pos_counts))\n",
    "\n",
    "# 3/ Use synonyms to determine the most likely part of speech  \n",
    "    # nouns\n",
    "    pos_counts[\"n\"] = len( [ word for word in probable_part_of_speech if word.pos()==\"n\"] ) \n",
    "    # verbs\n",
    "    pos_counts[\"v\"] = len( [ word for word in probable_part_of_speech if word.pos()==\"v\"] )\n",
    "     # adjectives\n",
    "    pos_counts[\"a\"] = len( [ word for word in probable_part_of_speech if word.pos()==\"a\"] )\n",
    "    # adverb (n)\n",
    "    pos_counts[\"r\"] = len( [ word for word in probable_part_of_speech if word.pos()==\"r\"] )\n",
    "  #    NOTES PERSO:\n",
    "  # -ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n",
    "  # - item.pos(): is the position of the \"n\" or \"v\" or \"a\" or \"r\".\n",
    "  #   checks if one of these letters is in this position in \"probable_part_of_speech\"\n",
    "  # - if yes, takes the word (synonym) by doing: \"word for word\" command in puts it in a list:\n",
    "  #  \"[ word for word in probable_part_of_speech if word.pos()==\"n\"]\"\n",
    "  # - then, takes length (len) of list = number of times the type of POS appears in the list \n",
    "  # - pos_counts[\"type of POS\"]: adds len as value in dictionnary (key-value) = ( POS: count )\n",
    "    #print(\"- pos_counts AFTER: \\n\", pos_counts, \"   type:  \", type(pos_counts))\n",
    "\n",
    "# 4/ Returns the most common part of speech   \n",
    "    # most_common(n): Returns n most common elements\n",
    "    most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "  # [0]: first indexer for getting the top POS from list, \n",
    "  # [0]: second indexer for getting POS from tuple( POS: count )\n",
    "    #print(\"- most_likely_part_of_speech: \", most_likely_part_of_speech, \"   type:  \", type(most_likely_part_of_speech), \"\\n\")\n",
    "    return most_likely_part_of_speech # returned synonyms come with their part of speech\n",
    "\n",
    "\n",
    "# sentence we want to \"stemmize\"\n",
    "populated_island = 'Indonesia was founded in 1945. It contains the most populated island in the world, Java, with over 140 million people.'\n",
    "\n",
    "# tokenisation of the sentence\n",
    "tokenized_string = word_tokenize(populated_island)\n",
    "\n",
    "# lematization process\n",
    "lemmatizer = WordNetLemmatizer() # initializes instance of \"WordNetLemmatizer\"\n",
    "lemmatized_pos = [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized_string] # lemmatization POS based\n",
    "\n",
    "# check it\n",
    "print(\"lemmatization POS based: \\n\", lemmatized_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Same code as above but more 'professional' with less comments here* in a single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the synonyms and then the POS for each word (print the prints to understand)\n",
    "def get_part_of_speech_via_synonyms(word):\n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize \n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    # Import wordnet and Counter\n",
    "    # wordnet:  database use for contextualizing words\n",
    "    # Counter: container that stores elements as dictionary keys\n",
    "    from nltk.corpus import wordnet # To get words in dictionary with their parts of speech\n",
    "    from collections import Counter #lemmatizes word based on it's parts of speech\n",
    "    \n",
    "    # tokenisation of the sentence\n",
    "    tokenized_string = word_tokenize(sentence_to_tokegnize)\n",
    "    \n",
    "    probable_part_of_speech = wordnet.synsets(word) # gets the synonyms \n",
    "    pos_counts = Counter() # container: stores elements as dictionary keys\n",
    "    # Use synonyms to determine the most likely part of speech  \n",
    "    # nouns\n",
    "    pos_counts[\"n\"] = len( [ word for word in probable_part_of_speech if word.pos()==\"n\"] ) \n",
    "    # verbs\n",
    "    pos_counts[\"v\"] = len( [ word for word in probable_part_of_speech if word.pos()==\"v\"] )\n",
    "     # adjectives\n",
    "    pos_counts[\"a\"] = len( [ word for word in probable_part_of_speech if word.pos()==\"a\"] )\n",
    "    # adverb\n",
    "    pos_counts[\"r\"] = len( [ word for word in probable_part_of_speech if word.pos()==\"r\"] )\n",
    "    # Returns n most common elements\n",
    "    most_likely_part_of_speech = pos_counts.most_common(1)[0][0] # # most_common(n): Returns n most common elements\n",
    "    return most_likely_part_of_speech # returned synonyms come with their part of speech\n",
    "\n",
    "    # lematization process\n",
    "    lemmatizer = WordNetLemmatizer() # initializes instance of \"WordNetLemmatizer\"\n",
    "    lemmatized_pos = [lemmatizer.lemmatize(token, most_likely_part_of_speech(token)) for token in tokenized_string] # lemmatization POS based\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as above, but splitted in 2 functions ready-to-use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "normalizer = WordNetLemmatizer()\n",
    "\n",
    "def get_part_of_speech(word): # ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n",
    "  probable_part_of_speech = wordnet.synsets(word)\n",
    "  pos_counts = Counter()\n",
    "  pos_counts[\"n\"] = len([item for item in probable_part_of_speech if item.pos()==\"n\"])\n",
    "  pos_counts[\"v\"] = len([item for item in probable_part_of_speech if item.pos()==\"v\"])\n",
    "  pos_counts[\"a\"] = len([item for item in probable_part_of_speech if item.pos()==\"a\"])\n",
    "  pos_counts[\"r\"] = len([item for item in probable_part_of_speech if item.pos()==\"r\"]) \n",
    "  most_likely_part_of_speech = pos_counts.most_common(1)[0][0] # (1): selects the highest tuple \n",
    "# inside a list. (1)[0]: extract that tuple from the list. (1)[0][0] : return key of highets values\n",
    "  return most_likely_part_of_speech\n",
    "\n",
    "def preprocess_text(text): # # \"w+\" Open a text file for update (reading and writing), \n",
    "# first truncating the file to zero length if it exists or creating the file if it does not exist\n",
    "  cleaned = re.sub(r'\\W+', ' ', text).lower() \n",
    "  tokenized = word_tokenize(cleaned)\n",
    "  normalized = [normalizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized]\n",
    "  return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for us it is good to know the difference:\n",
    "\n",
    ". lemmatized sentence:   \n",
    " ['Indonesia', 'wa', 'founded', 'in', '1945', '.', 'It', 'contains', 'the', 'most', 'populated', 'island', 'in', 'the', 'world', ',', 'Java', ',', 'with', 'over', '140', 'million', 'people', '.']\n",
    "\n",
    ". lemmatization POS based:   \n",
    " ['Indonesia', 'be', 'found', 'in', '1945', '.', 'It', 'contain', 'the', 'most', 'populate', 'island', 'in', 'the', 'world', ',', 'Java', ',', 'with', 'over', '140', 'million', 'people', '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6/ REVIEW\n",
    "Let’s review what we covered in this lesson:\n",
    "\n",
    "   - Text preprocessing is all about cleaning and prepping text data so that it’s ready for other NLP tasks.\n",
    "   \n",
    "   \n",
    "   - Noise removal is a text preprocessing step concerned with removing unnecessary formatting from our text.\n",
    "   \n",
    "   \n",
    "   - Tokenization is a text preprocessing step devoted to breaking up text into smaller units (usually words or discrete terms).\n",
    "   \n",
    "\n",
    "   - Normalization is the name we give most other text preprocessing tasks, including stemming, lemmatization, upper and lowercasing, and stopword removal.\n",
    "   \n",
    "   \n",
    "   - Stemming is the normalization preprocessing task focused on removing word affixes.\n",
    "   \n",
    "   \n",
    "   - Lemmatization is the normalization preprocessing task that more carefully brings words down to their root forms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
