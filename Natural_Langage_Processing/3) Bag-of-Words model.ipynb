{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-words (BoW)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-of-words (BoW) is a statistical language model based on **word count** ( = how many times each word appears in a document)  \n",
    "Bag-of-words has high *perplexity*, meaning that it’s *not* a very accurate model for language prediction.  \n",
    "BoW suffers from *overfitting* when it comes to vocabulary (if in the corpus, with which it was trained, there is not  a certain word)\n",
    "\n",
    "* **Statistical language model** :   \n",
    "is a way for computers to make sense of language based on probability. \n",
    "  *     **unigram model**: n-gram model, with n = 1 (the number of words in a sequence)\n",
    "  * **bigram model**  \n",
    "  * and so on\n",
    "\n",
    "* **Feature extraction** or **Vectorization**:  \n",
    "for a word present in a document; we want to know how many times it occurred\n",
    "\n",
    "\n",
    "> ex:  \n",
    "-*Features Dictionnary*:  \n",
    "{'**five**': 0,  \n",
    "'fantastic': 1,  \n",
    "'**fish**': 2,  \n",
    "'fly': 3,  \n",
    "'off': 4,  \n",
    "'to': 5,  \n",
    "'**find**': 6,  \n",
    "'**faraway**': 7,  \n",
    "'function': 8,  \n",
    "'maybe': 9,  \n",
    "'**another**': 10}  \n",
    "-For the *string* (sentence) :  \"Another five fish find another faraway fish.\"  \n",
    "-We get *BoW vector* (in form of a list):    [1, 0, 2, 0, 0, 0, 1, 1, 0, 0, 2]\n",
    "\n",
    "\n",
    "\n",
    "![Schema of overview](/home/gluecksman/git_hub/big_data/Natural_Langage_Processing/Capture du 2020-02-25 18-17-12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global overview : \n",
    "\n",
    "\n",
    "* Bag-of-words (BoW) = unigram model = statistical language model based on word count (that have a lot of real-world applications for BoW).\n",
    "\n",
    "\n",
    "* BoW can be implemented as a Python **features dictionary** with each key set to a word and each value set to the number of times that word appears in a text (corpus used coming from a training set).\n",
    "* For BoW, training data is the text that is used to build a BoW model.\n",
    "* *BoW test data* is the new text that is converted to a *BoW vector* using a trained **features dictionary**.\n",
    "* A feature vector is a numeric depiction of an item’s salient features.\n",
    "* Feature extraction (or vectorization) is the process of turning text into a BoW vector.\n",
    "* A features dictionary is a mapping of each unique word in the training data to a unique index. This is used to build out BoW vectors.\n",
    "\n",
    "\n",
    "* Because bag-of-words relies on single words, rather than sequences of words, there are more examples of each unit of language in the training corpus. More examples means the model has less data sparsity.   \n",
    "  -> BoW has less data sparsity than other statistical models.   \n",
    "  -> It also suffers less from overfitting.\n",
    "\n",
    "\n",
    "* BoW has higher perplexity than other models,    \n",
    "  making it less ideal for language prediction.\n",
    "\n",
    "\n",
    "* One solution to overfitting is language smoothing, in which a bit of probability is taken from known words and allotted to unknown words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1/ Feature extraction or Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-a / Detailed **step-by-step** code  \n",
    "Creates word vector (**features dictionnary** then, **word-vector creation**) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Features Dictionary:\n",
      " {'five': 0, 'fantastic': 1, 'fish': 2, 'fly': 3, 'off': 4, 'to': 5, 'find': 6, 'faraway': 7, 'function': 8, 'maybe': 9, 'another': 10, 'my': 11, 'with': 12, 'a': 13, 'please': 14}\n",
      "\n",
      " - BoW Vector:\n",
      " [1, 0, 2, 0, 0, 0, 1, 1, 0, 0, 2, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#######################################################################################\n",
    "import nltk, re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "normalizer = WordNetLemmatizer()\n",
    "\n",
    "def get_part_of_speech(word): # ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n",
    "    probable_part_of_speech = wordnet.synsets(word)\n",
    "    pos_counts = Counter()\n",
    "    pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "    pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "    pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "    pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "    most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "    return most_likely_part_of_speech\n",
    "\n",
    "def preprocess_text(text):\n",
    "    cleaned = re.sub(r'\\W+', ' ', text).lower()\n",
    "    tokenized = word_tokenize(cleaned)\n",
    "    normalized = [normalizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized]\n",
    "    return normalized\n",
    "\n",
    "# ABOVE : is for the pre-processing part to tokenize + lemmatization \n",
    "#######################################################################################\n",
    "# the code to understand is BELOW\n",
    "\n",
    "# every unique word that is trained with a corpus, will be inside it\n",
    "def create_features_dictionary(documents):   # ---> Features dictionnary\n",
    "    features_dictionary = {}\n",
    "    merged = \" \".join(documents)\n",
    "    tokens = preprocess_text(merged) # every token is under its lemmatized form\n",
    "    index = 0\n",
    "    for token in tokens:\n",
    "        if token not in features_dictionary:\n",
    "            features_dictionary[token] = index\n",
    "            index +=1\n",
    "    return features_dictionary, tokens\n",
    "\n",
    "training_documents = [\"Five fantastic fish flew off to find faraway functions.\", \n",
    "\"Maybe find another five fantastic fish?\", \"Find my fish with a function please!\"]\n",
    "# tuple so we need to take back the first element of this thing here, that is a dict\n",
    "features_dictionary = create_features_dictionary(training_documents)[0]\n",
    "print(\"- Features Dictionary:\\n\", features_dictionary) \n",
    "\n",
    "\n",
    "def text_to_bow_vector(some_text, features_dictionary):   # ---> Vectorization\n",
    "    bow_vector = [0] * len(features_dictionary)\n",
    "    tokens = preprocess_text(some_text)\n",
    "    for token in tokens:\n",
    "        feature_index = features_dictionary[token]\n",
    "        bow_vector[feature_index] += 1\n",
    "    return bow_vector, tokens\n",
    "\n",
    "\n",
    "text = \"Another five fish find another faraway fish.\"\n",
    "print(\"\\n - BoW Vector:\\n\", text_to_bow_vector(text, features_dictionary)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-b / Built-in-library **ScikitLearn** (with specific implementation) code  \n",
    "Creates a long word **features-dictionnary** based on words from a corpus (training set) then,   \n",
    "a **word-vector** (based on features-dictionnary) as a test for the text provided (test set)\n",
    "\n",
    "\n",
    "(both are not exactly vectors because of strange Scikitlearn transformations but are used like that)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -training_vectors 'features-dictionnary': \n",
      " [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]] \n",
      "\n",
      "(#sentence corpus x # words corpus)\n",
      "\n",
      "\n",
      " -test_vectors 'word-vector': \n",
      " [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]] \n",
      "\n",
      "(3 x # words corpus)\n"
     ]
    }
   ],
   "source": [
    "##############################################################################################\n",
    "# preprocessing steps for training and test sets (scikit needs it otherwise not working)\n",
    "# we need just to create a list of sentences (to \"feed\" the scikit learn algorithm)\n",
    "\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "\n",
    "# training set ---\n",
    "texte = open(\"the_iliad.txt\",encoding='utf-8').read().lower() # is a string, for the moment\n",
    "texte = re.sub('[\\@#$\\n\"\"'';,:]', '', texte) # deletes all signs but keeps the points\n",
    "texte = re.sub('[0-9]', '', texte) # deletes all numbers\n",
    "# \"PunktSentenceTokenizer\" = divides text into list of sentences (use unsupervised algorithm)\n",
    "sentence_tokenizer = PunktSentenceTokenizer(texte)\n",
    "sentence_tokenized = sentence_tokenizer.tokenize(texte) # becomes a list \n",
    "training_docs = sentence_tokenized\n",
    "\n",
    "# test set ---\n",
    "test_text = [\"We will see if it works with three sentences.\", \n",
    "            \"Will it work with the words inside the corpus?\", \n",
    "            \"If not more corpus are usefull to train the model bolow!\"]\n",
    "# we need just to create a list of sentences (to \"feed\" the algorithm)\n",
    "texte = \" \".join(test_docs).lower() # is a string\n",
    "texte = re.sub('[\\@#$\\n\"\"'';,:]', '', texte) # deletes all signs but keeps the points\n",
    "texte = re.sub('[0-9]', '', texte) # deletes all numbers\n",
    "# \"PunktSentenceTokenizer\" = divides text into list of sentences (use unsupervised algorithm)\n",
    "sentence_tokenizer = PunktSentenceTokenizer(texte)\n",
    "sentence_tokenized = sentence_tokenizer.tokenize(texte) # becomes a list\n",
    "test_docs = sentence_tokenized\n",
    "\n",
    "\n",
    "##############################################################################################\n",
    "# this part is creating a dictionnary (based on the corpus 'Iliad') for the training vector\n",
    "# and a Bow vector, for the test_vectors, that is based on the test_text above (3 sentences)\n",
    "\n",
    "\n",
    "# Import CountVectorizer from sklearn:\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# Define bow_vectorizer \"bow_vector\":\n",
    "bow_vectorizer = CountVectorizer()\n",
    "\n",
    "\n",
    "# Define training_vectors:  # if corpus long => training_vectors also\n",
    "training_vectors = bow_vectorizer.fit_transform(training_docs)\n",
    "# with \"training_docs\" = list containing long text\n",
    "# \"fit_transform()\": features-dictionary + vectorization (array) of training data (corpus)\n",
    "#print(bow_vectorizer.get_feature_names()) # check\n",
    "print(\" -training_vectors 'features-dictionnary': \\n\",\n",
    "      training_vectors.toarray(),\n",
    "\"\\n\\n(#sentence corpus x # words corpus)\")   # check the array\n",
    "\n",
    "\n",
    "# Define test_vectors: \n",
    "# will be a looong array because is based on a long features-dictionnary (from a long corpus)\n",
    "test_vectors = bow_vectorizer.transform(test_docs) # because of the corpus\n",
    "print(\"\\n\\n -test_vectors 'word-vector': \\n\",\n",
    "      test_vectors.toarray(),\n",
    "     \"\\n\\n(3 x # words corpus)\")             # check final array (\"test_set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) BoW application to see most frequent used words in a text\n",
    "**Comparaison between Bigramms and Bow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIGRAMS:\n",
      "- Word sequences and the number of occurrences according to Bigrams: Counter({('it', 's'): 1, ('s', 'excite'): 1, ('excite', 'to'): 1, ('to', 'watch'): 1, ('watch', 'fly'): 1, ('fly', 'fish'): 1, ('fish', 'after'): 1, ('after', 'a'): 1, ('a', 'hard'): 1, ('hard', 'day'): 1, ('day', 's'): 1, ('s', 'work'): 1, ('work', 'i'): 1, ('i', 'don'): 1, ('don', 't'): 1, ('t', 'know'): 1, ('know', 'why'): 1, ('why', 'some'): 1, ('some', 'fish'): 1, ('fish', 'prefer'): 1, ('prefer', 'fly'): 1, ('fly', 'and'): 1, ('and', 'other'): 1, ('other', 'fish'): 1, ('fish', 'would'): 1, ('would', 'rather'): 1, ('rather', 'swim'): 1, ('swim', 'it'): 1, ('it', 'seem'): 1, ('seem', 'like'): 1, ('like', 'the'): 1, ('the', 'fish'): 1, ('fish', 'just'): 1, ('just', 'wake'): 1, ('wake', 'up'): 1, ('up', 'one'): 1, ('one', 'day'): 1, ('day', 'and'): 1, ('and', 'decide'): 1, ('decide', 'hey'): 1, ('hey', 'today'): 1, ('today', 'be'): 1, ('be', 'the'): 1, ('the', 'day'): 1, ('day', 'to'): 1, ('to', 'fly'): 1, ('fly', 'away'): 1})\n",
      "- Three most frequent word sequences and the number of occurrences according to Bigrams: [(('it', 's'), 1), (('s', 'excite'), 1), (('excite', 'to'), 1)]\n",
      "\n",
      "BoW:\n",
      "- Three most frequent words and number of occurrences according to Bag-of-Words: [('fish', 4), ('fly', 3), ('day', 3)]\n"
     ]
    }
   ],
   "source": [
    "import nltk, re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "normalizer = WordNetLemmatizer()\n",
    "\n",
    "def get_part_of_speech(word): # ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n",
    "    probable_part_of_speech = wordnet.synsets(word)\n",
    "    pos_counts = Counter()\n",
    "    pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "    pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "    pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "    pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "    most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "    return most_likely_part_of_speech\n",
    "\n",
    "def preprocess_text(text):\n",
    "    cleaned = re.sub(r'\\W+', ' ', text).lower()\n",
    "    tokenized = word_tokenize(cleaned)\n",
    "    normalized = [normalizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized]\n",
    "    return normalized\n",
    "\n",
    "# ABOVE : is for the pre-processing part to tokenize + lemmatization \n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "# the code to understand is BELOW\n",
    "\n",
    "from nltk.util import ngrams\n",
    "\n",
    "text = \"It's exciting to watch flying fish after a hard day's work. \\\n",
    "I don't know why some fish prefer flying and other fish would rather swim.\\\n",
    "It seems like the fish just woke up one day and decided, 'hey, today is the day to fly away.'\"\n",
    "\n",
    "tokens = preprocess_text(text)\n",
    "\n",
    "# Bigram approach:\n",
    "bigrams_prepped = ngrams(tokens, 2) # creates a transitory object of two paired tokens\n",
    "bigrams = Counter(bigrams_prepped) # creates dict and count words\n",
    "bigrams_most_common = bigrams.most_common(3) # most frequent terms\n",
    "\n",
    "# Bag-of-Words approach:\n",
    "bag_of_words = Counter(tokens)\n",
    "most_common_three = bag_of_words.most_common(3)\n",
    "\n",
    "# Prints\n",
    "print(\"BIGRAMS:\")\n",
    "print(\"- Word sequences and the number of occurrences according to Bigrams:\", bigrams)\n",
    "print(\"- Three most frequent word sequences and the number of occurrences according to Bigrams:\",\n",
    "      bigrams_most_common) # most frequent terms\n",
    "print(\"\\nBoW:\")\n",
    "print(\"- Three most frequent words and number of occurrences according to Bag-of-Words:\", \n",
    "      most_common_three)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
