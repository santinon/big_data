{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP tutorial overview\n",
    "based on the courses of CodeCademy\n",
    "\n",
    "##### Natural language processing combines computer science, linguistics, and artificial intelligence to enable computers to process human languages\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How to analyse text:\n",
    "   * the different parts of NLP will be covered in general, with explanations and codes as examples\n",
    "   \n",
    "* Libraries : sklearn, nltk, spacy and more ... \n",
    "\n",
    "* As usual, many comments and text will help you, hopefully, to understand a bit more about it \n",
    "\n",
    "* ENJOY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MENU: \n",
    "\n",
    "1) Text Preprocessing\n",
    "\n",
    "2) Parsing Text\n",
    "\n",
    "3) Language Models\n",
    "\n",
    "4) Topic Models\n",
    "\n",
    "5) Text Similarity\n",
    "\n",
    "6) Language Prediction & Text Generation (LSTM ...)\n",
    "\n",
    "7) Advanced NLP Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Text Preprocessing\n",
    "### Text preprocessing is a stage of NLP focused on cleaning and preparing text for other NLP tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**. Noise removal — stripping text of formatting (e.g., HTML tags)**\n",
    "\n",
    "**. Tokenization — breaking text into individual words** \n",
    "\n",
    "**. Normalization — cleaning text data in any other way:**\n",
    "   * -> **Stemming** is like a blunt axe that chops off word prefixes and suffixes. \n",
    "     (“booing” and “booed” become “boo”, but  “sing” may become “s” \n",
    "     and “sung” would remain “sung.”).\n",
    "   * -> **Lemmatization** is a scalpel to bring words down to their root forms. \n",
    "     For example, NLTK’s savvy lemmatizer knows “am” and “are” are related to “be.”\n",
    "     -Other common tasks include lowercasing, stopwords removal, spelling correction, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned \n",
      " So many squids are jumping out of suitcases these days that you can barely go anywhere without seeing one burst forth from a tightly packed valise I went to the dentist the other day and sure enough I saw an angry one jump out of my dentist s bag within minutes of arriving She hardly even noticed \n",
      "\n",
      " Stemmed text:\n",
      "['So', 'mani', 'squid', 'are', 'jump', 'out', 'of', 'suitcas', 'these', 'day', 'that', 'you', 'can', 'bare', 'go', 'anywher', 'without', 'see', 'one', 'burst', 'forth', 'from', 'a', 'tightli', 'pack', 'valis', 'I', 'went', 'to', 'the', 'dentist', 'the', 'other', 'day', 'and', 'sure', 'enough', 'I', 'saw', 'an', 'angri', 'one', 'jump', 'out', 'of', 'my', 'dentist', 's', 'bag', 'within', 'minut', 'of', 'arriv', 'she', 'hardli', 'even', 'notic']\n",
      "\n",
      "lemmatized_lessgood text:\n",
      "['So', 'many', 'squid', 'are', 'jumping', 'out', 'of', 'suitcase', 'these', 'day', 'that', 'you', 'can', 'barely', 'go', 'anywhere', 'without', 'seeing', 'one', 'burst', 'forth', 'from', 'a', 'tightly', 'packed', 'valise', 'I', 'went', 'to', 'the', 'dentist', 'the', 'other', 'day', 'and', 'sure', 'enough', 'I', 'saw', 'an', 'angry', 'one', 'jump', 'out', 'of', 'my', 'dentist', 's', 'bag', 'within', 'minute', 'of', 'arriving', 'She', 'hardly', 'even', 'noticed']\n",
      "\n",
      "Better Lemmatized text:\n",
      "['So', 'many', 'squid', 'be', 'jump', 'out', 'of', 'suitcase', 'these', 'day', 'that', 'you', 'can', 'barely', 'go', 'anywhere', 'without', 'see', 'one', 'burst', 'forth', 'from', 'a', 'tightly', 'pack', 'valise', 'I', 'go', 'to', 'the', 'dentist', 'the', 'other', 'day', 'and', 'sure', 'enough', 'I', 'saw', 'an', 'angry', 'one', 'jump', 'out', 'of', 'my', 'dentist', 's', 'bag', 'within', 'minute', 'of', 'arrive', 'She', 'hardly', 'even', 'notice']\n"
     ]
    }
   ],
   "source": [
    "# regex for removing punctuation!\n",
    "import re\n",
    "# nltk preprocessing magic\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# grabbing a part of speech function:\n",
    "from part_of_speech import get_part_of_speech # just the text\n",
    "\n",
    "text = \"\"\"So many squids are jumping out of suitcases these days that you can barely \n",
    "go anywhere without seeing one burst forth from a tightly packed valise. \n",
    "I went to the dentist the other day, and sure enough I saw an angry one jump out of \n",
    "my dentist's bag within minutes of arriving. She hardly even noticed.\"\"\"\n",
    "\n",
    "cleaned = re.sub('\\W+', ' ', text)\n",
    "print(\"cleaned \\n\", cleaned)\n",
    "# Tokenization\n",
    "tokenized = word_tokenize(cleaned)\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(token) for token in tokenized]\n",
    "# Lemmatization (by default \"lemmatize()\"\" treats every word as a noun)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_lessgood = [lemmatizer.lemmatize(token) for token in tokenized]\n",
    "lemmatized = [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized]\n",
    "\n",
    "print(\"\\n Stemmed text:\")\n",
    "print(stemmed)\n",
    "print(\"\\nlemmatized_lessgood text:\")\n",
    "print(lemmatized_lessgood)\n",
    "print(\"\\nBetter Lemmatized text:\")\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Parsing Text\n",
    "### Parsing is a stage of NLP concerned with breaking up text based on syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Parsing is a stage of NLP concerned with segmenting text based on syntax.*\n",
    "\n",
    "\n",
    "   * **Part-of-speech** tagging (**POS** tagging) \n",
    "     identifies parts of speech (verbs, nouns, adjectives, etc.).\n",
    "\n",
    "   * **Named entity recognition** (**NER**)  \n",
    "     helps identify the proper nouns (e.g., “Natalia” or “Berlin”) in a text. \n",
    "    \n",
    "   * **Dependency grammar trees**\n",
    "     help you understand the relationship between the words in a sentence. \n",
    "     Python library \"spaCy\" is at your service, even if it isn’t always perfect. \n",
    "\n",
    "   * **Regex parsing**  \n",
    "     using Python’s \"re\" library, allows for a bit more nuance. \n",
    "     When coupled with POS tagging, you can identify specific phrase pieces. \n",
    "     (On its own, it can find you addresses, emails, and many other common \n",
    "     patterns within large pieces of text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a7ec5f197bc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msquids\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msquids_text\u001b[0m \u001b[0;31m# just the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdependency_parser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mparsed_squids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdependency_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msquids_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "# Dependency grammar trees\n",
    "import spacy\n",
    "from nltk import Tree\n",
    "from squids import squids_text # just the text\n",
    "\n",
    "dependency_parser = spacy.load('en')\n",
    "\n",
    "parsed_squids = dependency_parser(squids_text)\n",
    "\n",
    "# Assign my_sentence a new value:\n",
    "my_sentence = \"I can say shit aswell here!\"\n",
    "my_parsed_sentence = dependency_parser(my_sentence)\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "  if node.n_lefts + node.n_rights > 0:\n",
    "    parsed_child_nodes = [to_nltk_tree(child) for child in node.children]\n",
    "    return Tree(node.orth_, parsed_child_nodes)\n",
    "  else:\n",
    "    return node.orth_\n",
    "\n",
    "for sent in parsed_squids.sents:\n",
    "  to_nltk_tree(sent.root).pretty_print()\n",
    "  \n",
    "for sent in my_parsed_sentence.sents:\n",
    "  to_nltk_tree(sent.root).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Language Models\n",
    "### Language models are probabilistic machine models of language use for NLP comprehension tasks. Common models include bag-of-words, n-gram models, and neural language modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-> We can help computers make predictions about language by training a\n",
    "language model on a corpus (a bunch of example text)*\n",
    "\n",
    " _**Language models**_ = probabilistic computer models of language. \n",
    "    We build and use these models to figure out the likelihood that \n",
    "    a given sound, letter, word, or phrase will be used. \n",
    "    Once a model has been trained, it can be tested out on new texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1 Bag-of-Words Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Bag-of-words model** (good when grammar and word-order are irrelevant)\n",
    "  = It counts the number of times a word is present in a text (after processing step)\n",
    "  Shared topics (words) between different examples (textes) will be observed.\n",
    "  Makes predictions concerning topic or sentiment of a text\n",
    "  = One of the most common (unigram) statistical language models.\n",
    "\n",
    "  Problem : During training, Bag-of-words model will probably come across \n",
    "  test words that it has never encountered before.\n",
    "\n",
    "\n",
    " > ex: \n",
    " \n",
    "      “The squids jumped out of the suitcases.”\n",
    " \n",
    "      {\"the\": 2, \"squid\": 1, \"jump\": 1, \"out\": 1, \"of\": 1, \"suitcase\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# importing regex and nltk\n",
    "import re, nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# importing Counter to get word counts for bag of words\n",
    "from collections import Counter\n",
    "# importing a passage from Through the Looking Glass\n",
    "#from looking_glass import looking_glass_text\n",
    "# importing part-of-speech function for lemmatization\n",
    "from part_of_speech import get_part_of_speech # just the text\n",
    "\n",
    "# Change text to another string:\n",
    "text = \"Let's know what to write in order to check. This is your text. Just try it out\"\n",
    "\n",
    "cleaned = re.sub('\\W+', ' ', text).lower()\n",
    "tokenized = word_tokenize(cleaned)\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "filtered = [word for word in tokenized if word not in stop_words]\n",
    "\n",
    "normalizer = WordNetLemmatizer()\n",
    "normalized = [normalizer.lemmatize(token, get_part_of_speech(token)) for token in filtered]\n",
    "# Comment out the print statement below\n",
    "bag_of_looking_glass_words = Counter(normalized)\n",
    "print(bag_of_looking_glass_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2 N-Grams and NLM (neural language models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **n-gram model**: considers a sequence of some number (n) units and calculates the probability of each unit in a    \n",
    "    body of language given the preceding sequence of length n.\n",
    " \n",
    "   Problem :\n",
    "   During training, Bag-of-words model will probably come across test words that it has never encountered\n",
    "   before.  \n",
    "   As the chosen sequence length grows n, the number of examples (shifts) of each sequence within your\n",
    "   training corpus (text) shrinks.          \n",
    "\n",
    " > ex : \n",
    "\n",
    "      “The squids jumped out of the suitcases. The squids were furious.” \n",
    "       \n",
    "       For n = 2: \n",
    "       \n",
    "        {('', 'the'): 2, ('the', 'squids'): 2, ('squids', 'jumped'): 1, ('jumped', 'out'): 1, ('out', 'of'): 1, ('of', 'the'): 1, ('the', 'suitcases'): 1, ('suitcases', ''): 1, ('squids', 'were'): 1, ('were', 'furious'): 1, ('furious', ''): 1}        \n",
    "\n",
    ".\n",
    ".\n",
    "\n",
    "* **neural language models** (**NLM**): Deep learning approach. NLP has involved developing and training neural networks to approximate the approach our human brains take towards language. \n",
    "Allows computers a much more adaptive tack to processing human language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "from nltk.tokenize import word_tokenize\n",
    "# importing ngrams module from nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from looking_glass import looking_glass_full_text # just the text \n",
    "\n",
    "cleaned = re.sub('\\W+', ' ', looking_glass_full_text).lower()\n",
    "tokenized = word_tokenize(cleaned)\n",
    "\n",
    "# Change the n value to 2:\n",
    "looking_glass_bigrams = ngrams(tokenized, 2)\n",
    "looking_glass_bigrams_frequency = Counter(looking_glass_bigrams)\n",
    "\n",
    "# Change the n value to 3:\n",
    "looking_glass_trigrams = ngrams(tokenized, 3)\n",
    "looking_glass_trigrams_frequency = Counter(looking_glass_trigrams)\n",
    "\n",
    "# Change the n value to a number greater than 3:\n",
    "looking_glass_ngrams = ngrams(tokenized, 1)\n",
    "looking_glass_ngrams_frequency = Counter(looking_glass_ngrams)\n",
    "\n",
    "print(\"Looking Glass Bigrams:\")\n",
    "print(looking_glass_bigrams_frequency.most_common(10))\n",
    "\n",
    "print(\"\\nLooking Glass Trigrams:\")\n",
    "print(looking_glass_trigrams_frequency.most_common(10))\n",
    "\n",
    "print(\"\\nLooking Glass n-grams:\")\n",
    "print(looking_glass_ngrams_frequency.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 ) Topic Models\n",
    "### Topic modeling is the NLP process by which hidden topics are identified given a body of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Topic modeling is an area of NLP dedicated to uncovering latent, or hidden, topics within a body of language_  \n",
    "* **Term frequency-inverse document frequency** (**tf-idf**) :  \n",
    "     * _**deprioritize**_ the _most common words_ and \n",
    "     * **_prioritize_** the _less frequently_ used terms \n",
    "     \n",
    "      \n",
    "     \n",
    "  (When we’re working with a lot of text, it makes sense if we don’t want our topics filled \n",
    "  with words like “the” and “is”).   \n",
    "  The Python libraries gensim and sklearn have modules to handle _tf-idf_.\n",
    "  \n",
    "  \n",
    "* **Latent Dirichlet Allocation** (**LDA**): \n",
    "    LDA is a statistical model that takes documents and determines which words keep popping up together in \n",
    "    the same contexts (i.e., documents).  \n",
    "    The sklearn library can tackle this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "from sherlock_holmes import bohemia_ch1, bohemia_ch2, bohemia_ch3, boscombe_ch1, boscombe_ch2, boscombe_ch3 # just the text\n",
    "from preprocessing import preprocess_text\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# preparing the text\n",
    "corpus = [bohemia_ch1, bohemia_ch2, bohemia_ch3, boscombe_ch1, boscombe_ch2, boscombe_ch3]\n",
    "preprocessed_corpus = [preprocess_text(chapter) for chapter in corpus]\n",
    "\n",
    "# Update stop_list:\n",
    "stop_list = [\"say\", \"see\", \"holmes\", \"shall\", \"say\", \n",
    "\"man\", \"upon\", \"know\", \"quite\", \"one\", \n",
    "\"well\", \"could\", \"would\", \"take\", \"may\", \n",
    "\"think\", \"come\", \"go\", \"little\", \"must\", \n",
    "\"look\"]\n",
    "# filtering topics for stop words\n",
    "def filter_out_stop_words(corpus):\n",
    "  no_stops_corpus = []\n",
    "  for chapter in corpus:\n",
    "    no_stops_chapter = \" \".join([word for word in chapter.split(\" \") if word not in stop_list])\n",
    "    no_stops_corpus.append(no_stops_chapter)\n",
    "  return no_stops_corpus\n",
    "filtered_for_stops = filter_out_stop_words(preprocessed_corpus)\n",
    "\n",
    "# creating the bag of words model\n",
    "bag_of_words_creator = CountVectorizer()\n",
    "bag_of_words = bag_of_words_creator.fit_transform(filtered_for_stops)\n",
    "\n",
    "# creating the tf-idf model\n",
    "tfidf_creator = TfidfVectorizer(min_df = 0.2)\n",
    "tfidf = tfidf_creator.fit_transform(preprocessed_corpus)\n",
    "\n",
    "# creating the bag of words LDA model\n",
    "lda_bag_of_words_creator = LatentDirichletAllocation(learning_method='online', n_components=10)\n",
    "lda_bag_of_words = lda_bag_of_words_creator.fit_transform(bag_of_words)\n",
    "\n",
    "# creating the tf-idf LDA model\n",
    "lda_tfidf_creator = LatentDirichletAllocation(learning_method='online', n_components=10)\n",
    "lda_tfidf = lda_tfidf_creator.fit_transform(tfidf)\n",
    "\n",
    "print(\"~~~ Topics found by bag of words LDA ~~~\")\n",
    "for topic_id, topic in enumerate(lda_bag_of_words_creator.components_):\n",
    "  message = \"Topic #{}: \".format(topic_id + 1)\n",
    "  message += \" \".join([bag_of_words_creator.get_feature_names()[i] for i in topic.argsort()[:-5 :-1]])\n",
    "  print(message)\n",
    "\n",
    "print(\"\\n\\n~~~ Topics found by tf-idf LDA ~~~\")\n",
    "for topic_id, topic in enumerate(lda_tfidf_creator.components_):\n",
    "  message = \"Topic #{}: \".format(topic_id + 1)\n",
    "  message += \" \".join([tfidf_creator.get_feature_names()[i] for i in topic.argsort()[:-5 :-1]])\n",
    "  print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Text Similarity\n",
    "### Text similarity is a facet of NLP concerned with semblance between instances of language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Levenshtein distance**   \n",
    "    = *minimal edit distance between two words*  \n",
    "    Distance is calculated through the minimum number of _insertions_, _deletions_, and _substitutions_ that would \n",
    "    need   to occur for one word to become another \n",
    "   \n",
    "> Ex:\n",
    "turning “bees” into “beans” would require one _substitution_ (“a” for “e”) and one _insertion_ (“n ”), so the Levenshtein distance would be two  \n",
    "-> Addressing word similarity and misspelling for spellcheck or autocorrect often involves considering\n",
    "\n",
    "* **Phonetic similarity**  \n",
    "  _how much two words or phrases sound the same_  \n",
    "   Is a major challenge within speech recognition\n",
    "\n",
    "> Ex : English-speaking humans can easily tell from context whether someone said “euthanasia” or “youth in Asia”\n",
    "  \n",
    "  \n",
    "* **Lexical similarity**  \n",
    "   _the degree to which texts use the same vocabulary and phrases_  \n",
    "   Is helpful to find out if texts are the same to guard against plagiarism\n",
    "   \n",
    "\n",
    "* **Semantic similarity**  \n",
    "    _the degree to which documents contain similar meaning or topics_  \n",
    "    Is useful when you want to find (or recommend) an article or book similar to one you recently finished.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# NLTK has a built-in function\n",
    "# to check Levenshtein distance:\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "def print_levenshtein(string1, string2):\n",
    "  print(\"The Levenshtein distance from '{0}' to '{1}' is {2}!\".format(string1, string2, edit_distance(string1, string2)))\n",
    "\n",
    "# Check the distance between\n",
    "print_levenshtein(\"fart\", \"target\") # levenstein distance = 3 \n",
    "# Assign passing strings here:\n",
    "three_away_from_code = \"mate\" # levenstein distance = 2 \n",
    "two_away_from_chunk = \"junk\" # levenstein distance = 3\n",
    "#executes the functions\n",
    "print_levenshtein(\"code\", three_away_from_code)\n",
    "print_levenshtein(\"chunk\", two_away_from_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Language Prediction & Text Generation (LSTM ...)\n",
    "### Language prediction is an application of NLP concerned with predicting language given preceding language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Language prediction**  \n",
    "Is an application of NLP concerned with predicting text given preceding text. _Autosuggest_, _autocomplete_, and _suggested replies_ are common forms of language prediction  \n",
    " * Bag of words alone is generally not a great model for language prediction; no matter what the preceding word was, you will just get one of the most commonly used words from your training corpus.\n",
    " * **Markov chains** are memory-less and make statistical predictions based entirely on the current _n-gram_ on hand. Rely on Markov chains to predict the statistical likelihood of each following word (or character) based on the training corpus\n",
    " >Ex : “I ate so many grilled cheese (_something_)”.  \n",
    " _n-gram_ used: Trigram model (n = 3)   \n",
    " Markov chain predicts the following word (_something_) as “_sandwiches_” based on the number of times the sequence “grilled cheese sandwiches” has appeared in the training data out of all the times “grilled cheese” has appeared in the training data\n",
    " * **Long Short Term Memory** (**LSTM**) model, is a more advanced approach using a neural language model. LSTM uses deep learning with a network of artificial “cells” that manage memory, making them _better_ suited for text prediction than traditional neural networks.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, random\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict, deque\n",
    "from document1 import training_doc1 # lose yourself, eminem\n",
    "from document2 import training_doc2 # linkin park, in the end\n",
    "from document3 import training_doc3 # queen, bohemian rhaspsody\n",
    "\n",
    "class MarkovChain:\n",
    "  def __init__(self):\n",
    "    self.lookup_dict = defaultdict(list)\n",
    "    self._seeded = False\n",
    "    self.__seed_me()\n",
    "\n",
    "  def __seed_me(self, rand_seed=None):\n",
    "    if self._seeded is not True:\n",
    "      try:\n",
    "        if rand_seed is not None:\n",
    "          random.seed(rand_seed)\n",
    "        else:\n",
    "          random.seed()\n",
    "        self._seeded = True\n",
    "      except NotImplementedError:\n",
    "        self._seeded = False\n",
    "    \n",
    "  def add_document(self, str):\n",
    "    preprocessed_list = self._preprocess(str)\n",
    "    pairs = self.__generate_tuple_keys(preprocessed_list)\n",
    "    for pair in pairs:\n",
    "      self.lookup_dict[pair[0]].append(pair[1])\n",
    "  \n",
    "  def _preprocess(self, str):\n",
    "    cleaned = re.sub(r'\\W+', ' ', str).lower()\n",
    "    tokenized = word_tokenize(cleaned)\n",
    "    return tokenized\n",
    "\n",
    "  def __generate_tuple_keys(self, data):\n",
    "    if len(data) < 1:\n",
    "      return\n",
    "\n",
    "    for i in range(len(data) - 1):\n",
    "      yield [ data[i], data[i + 1] ]\n",
    "      \n",
    "  def generate_text(self, max_length=50):\n",
    "    context = deque()\n",
    "    output = []\n",
    "    if len(self.lookup_dict) > 0:\n",
    "      self.__seed_me(rand_seed=len(self.lookup_dict))\n",
    "      chain_head = [list(self.lookup_dict)[0]]\n",
    "      context.extend(chain_head)\n",
    "      \n",
    "      while len(output) < (max_length - 1):\n",
    "        next_choices = self.lookup_dict[context[-1]]\n",
    "        if len(next_choices) > 0:\n",
    "          next_word = random.choice(next_choices)\n",
    "          context.append(next_word)\n",
    "          output.append(context.popleft())\n",
    "        else:\n",
    "          break\n",
    "      output.extend(list(context))\n",
    "    return \" \".join(output)\n",
    "\n",
    "my_markov = MarkovChain()\n",
    "my_markov.add_document(training_doc1)\n",
    "my_markov.add_document(training_doc2)\n",
    "my_markov.add_document(training_doc3)\n",
    "generated_text = my_markov.generate_text()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Advanced NLP Topics\n",
    "##### rely on deep learning and neural networks\n",
    "There are many social and ethical considerations to take into account when designing NLP tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Naive Bayes classifiers**  are supervised machine learning algorithms that leverage a probabilistic theorem to make predictions and classifications. They are widely used for sentiment analysis (determining whether a given block of language expresses negative or positive feelings) and spam filtering\n",
    "* **Machine translation**\n",
    "* **Language accessibility**\n",
    "  Text-to-speech functionality and speech recognition have improved rapidly thanks to neural language models (can be for people with disabilities)\n",
    "\n",
    "\n",
    "* **WARNINGS** :\n",
    "     * Because so many NLP tools are built by and for English speakers, these tools may lag behind in processing other languages. \n",
    "     * When we build and train NLP tools, are we only building them for one type of English speaker? The tools may also be programmed with cultural and linguistic biases specific to English speakers.\n",
    "     * As an NLP developer, it’s important to consider biases, both within your code and within the training corpus. A machine will learn the same biases you teach it, whether intentionally or unintentionally.\n",
    "     * There are many powerful NLP tools that come head-to-head with privacy concerns. Who is collecting your data? How much data is being collected and what do those companies plan to do with your data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
