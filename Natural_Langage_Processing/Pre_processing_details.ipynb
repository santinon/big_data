{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE - PROCESSING TUTORIAL (beginner friendly) \n",
    "based on the courses of CodeCademy\n",
    "\n",
    "### This tutorial will help to becomme familiar with NLP by explaining what are the principals things. I added many comments, explanations and examples to make things easier for your understanding. \n",
    "\n",
    "Pre-processing is:\n",
    "- Cleaning and preparing text data for use in a specific context (ultimate goal is to reduce the text to only the words that you need for your NLP goals)\n",
    "- Removing noise from data.\n",
    "\n",
    "\n",
    "While this list is not exhaustive, we will cover a few common approaches for cleaning and processing text data. They include:\n",
    "\n",
    "    . Using Regex & NLTK libraries\n",
    "    . Removing unnecessary characters and formatting\n",
    "    . Tokenization – break multi-word strings into smaller components\n",
    "    . Normalization – a catch-all term for processing data; this includes stemming and lemmatization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Noise Removal  \n",
    "[More details abour Regular Expressions (re)](https://github.com/santinon/big_data/blob/master/Natural_Langage_Processing/Regular%20Expressions%20(RE)%2C%20regex%20or%20regexp.ipynb)\n",
    "\n",
    ".sub() method in Python’s regular expression (re) library for most of your noise removal needs\n",
    "\n",
    "\n",
    "The .sub() method has three required arguments:\n",
    "- *pattern*:  a regular expression that is searched for in the input string. There must be an *r* preceding the string to indicate it is a raw string, which treats backslashes as literal characters.\n",
    "- *replacement_text* : text that replaces all matches in the input string\n",
    "- *input* : the input string that will be edited by the *.sub()* method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    This is a paragraph\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "text = \"<p>    This is a paragraph</p>\"\n",
    "# removes the HTML p signs (replaces the tags with an empty string '')\n",
    "result = re.sub(r'<.?p>', '', text)  # \".?p\" = whatever there is , if there is something (?), before the p\n",
    "\n",
    "print(result) # see the spaces in-front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a paragraph\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "text = \"    This is a paragraph\" # The whitespace consists of four spaces.\n",
    "# removes the spaces (replaces each by an empty string '')\n",
    "result = re.sub(r'\\s{4}', '', text)\n",
    "\n",
    "print(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nation's Top Pseudoscientists Harness High-Energy Quartz Crystal Capable Of Reversing Effects Of Being Gemini \n",
      "\n",
      "@fat_meats, veggies are better than you think.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "headline_one = '<h1>Nation\\'s Top Pseudoscientists Harness High-Energy Quartz Crystal Capable Of Reversing Effects Of Being Gemini</h1>'\n",
    "\n",
    "tweet = '@fat_meats, veggies are better than you think.'\n",
    "\n",
    "headline_no_tag = re.sub(r'<.?h1>', '', headline_one) # \".?h1\" = whatever (.)there is , if there is something (?), before the h1\n",
    "tweet_no_at = re.sub(r'@', '', tweet)\n",
    "\n",
    "print(headline_no_tag, \"\\n\") # \"\\n\" = empty-line\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Tokenization\n",
    "\n",
    "The method for breaking text into smaller components is called tokenization and the individual components are called tokens   \n",
    "= separates text words-by-words (tokens)\n",
    "\n",
    "A few common operations that require tokenization include:\n",
    "\n",
    "    - Finding how many words or sentences appear in text\n",
    "    - Determining how many times a specific word or phrase exists\n",
    "    - Accounting for which terms are likely to co-occur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenize', 'this', 'text']\n"
     ]
    }
   ],
   "source": [
    "# words-tokenization\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Tokenize this text\"\n",
    "tokenized = word_tokenize(text)\n",
    "\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenize this sentence.', 'Also, tokenize this sentence.']\n"
     ]
    }
   ],
   "source": [
    "# sentences-tokenization\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Tokenize this sentence. Also, tokenize this sentence.\"\n",
    "tokenized = sent_tokenize(text)\n",
    "\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['An', 'electrocardiogram', 'is', 'used', 'to', 'record', 'the', 'electrical', 'conduction', 'through', 'a', 'person', \"'s\", 'heart', '.', 'The', 'readings', 'can', 'be', 'used', 'to', 'diagnose', 'cardiac', 'arrhythmias', '.'] \n",
      "\n",
      "[\"An electrocardiogram is used to record the electrical conduction through a person's heart.\", 'The readings can be used to diagnose cardiac arrhythmias.']\n"
     ]
    }
   ],
   "source": [
    "# both tokenization occur here part below\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "ecg_text = 'An electrocardiogram is used to record the electrical conduction through a person\\'s heart. \\\n",
    "            The readings can be used to diagnose cardiac arrhythmias.'\n",
    "\n",
    "tokenized_by_word = word_tokenize(ecg_text)\n",
    "tokenized_by_sentence = sent_tokenize(ecg_text)\n",
    "\n",
    "print(tokenized_by_word, \"\\n\") # \"\\n\" = empty-line\n",
    "print(tokenized_by_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Normalization\n",
    "\n",
    "Text normalization is a catch-all term for various text pre-processing tasks. In the next few exercises, we’ll cover a few of them:\n",
    "\n",
    "* Upper or lowercasing\n",
    "\n",
    "\n",
    "* _Stopword_ removal: \n",
    "   * They include words such as “a”, “an”, and “the”. NLTK provides a built-in library with these words\n",
    "\n",
    "\n",
    "* **Stemming**: bluntly removing prefixes and suffixes from a word\n",
    "\n",
    "\n",
    "* **Lemmatization**: replacing a single-word token with its root\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THIS HAS A MIX OF CASES OH SHIT\n",
      "this has a mix of cases oh shit \n",
      "\n",
      "salvation ira, ymca, boys & girls club of ireland, feuk. brexit\n",
      "SALVATION IRA, YMCA, BOYS & GIRLS CLUB OF IRELAND, FEUK. BREXIT\n"
     ]
    }
   ],
   "source": [
    "# here,  Python’s built-in String methods to make a string all uppercase or lowercase\n",
    "\n",
    "my_string = 'tHiS HaS a MiX oF cAsEs OH shiT'\n",
    "\n",
    "print(my_string.upper())\n",
    "\n",
    "print(my_string.lower(), \"\\n\")\n",
    "\n",
    "\n",
    "#other example : \n",
    "brands = 'Salvation IRA, YMCA, Boys & Girls Club of Ireland, feuk. Brexit'\n",
    "\n",
    "brands_lower = brands.lower()\n",
    "print(brands_lower)\n",
    "brands_upper = brands.upper()\n",
    "print(brands_upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens with stopwords: \n",
      " ['NBC', 'was', 'founded', 'in', '1926', 'making', 'it', 'the', 'oldest', 'major', 'broadcast', 'network', 'in', 'the', 'USA']\n",
      "\n",
      " tokens without stopwords: \n",
      " ['NBC', 'founded', '1926', 'making', 'oldest', 'major', 'broadcast', 'network', 'USA']\n",
      "\n",
      " 2nd example \n",
      "\n",
      "\n",
      " tokens with stopwords: \n",
      " ['A', 'YouGov', 'study', 'found', 'that', 'American', \"'s\", 'like', 'Italian', 'food', 'more', 'than', 'any', 'other', 'country', \"'s\", 'cuisine', '.']\n",
      "\n",
      " tokens without stopwords: \n",
      " ['A', 'YouGov', 'study', 'found', 'American', \"'s\", 'like', 'Italian', 'food', 'country', \"'s\", 'cuisine', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stopword removal\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english')) # creates a set (the type) of stopwords\n",
    "\n",
    "sentence = \"NBC was founded in 1926 making it the oldest major broadcast network in the USA\"\n",
    "\n",
    "# tokenize nbc_statement\n",
    "word_tokens = word_tokenize(sentence) \n",
    "\n",
    "# list comprehension to remove them from a sentence\n",
    "statement_no_stop = [word for word in word_tokens if word not in stop_words]\n",
    "\n",
    "# check here\n",
    "print(\"tokens with stopwords: \\n\", word_tokens)\n",
    "print(\"\\n tokens without stopwords: \\n\",statement_no_stop)\n",
    "\n",
    "\n",
    "print(\"\\n 2nd example \\n\")\n",
    "# Other example:\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "survey_text = 'A YouGov study found that American\\'s like Italian food more \\\n",
    "               than any other country\\'s cuisine.'\n",
    "\n",
    "tokenized_survey = word_tokenize(survey_text)\n",
    "\n",
    "text_no_stops = [word for word in tokenized_survey if word not in stop_words]\n",
    "\n",
    "print(\"\\n tokens with stopwords: \\n\", tokenized_survey)\n",
    "print(\"\\n tokens without stopwords: \\n\", text_no_stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is the text preprocessing normalization task concerned with bluntly **removing word affixes (prefixes and suffixes)**. \n",
    "> ex: stemming would cast the word “going” to “go”.   \n",
    "(this is a common method used by search engines to improve matching between user input and website hits).\n",
    "\n",
    "NLTK has a built-in stemmer called _PorterStemmer_. You can use it with a _list comprehension_ to stem each word in a tokenized list of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nbc', 'wa', 'found', 'in', '1926', '.', 'thi', 'make', 'nbc', 'the', 'oldest', 'major', 'broadcast', 'network', '.']\n"
     ]
    }
   ],
   "source": [
    "# import and initialize the stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# apply stemmer to each word in a list using a list comprehension\n",
    "tokenized = ['NBC', 'was', 'founded', 'in', '1926', '.', 'This', 'makes', 'NBC', 'the',\n",
    "             'oldest', 'major', 'broadcast', 'network', '.']\n",
    "\n",
    "stemmed = [stemmer.stem(token) for token in tokenized]\n",
    "\n",
    "# check\n",
    "print(stemmed)\n",
    "\n",
    "# Notice, the words like ‘was’ and ‘founded’ became ‘wa’ and ‘found’, respectively.  \n",
    "# We you need to be careful when stemming strings.  \n",
    "# Words can often be converted to something unrecognizable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tokenized sentence: \n",
      " ['Java', 'is', 'an', 'Indonesian', 'island', 'in', 'the', 'Pacific', 'Ocean', '.', 'It', 'is', 'the', 'most', 'populated', 'island', 'in', 'the', 'world', ',', 'with', 'over', '140', 'million', 'people', '.'] \n",
      "\n",
      "\"stemmed sentence\" : \n",
      " ['java', 'is', 'an', 'indonesian', 'island', 'in', 'the', 'pacif', 'ocean', '.', 'It', 'is', 'the', 'most', 'popul', 'island', 'in', 'the', 'world', ',', 'with', 'over', '140', 'million', 'peopl', '.']\n"
     ]
    }
   ],
   "source": [
    "#  import and initialize the stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# import tokenization thing\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "populated_island = 'Java is an Indonesian island in the Pacific Ocean. It is the most populated \\\n",
    "                    island in the world, with over 140 million people.'\n",
    "\n",
    "# \"populated_island\" tokenization first\n",
    "island_tokenized = word_tokenize(populated_island)\n",
    "print(\" tokenized sentence: \\n\", island_tokenized, \"\\n\")\n",
    "\n",
    "# stemming use list-comprehension \n",
    "stemmed = [stemmer.stem(token) for token in island_tokenized]\n",
    "print('\"stemmed sentence\" : \\n', stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is a method for casting words to their **root forms**.   \n",
    "(this is a more involved process than stemming, because it requires the method to know the _part-of-speech_ for _each word_).   \n",
    "\n",
    "Since lemmatization requires the part-of-speech, it is a less efficient approach than stemming.\n",
    "\n",
    "#### To take advantage of the power of lemmatization, we need to tag each word in our text with the most likely part of speech.   We’ll do that in the next part lemmatization 5.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NBC', 'wa', 'founded', 'in', '1926']\n"
     ]
    }
   ],
   "source": [
    "# import NLTK’s WordNetLemmatizer to lemmatize text\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() # initializes instance of \"WordNetLemmatizer\"\n",
    "\n",
    "# a list comprehension to apply the lemmatize operation to each word in a list\n",
    "tokenized = [\"NBC\", \"was\", \"founded\", \"in\", \"1926\"]\n",
    "\n",
    "lemmatized = [lemmatizer.lemmatize(token) for token in tokenized]\n",
    "\n",
    "# check it\n",
    "print(lemmatized)\n",
    "\n",
    "# The result, saved to lemmatized contains 'wa', while the rest of the words remain the same. Not too useful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tokenized sentence: \n",
      " ['Indonesia', 'was', 'founded', 'in', '1945', '.', 'It', 'contains', 'the', 'most', 'populated', 'island', 'in', 'the', 'world', ',', 'Java', ',', 'with', 'over', '140', 'million', 'people', '.'] \n",
      "\n",
      "lemmatized sentence: \n",
      " ['Indonesia', 'wa', 'founded', 'in', '1945', '.', 'It', 'contains', 'the', 'most', 'populated', 'island', 'in', 'the', 'world', ',', 'Java', ',', 'with', 'over', '140', 'million', 'people', '.']\n"
     ]
    }
   ],
   "source": [
    "# import tokenization library\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# import lemmatization library\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() # initializes instance of \"WordNetLemmatizer\"\n",
    "\n",
    "# text\n",
    "populated_island = 'Indonesia was founded in 1945. It contains the most populated island in the world, Java, with over 140 million people.'\n",
    "\n",
    "# tokenization\n",
    "tokenized_string = word_tokenize(populated_island)\n",
    "print(\" tokenized sentence: \\n\", tokenized_string, \"\\n\")\n",
    "\n",
    "# lemmatization\n",
    "lemmatized_words = [lemmatizer.lemmatize(token) for token in tokenized_string]\n",
    "\n",
    "# check\n",
    "print(\"lemmatized sentence: \\n\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lemmatization 5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
