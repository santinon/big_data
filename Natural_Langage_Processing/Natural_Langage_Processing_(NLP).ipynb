{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP tutorial\n",
    "\n",
    "\n",
    "* How to analyse text:\n",
    "   * the different parts of NLP will be covered in general, with explanations and codes as examples\n",
    "   \n",
    "   \n",
    "* Libraries : sklearn, nltk, spacy and more ... \n",
    "\n",
    "\n",
    "* As usual, many comments and text will help you, hopefully, to understand a bit more about it \n",
    "\n",
    "\n",
    "* ENJOY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**. Noise removal — stripping text of formatting (e.g., HTML tags)**\n",
    "\n",
    "**. Tokenization — breaking text into individual words** \n",
    "\n",
    "**. Normalization — cleaning text data in any other way:**\n",
    "   * -> **Stemming** is like a blunt axe that chops off word prefixes and suffixes. \n",
    "     (“booing” and “booed” become “boo”, but  “sing” may become “s” \n",
    "     and “sung” would remain “sung.”).\n",
    "   * -> **Lemmatization** is a scalpel to bring words down to their root forms. \n",
    "     For example, NLTK’s savvy lemmatizer knows “am” and “are” are related to “be.”\n",
    "     -Other common tasks include lowercasing, stopwords removal, spelling correction, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned \n",
      " So many squids are jumping out of suitcases these days that you can barely go anywhere without seeing one burst forth from a tightly packed valise I went to the dentist the other day and sure enough I saw an angry one jump out of my dentist s bag within minutes of arriving She hardly even noticed \n",
      "\n",
      " Stemmed text:\n",
      "['So', 'mani', 'squid', 'are', 'jump', 'out', 'of', 'suitcas', 'these', 'day', 'that', 'you', 'can', 'bare', 'go', 'anywher', 'without', 'see', 'one', 'burst', 'forth', 'from', 'a', 'tightli', 'pack', 'valis', 'I', 'went', 'to', 'the', 'dentist', 'the', 'other', 'day', 'and', 'sure', 'enough', 'I', 'saw', 'an', 'angri', 'one', 'jump', 'out', 'of', 'my', 'dentist', 's', 'bag', 'within', 'minut', 'of', 'arriv', 'she', 'hardli', 'even', 'notic']\n",
      "\n",
      "lemmatized_lessgood text:\n",
      "['So', 'many', 'squid', 'are', 'jumping', 'out', 'of', 'suitcase', 'these', 'day', 'that', 'you', 'can', 'barely', 'go', 'anywhere', 'without', 'seeing', 'one', 'burst', 'forth', 'from', 'a', 'tightly', 'packed', 'valise', 'I', 'went', 'to', 'the', 'dentist', 'the', 'other', 'day', 'and', 'sure', 'enough', 'I', 'saw', 'an', 'angry', 'one', 'jump', 'out', 'of', 'my', 'dentist', 's', 'bag', 'within', 'minute', 'of', 'arriving', 'She', 'hardly', 'even', 'noticed']\n",
      "\n",
      "Better Lemmatized text:\n",
      "['So', 'many', 'squid', 'be', 'jump', 'out', 'of', 'suitcase', 'these', 'day', 'that', 'you', 'can', 'barely', 'go', 'anywhere', 'without', 'see', 'one', 'burst', 'forth', 'from', 'a', 'tightly', 'pack', 'valise', 'I', 'go', 'to', 'the', 'dentist', 'the', 'other', 'day', 'and', 'sure', 'enough', 'I', 'saw', 'an', 'angry', 'one', 'jump', 'out', 'of', 'my', 'dentist', 's', 'bag', 'within', 'minute', 'of', 'arrive', 'She', 'hardly', 'even', 'notice']\n"
     ]
    }
   ],
   "source": [
    "# regex for removing punctuation!\n",
    "import re\n",
    "# nltk preprocessing magic\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# grabbing a part of speech function:\n",
    "from part_of_speech import get_part_of_speech # just the text\n",
    "\n",
    "text = \"\"\"So many squids are jumping out of suitcases these days that you can barely \n",
    "go anywhere without seeing one burst forth from a tightly packed valise. \n",
    "I went to the dentist the other day, and sure enough I saw an angry one jump out of \n",
    "my dentist's bag within minutes of arriving. She hardly even noticed.\"\"\"\n",
    "\n",
    "cleaned = re.sub('\\W+', ' ', text)\n",
    "print(\"cleaned \\n\", cleaned)\n",
    "# Tokenization\n",
    "tokenized = word_tokenize(cleaned)\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(token) for token in tokenized]\n",
    "# Lemmatization (by default \"lemmatize()\"\" treats every word as a noun)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_lessgood = [lemmatizer.lemmatize(token) for token in tokenized]\n",
    "lemmatized = [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized]\n",
    "\n",
    "print(\"\\n Stemmed text:\")\n",
    "print(stemmed)\n",
    "print(\"\\nlemmatized_lessgood text:\")\n",
    "print(lemmatized_lessgood)\n",
    "print(\"\\nBetter Lemmatized text:\")\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Parsing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Parsing is a stage of NLP concerned with segmenting text based on syntax.*\n",
    "\n",
    "\n",
    "   * **Part-of-speech** tagging (**POS** tagging) \n",
    "     identifies parts of speech (verbs, nouns, adjectives, etc.).\n",
    "\n",
    "   * **Named entity recognition** (**NER**)  \n",
    "     helps identify the proper nouns (e.g., “Natalia” or “Berlin”) in a text. \n",
    "    \n",
    "   * **Dependency grammar trees**\n",
    "     help you understand the relationship between the words in a sentence. \n",
    "     Python library \"spaCy\" is at your service, even if it isn’t always perfect. \n",
    "\n",
    "   * **Regex parsing**  \n",
    "     using Python’s \"re\" library, allows for a bit more nuance. \n",
    "     When coupled with POS tagging, you can identify specific phrase pieces. \n",
    "     (On its own, it can find you addresses, emails, and many other common \n",
    "     patterns within large pieces of text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-ac0a0d1f61c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msquids\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msquids_text\u001b[0m \u001b[0;31m# just the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdependency_parser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mparsed_squids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdependency_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msquids_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk import Tree\n",
    "from squids import squids_text # just the text\n",
    "\n",
    "dependency_parser = spacy.load('en')\n",
    "\n",
    "parsed_squids = dependency_parser(squids_text)\n",
    "\n",
    "# Assign my_sentence a new value:\n",
    "my_sentence = \"I can say shit aswell here!\"\n",
    "my_parsed_sentence = dependency_parser(my_sentence)\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "  if node.n_lefts + node.n_rights > 0:\n",
    "    parsed_child_nodes = [to_nltk_tree(child) for child in node.children]\n",
    "    return Tree(node.orth_, parsed_child_nodes)\n",
    "  else:\n",
    "    return node.orth_\n",
    "\n",
    "for sent in parsed_squids.sents:\n",
    "  to_nltk_tree(sent.root).pretty_print()\n",
    "  \n",
    "for sent in my_parsed_sentence.sents:\n",
    "  to_nltk_tree(sent.root).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-> We can help computers make predictions about language by training a\n",
    "language model on a corpus (a bunch of example text)*\n",
    "\n",
    " _**Language models**_ = probabilistic computer models of language. \n",
    "    We build and use these models to figure out the likelihood that \n",
    "    a given sound, letter, word, or phrase will be used. \n",
    "    Once a model has been trained, it can be tested out on new texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1 Bag-of-Words Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Bag-of-words model** (good when grammar and word-order are irrelevant)\n",
    "  = It counts the number of times a word is present in a text (after processing step)\n",
    "  Shared topics (words) between different examples (textes) will be observed.\n",
    "  Makes predictions concerning topic or sentiment of a text\n",
    "  = One of the most common (unigram) statistical language models.\n",
    "\n",
    "  Problem : During training, Bag-of-words model will probably come across \n",
    "  test words that it has never encountered before.\n",
    "\n",
    "\n",
    " > ex: \n",
    " \n",
    "      “The squids jumped out of the suitcases.”\n",
    " \n",
    "      {\"the\": 2, \"squid\": 1, \"jump\": 1, \"out\": 1, \"of\": 1, \"suitcase\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'let': 1, 'know': 1, 'write': 1, 'order': 1, 'check': 1, 'text': 1, 'try': 1})\n"
     ]
    }
   ],
   "source": [
    "# importing regex and nltk\n",
    "import re, nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# importing Counter to get word counts for bag of words\n",
    "from collections import Counter\n",
    "# importing a passage from Through the Looking Glass\n",
    "#from looking_glass import looking_glass_text\n",
    "# importing part-of-speech function for lemmatization\n",
    "from part_of_speech import get_part_of_speech # just the text\n",
    "\n",
    "# Change text to another string:\n",
    "text = \"Let's know what to write in order to check. This is your text. Just try it out\"\n",
    "\n",
    "cleaned = re.sub('\\W+', ' ', text).lower()\n",
    "tokenized = word_tokenize(cleaned)\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "filtered = [word for word in tokenized if word not in stop_words]\n",
    "\n",
    "normalizer = WordNetLemmatizer()\n",
    "normalized = [normalizer.lemmatize(token, get_part_of_speech(token)) for token in filtered]\n",
    "# Comment out the print statement below\n",
    "bag_of_looking_glass_words = Counter(normalized)\n",
    "print(bag_of_looking_glass_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2 N-Grams and NLM (neural language models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **n-gram model**: considers a sequence of some number (n) units and calculates the probability of each unit in a    \n",
    "    body of language given the preceding sequence of length n.\n",
    " \n",
    "   Problem :\n",
    "   During training, Bag-of-words model will probably come across test words that it has never encountered\n",
    "   before.  \n",
    "   As the chosen sequence length grows n, the number of examples (shifts) of each sequence within your\n",
    "   training corpus (text) shrinks.          \n",
    "\n",
    " > ex : \n",
    "\n",
    "      “The squids jumped out of the suitcases. The squids were furious.” \n",
    "       \n",
    "       For n = 2: \n",
    "       \n",
    "        {('', 'the'): 2, ('the', 'squids'): 2, ('squids', 'jumped'): 1, ('jumped', 'out'): 1, ('out', 'of'): 1, ('of', 'the'): 1, ('the', 'suitcases'): 1, ('suitcases', ''): 1, ('squids', 'were'): 1, ('were', 'furious'): 1, ('furious', ''): 1}        \n",
    "\n",
    ".\n",
    ".\n",
    "\n",
    "* **neural language models** (**NLM**): Deep learning approach. NLP has involved developing and training neural networks to approximate the approach our human brains take towards language. \n",
    "Allows computers a much more adaptive tack to processing human language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking Glass Bigrams:\n",
      "[(('of', 'the'), 101), (('said', 'the'), 98), (('in', 'a'), 97), (('in', 'the'), 90), (('as', 'she'), 82), (('you', 'know'), 72), (('a', 'little'), 68), (('the', 'queen'), 67), (('said', 'alice'), 67), (('to', 'the'), 66)]\n",
      "\n",
      "Looking Glass Trigrams:\n",
      "[(('the', 'red', 'queen'), 54), (('the', 'white', 'queen'), 31), (('said', 'in', 'a'), 21), (('she', 'went', 'on'), 18), (('said', 'the', 'red'), 17), (('thought', 'to', 'herself'), 16), (('the', 'queen', 'said'), 16), (('said', 'to', 'herself'), 14), (('said', 'humpty', 'dumpty'), 14), (('the', 'knight', 'said'), 14)]\n",
      "\n",
      "Looking Glass n-grams:\n",
      "[(('the',), 1575), (('and',), 850), (('a',), 758), (('to',), 728), (('it',), 557), (('she',), 524), (('i',), 509), (('you',), 501), (('of',), 484), (('said',), 470)]\n"
     ]
    }
   ],
   "source": [
    "import nltk, re\n",
    "from nltk.tokenize import word_tokenize\n",
    "# importing ngrams module from nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from looking_glass import looking_glass_full_text # just the text \n",
    "\n",
    "cleaned = re.sub('\\W+', ' ', looking_glass_full_text).lower()\n",
    "tokenized = word_tokenize(cleaned)\n",
    "\n",
    "# Change the n value to 2:\n",
    "looking_glass_bigrams = ngrams(tokenized, 2)\n",
    "looking_glass_bigrams_frequency = Counter(looking_glass_bigrams)\n",
    "\n",
    "# Change the n value to 3:\n",
    "looking_glass_trigrams = ngrams(tokenized, 3)\n",
    "looking_glass_trigrams_frequency = Counter(looking_glass_trigrams)\n",
    "\n",
    "# Change the n value to a number greater than 3:\n",
    "looking_glass_ngrams = ngrams(tokenized, 1)\n",
    "looking_glass_ngrams_frequency = Counter(looking_glass_ngrams)\n",
    "\n",
    "print(\"Looking Glass Bigrams:\")\n",
    "print(looking_glass_bigrams_frequency.most_common(10))\n",
    "\n",
    "print(\"\\nLooking Glass Trigrams:\")\n",
    "print(looking_glass_trigrams_frequency.most_common(10))\n",
    "\n",
    "print(\"\\nLooking Glass n-grams:\")\n",
    "print(looking_glass_ngrams_frequency.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 ) Topic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Topic modeling is an area of NLP dedicated to uncovering latent, or hidden, topics within a body of language_  \n",
    "* **Term frequency-inverse document frequency** (**tf-idf**) :  \n",
    "     * _**deprioritize**_ the _most common words_ and \n",
    "     * **_prioritize_** the _less frequently_ used terms \n",
    "     \n",
    "      \n",
    "     \n",
    "  (When we’re working with a lot of text, it makes sense if we don’t want our topics filled \n",
    "  with words like “the” and “is”).   \n",
    "  The Python libraries gensim and sklearn have modules to handle _tf-idf_.\n",
    "  \n",
    "  \n",
    "* **Latent Dirichlet Allocation** (**LDA**): \n",
    "    LDA is a statistical model that takes documents and determines which words keep popping up together in \n",
    "    the same contexts (i.e., documents).  \n",
    "    The sklearn library can tackle this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~ Topics found by bag of words LDA ~~~\n",
      "Topic #1: find foot hand good\n",
      "Topic #2: follow house cry time\n",
      "Topic #3: hand cigar bad word\n",
      "Topic #4: mccarthy father son mr\n",
      "Topic #5: son mccarthy cigar mr\n",
      "Topic #6: street house room minute\n",
      "Topic #7: king majesty photograph sherlock\n",
      "Topic #8: hand photograph woman street\n",
      "Topic #9: mr hand majesty street\n",
      "Topic #10: note paper matter write\n",
      "\n",
      "\n",
      "~~~ Topics found by tf-idf LDA ~~~\n",
      "Topic #1: slowly foot ten come\n",
      "Topic #2: briony male read maid\n",
      "Topic #3: hang last centre title\n",
      "Topic #4: hat second quietly family\n",
      "Topic #5: nature field service detail\n",
      "Topic #6: holmes say upon man\n",
      "Topic #7: colony pity whole immense\n",
      "Topic #8: alive prefer bad god\n",
      "Topic #9: fellow colour majesty listen\n",
      "Topic #10: gun minute drive nice\n"
     ]
    }
   ],
   "source": [
    "import nltk, re\n",
    "from sherlock_holmes import bohemia_ch1, bohemia_ch2, bohemia_ch3, boscombe_ch1, boscombe_ch2, boscombe_ch3 # just the text\n",
    "from preprocessing import preprocess_text\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# preparing the text\n",
    "corpus = [bohemia_ch1, bohemia_ch2, bohemia_ch3, boscombe_ch1, boscombe_ch2, boscombe_ch3]\n",
    "preprocessed_corpus = [preprocess_text(chapter) for chapter in corpus]\n",
    "\n",
    "# Update stop_list:\n",
    "stop_list = [\"say\", \"see\", \"holmes\", \"shall\", \"say\", \n",
    "\"man\", \"upon\", \"know\", \"quite\", \"one\", \n",
    "\"well\", \"could\", \"would\", \"take\", \"may\", \n",
    "\"think\", \"come\", \"go\", \"little\", \"must\", \n",
    "\"look\"]\n",
    "# filtering topics for stop words\n",
    "def filter_out_stop_words(corpus):\n",
    "  no_stops_corpus = []\n",
    "  for chapter in corpus:\n",
    "    no_stops_chapter = \" \".join([word for word in chapter.split(\" \") if word not in stop_list])\n",
    "    no_stops_corpus.append(no_stops_chapter)\n",
    "  return no_stops_corpus\n",
    "filtered_for_stops = filter_out_stop_words(preprocessed_corpus)\n",
    "\n",
    "# creating the bag of words model\n",
    "bag_of_words_creator = CountVectorizer()\n",
    "bag_of_words = bag_of_words_creator.fit_transform(filtered_for_stops)\n",
    "\n",
    "# creating the tf-idf model\n",
    "tfidf_creator = TfidfVectorizer(min_df = 0.2)\n",
    "tfidf = tfidf_creator.fit_transform(preprocessed_corpus)\n",
    "\n",
    "# creating the bag of words LDA model\n",
    "lda_bag_of_words_creator = LatentDirichletAllocation(learning_method='online', n_components=10)\n",
    "lda_bag_of_words = lda_bag_of_words_creator.fit_transform(bag_of_words)\n",
    "\n",
    "# creating the tf-idf LDA model\n",
    "lda_tfidf_creator = LatentDirichletAllocation(learning_method='online', n_components=10)\n",
    "lda_tfidf = lda_tfidf_creator.fit_transform(tfidf)\n",
    "\n",
    "print(\"~~~ Topics found by bag of words LDA ~~~\")\n",
    "for topic_id, topic in enumerate(lda_bag_of_words_creator.components_):\n",
    "  message = \"Topic #{}: \".format(topic_id + 1)\n",
    "  message += \" \".join([bag_of_words_creator.get_feature_names()[i] for i in topic.argsort()[:-5 :-1]])\n",
    "  print(message)\n",
    "\n",
    "print(\"\\n\\n~~~ Topics found by tf-idf LDA ~~~\")\n",
    "for topic_id, topic in enumerate(lda_tfidf_creator.components_):\n",
    "  message = \"Topic #{}: \".format(topic_id + 1)\n",
    "  message += \" \".join([tfidf_creator.get_feature_names()[i] for i in topic.argsort()[:-5 :-1]])\n",
    "  print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Text Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Levenshtein distance**   \n",
    "    = *minimal edit distance between two words*  \n",
    "    Distance is calculated through the minimum number of _insertions_, _deletions_, and _substitutions_ that would \n",
    "    need   to occur for one word to become another \n",
    "   \n",
    "> Ex:\n",
    "turning “bees” into “beans” would require one _substitution_ (“a” for “e”) and one _insertion_ (“n ”), so the Levenshtein distance would be two  \n",
    "-> Addressing word similarity and misspelling for spellcheck or autocorrect often involves considering\n",
    "\n",
    "* **Phonetic similarity**  \n",
    "  _how much two words or phrases sound the same_  \n",
    "   Is a major challenge within speech recognition\n",
    "\n",
    "> Ex : English-speaking humans can easily tell from context whether someone said “euthanasia” or “youth in Asia”\n",
    "  \n",
    "  \n",
    "* **Lexical similarity**  \n",
    "   _the degree to which texts use the same vocabulary and phrases_  \n",
    "   Is helpful to find out if texts are the same to guard against plagiarism\n",
    "   \n",
    "\n",
    "* **Semantic similarity**  \n",
    "    _the degree to which documents contain similar meaning or topics_  \n",
    "    Is useful when you want to find (or recommend) an article or book similar to one you recently finished.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Levenshtein distance from 'fart' to 'target' is 3!\n",
      "The Levenshtein distance from 'code' to 'mate' is 3!\n",
      "The Levenshtein distance from 'chunk' to 'junk' is 2!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# NLTK has a built-in function\n",
    "# to check Levenshtein distance:\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "def print_levenshtein(string1, string2):\n",
    "  print(\"The Levenshtein distance from '{0}' to '{1}' is {2}!\".format(string1, string2, edit_distance(string1, string2)))\n",
    "\n",
    "# Check the distance between\n",
    "print_levenshtein(\"fart\", \"target\") # levenstein distance = 3 \n",
    "# Assign passing strings here:\n",
    "three_away_from_code = \"mate\" # levenstein distance = 2 \n",
    "two_away_from_chunk = \"junk\" # levenstein distance = 3\n",
    "#executes the functions\n",
    "print_levenshtein(\"code\", three_away_from_code)\n",
    "print_levenshtein(\"chunk\", two_away_from_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Language Prediction & Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Language prediction**  \n",
    "Is an application of NLP concerned with predicting text given preceding text. _Autosuggest_, _autocomplete_, and _suggested replies_ are common forms of language prediction  \n",
    " * Bag of words alone is generally not a great model for language prediction; no matter what the preceding word was, you will just get one of the most commonly used words from your training corpus.\n",
    " * **Markov chains** are memory-less and make statistical predictions based entirely on the current _n-gram_ on hand. Rely on Markov chains to predict the statistical likelihood of each following word (or character) based on the training corpus\n",
    " >Ex : “I ate so many grilled cheese (_something_)”.  \n",
    " _n-gram_ used: Trigram model (n = 3)   \n",
    " Markov chain predicts the following word (_something_) as “_sandwiches_” based on the number of times the sequence “grilled cheese sandwiches” has appeared in the training data out of all the times “grilled cheese” has appeared in the training data\n",
    " * **Long Short Term Memory** (**LSTM**) model, is a more advanced approach using a neural language model. LSTM uses deep learning with a network of artificial “cells” that manage memory, making them _better_ suited for text prediction than traditional neural networks.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "look out time when i m not that i m like i got so hard and you only one shot or shot do anything you only get one shot or shot do this there s cold product they were before you only one shot do the wind blows doesn t\n"
     ]
    }
   ],
   "source": [
    "import nltk, re, random\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict, deque\n",
    "from document1 import training_doc1 # lose yourself, eminem\n",
    "from document2 import training_doc2 # linkin park, in the end\n",
    "from document3 import training_doc3 # queen, bohemian rhaspsody\n",
    "\n",
    "class MarkovChain:\n",
    "  def __init__(self):\n",
    "    self.lookup_dict = defaultdict(list)\n",
    "    self._seeded = False\n",
    "    self.__seed_me()\n",
    "\n",
    "  def __seed_me(self, rand_seed=None):\n",
    "    if self._seeded is not True:\n",
    "      try:\n",
    "        if rand_seed is not None:\n",
    "          random.seed(rand_seed)\n",
    "        else:\n",
    "          random.seed()\n",
    "        self._seeded = True\n",
    "      except NotImplementedError:\n",
    "        self._seeded = False\n",
    "    \n",
    "  def add_document(self, str):\n",
    "    preprocessed_list = self._preprocess(str)\n",
    "    pairs = self.__generate_tuple_keys(preprocessed_list)\n",
    "    for pair in pairs:\n",
    "      self.lookup_dict[pair[0]].append(pair[1])\n",
    "  \n",
    "  def _preprocess(self, str):\n",
    "    cleaned = re.sub(r'\\W+', ' ', str).lower()\n",
    "    tokenized = word_tokenize(cleaned)\n",
    "    return tokenized\n",
    "\n",
    "  def __generate_tuple_keys(self, data):\n",
    "    if len(data) < 1:\n",
    "      return\n",
    "\n",
    "    for i in range(len(data) - 1):\n",
    "      yield [ data[i], data[i + 1] ]\n",
    "      \n",
    "  def generate_text(self, max_length=50):\n",
    "    context = deque()\n",
    "    output = []\n",
    "    if len(self.lookup_dict) > 0:\n",
    "      self.__seed_me(rand_seed=len(self.lookup_dict))\n",
    "      chain_head = [list(self.lookup_dict)[0]]\n",
    "      context.extend(chain_head)\n",
    "      \n",
    "      while len(output) < (max_length - 1):\n",
    "        next_choices = self.lookup_dict[context[-1]]\n",
    "        if len(next_choices) > 0:\n",
    "          next_word = random.choice(next_choices)\n",
    "          context.append(next_word)\n",
    "          output.append(context.popleft())\n",
    "        else:\n",
    "          break\n",
    "      output.extend(list(context))\n",
    "    return \" \".join(output)\n",
    "\n",
    "my_markov = MarkovChain()\n",
    "my_markov.add_document(training_doc1)\n",
    "my_markov.add_document(training_doc2)\n",
    "my_markov.add_document(training_doc3)\n",
    "generated_text = my_markov.generate_text()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
