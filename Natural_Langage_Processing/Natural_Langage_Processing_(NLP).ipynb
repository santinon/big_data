{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Text Preprocessing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "* . Noise removal — stripping text of formatting (e.g., HTML tags).\n",
    "\n",
    "* . Tokenization — breaking text into individual words.\n",
    "\n",
    "* . Normalization — cleaning text data in any other way:\n",
    "\n",
    "    - Stemming is like a blunt axe that chops off word prefixes and suffixes. \n",
    "    (“booing” and “booed” become “boo”, but  “sing” may become “s” \n",
    "    and “sung” would remain “sung.”).\n",
    "    - Lemmatization is a scalpel to bring words down to their root forms. \n",
    "    For example, NLTK’s savvy lemmatizer knows “am” and “are” are related to “be.”\n",
    "    - Other common tasks include lowercasing, stopwords removal, spelling correction, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned \n",
      " So many squids are jumping out of suitcases these days that you can barely go anywhere without seeing one burst forth from a tightly packed valise I went to the dentist the other day and sure enough I saw an angry one jump out of my dentist s bag within minutes of arriving She hardly even noticed \n",
      "\n",
      " Stemmed text:\n",
      "['So', 'mani', 'squid', 'are', 'jump', 'out', 'of', 'suitcas', 'these', 'day', 'that', 'you', 'can', 'bare', 'go', 'anywher', 'without', 'see', 'one', 'burst', 'forth', 'from', 'a', 'tightli', 'pack', 'valis', 'I', 'went', 'to', 'the', 'dentist', 'the', 'other', 'day', 'and', 'sure', 'enough', 'I', 'saw', 'an', 'angri', 'one', 'jump', 'out', 'of', 'my', 'dentist', 's', 'bag', 'within', 'minut', 'of', 'arriv', 'she', 'hardli', 'even', 'notic']\n",
      "\n",
      "lemmatized_lessgood text:\n",
      "['So', 'many', 'squid', 'are', 'jumping', 'out', 'of', 'suitcase', 'these', 'day', 'that', 'you', 'can', 'barely', 'go', 'anywhere', 'without', 'seeing', 'one', 'burst', 'forth', 'from', 'a', 'tightly', 'packed', 'valise', 'I', 'went', 'to', 'the', 'dentist', 'the', 'other', 'day', 'and', 'sure', 'enough', 'I', 'saw', 'an', 'angry', 'one', 'jump', 'out', 'of', 'my', 'dentist', 's', 'bag', 'within', 'minute', 'of', 'arriving', 'She', 'hardly', 'even', 'noticed']\n",
      "\n",
      "Better Lemmatized text:\n",
      "['So', 'many', 'squid', 'be', 'jump', 'out', 'of', 'suitcase', 'these', 'day', 'that', 'you', 'can', 'barely', 'go', 'anywhere', 'without', 'see', 'one', 'burst', 'forth', 'from', 'a', 'tightly', 'pack', 'valise', 'I', 'go', 'to', 'the', 'dentist', 'the', 'other', 'day', 'and', 'sure', 'enough', 'I', 'saw', 'an', 'angry', 'one', 'jump', 'out', 'of', 'my', 'dentist', 's', 'bag', 'within', 'minute', 'of', 'arrive', 'She', 'hardly', 'even', 'notice']\n"
     ]
    }
   ],
   "source": [
    "# regex for removing punctuation!\n",
    "import re\n",
    "# nltk preprocessing magic\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# grabbing a part of speech function:\n",
    "from part_of_speech import get_part_of_speech\n",
    "\n",
    "text = \"\"\"So many squids are jumping out of suitcases these days that you can barely \n",
    "go anywhere without seeing one burst forth from a tightly packed valise. \n",
    "I went to the dentist the other day, and sure enough I saw an angry one jump out of \n",
    "my dentist's bag within minutes of arriving. She hardly even noticed.\"\"\"\n",
    "\n",
    "cleaned = re.sub('\\W+', ' ', text)\n",
    "print(\"cleaned \\n\", cleaned)\n",
    "# Tokenization\n",
    "tokenized = word_tokenize(cleaned)\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(token) for token in tokenized]\n",
    "# Lemmatization (by default \"lemmatize()\"\" treats every word as a noun)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_lessgood = [lemmatizer.lemmatize(token) for token in tokenized]\n",
    "lemmatized = [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized]\n",
    "\n",
    "print(\"\\n Stemmed text:\")\n",
    "print(stemmed)\n",
    "print(\"\\nlemmatized_lessgood text:\")\n",
    "print(lemmatized_lessgood)\n",
    "print(\"\\nBetter Lemmatized text:\")\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Parsing Text"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Parsing is a stage of NLP concerned with segmenting text based on syntax.\n",
    "\n",
    "- Part-of-speech tagging (POS tagging) \n",
    "    identifies parts of speech (verbs, nouns, adjectives, etc.).\n",
    "\n",
    "- Named entity recognition (NER) \n",
    "    helps identify the proper nouns (e.g., “Natalia” or “Berlin”) in a text. \n",
    "    \n",
    "- Dependency grammar trees \n",
    "    help you understand the relationship between the words in a sentence. \n",
    "    Python library \"spaCy\" is at your service, even if it isn’t always perfect. \n",
    "\n",
    "- Regex parsing, \n",
    "    using Python’s \"re\" library, allows for a bit more nuance. \n",
    "    When coupled with POS tagging, you can identify specific phrase pieces. \n",
    "    (On its own, it can find you addresses, emails, and many other common \n",
    "    patterns within large pieces of text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-967b21d6e668>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msquids\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msquids_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdependency_parser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mparsed_squids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdependency_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msquids_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk import Tree\n",
    "from squids import squids_text\n",
    "\n",
    "dependency_parser = spacy.load('en')\n",
    "\n",
    "parsed_squids = dependency_parser(squids_text)\n",
    "\n",
    "# Assign my_sentence a new value:\n",
    "my_sentence = \"I can say shit aswell here!\"\n",
    "my_parsed_sentence = dependency_parser(my_sentence)\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "  if node.n_lefts + node.n_rights > 0:\n",
    "    parsed_child_nodes = [to_nltk_tree(child) for child in node.children]\n",
    "    return Tree(node.orth_, parsed_child_nodes)\n",
    "  else:\n",
    "    return node.orth_\n",
    "\n",
    "for sent in parsed_squids.sents:\n",
    "  to_nltk_tree(sent.root).pretty_print()\n",
    "  \n",
    "for sent in my_parsed_sentence.sents:\n",
    "  to_nltk_tree(sent.root).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Language Models"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-> We can help computers make predictions about language by training a \n",
    "language model on a corpus (a bunch of example text)\n",
    "\n",
    ". Language models = probabilistic computer models of language. \n",
    "    We build and use these models to figure out the likelihood that \n",
    "    a given sound, letter, word, or phrase will be used. \n",
    "    Once a model has been trained, it can be tested out on new texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1 Bag-of-Words Approach"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Bag-of-words model (good when grammar and word-order are irrelevant)\n",
    "= It counts the number of times a word is present in a text (after processing step)\n",
    "  Shared topics (words) between different examples (textes) will be observed.\n",
    "  Makes predictions concerning topic or sentiment of a text\n",
    "= One of the most common (unigram) statistical language models.\n",
    "\n",
    "Problem : During training, Bag-of-words model will probably come across \n",
    "          test words that it has never encountered before.\n",
    "\n",
    "\n",
    "ex: \n",
    "“The squids jumped out of the suitcases.”\n",
    "{\"the\": 2, \"squid\": 1, \"jump\": 1, \"out\": 1, \"of\": 1, \"suitcase\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'let': 1, 'know': 1, 'write': 1, 'order': 1, 'check': 1, 'text': 1, 'try': 1})\n"
     ]
    }
   ],
   "source": [
    "# importing regex and nltk\n",
    "import re, nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# importing Counter to get word counts for bag of words\n",
    "from collections import Counter\n",
    "# importing a passage from Through the Looking Glass\n",
    "#from looking_glass import looking_glass_text\n",
    "# importing part-of-speech function for lemmatization\n",
    "from part_of_speech import get_part_of_speech\n",
    "\n",
    "# Change text to another string:\n",
    "text = \"Let's know what to write in order to check. This is your text. Just try it out\"\n",
    "\n",
    "cleaned = re.sub('\\W+', ' ', text).lower()\n",
    "tokenized = word_tokenize(cleaned)\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "filtered = [word for word in tokenized if word not in stop_words]\n",
    "\n",
    "normalizer = WordNetLemmatizer()\n",
    "normalized = [normalizer.lemmatize(token, get_part_of_speech(token)) for token in filtered]\n",
    "# Comment out the print statement below\n",
    "bag_of_looking_glass_words = Counter(normalized)\n",
    "print(bag_of_looking_glass_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2 N-Grams and NLM (neural language models)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ". n-gram model \n",
    "considers a sequence of some number (n) units (words) and \n",
    "calculates the probability of each unit (word) in a body of \n",
    "language (text) given the preceding sequence of length n\n",
    "\n",
    "Problem : - During training, Bag-of-words model will probably come across \n",
    "            test words that it has never encountered before.\n",
    "          - As the chosen sequence length grows (n), the number of \n",
    "            examples (shifts) of each sequence within your training \n",
    "            corpus (text) shrinks.\n",
    "            \n",
    "\n",
    "ex : \n",
    "“The squids jumped out of the suitcases. The squids were furious.”\n",
    "For n = 2: \n",
    "{('', 'the'): 2, ('the', 'squids'): 2, ('squids', 'jumped'): 1, ('jumped', 'out'): 1, ('out', 'of'): 1, ('of', 'the'): 1, ('the', 'suitcases'): 1, ('suitcases', ''): 1, ('squids', 'were'): 1, ('were', 'furious'): 1, ('furious', ''): 1}\n",
    "\n",
    "\n",
    ". neural language models (NLM)\n",
    "Deep learning approach. NLP has involved developing and training neural networks\n",
    "to approximate the approach our human brains take towards language. \n",
    "Allows computers a much more adaptive tack to processing human language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking Glass Bigrams:\n",
      "[(('of', 'the'), 101), (('said', 'the'), 98), (('in', 'a'), 97), (('in', 'the'), 90), (('as', 'she'), 82), (('you', 'know'), 72), (('a', 'little'), 68), (('the', 'queen'), 67), (('said', 'alice'), 67), (('to', 'the'), 66)]\n",
      "\n",
      "Looking Glass Trigrams:\n",
      "[(('the', 'red', 'queen'), 54), (('the', 'white', 'queen'), 31), (('said', 'in', 'a'), 21), (('she', 'went', 'on'), 18), (('said', 'the', 'red'), 17), (('thought', 'to', 'herself'), 16), (('the', 'queen', 'said'), 16), (('said', 'to', 'herself'), 14), (('said', 'humpty', 'dumpty'), 14), (('the', 'knight', 'said'), 14)]\n",
      "\n",
      "Looking Glass n-grams:\n",
      "[(('the',), 1575), (('and',), 850), (('a',), 758), (('to',), 728), (('it',), 557), (('she',), 524), (('i',), 509), (('you',), 501), (('of',), 484), (('said',), 470)]\n"
     ]
    }
   ],
   "source": [
    "import nltk, re\n",
    "from nltk.tokenize import word_tokenize\n",
    "# importing ngrams module from nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from looking_glass import looking_glass_full_text\n",
    "\n",
    "cleaned = re.sub('\\W+', ' ', looking_glass_full_text).lower()\n",
    "tokenized = word_tokenize(cleaned)\n",
    "\n",
    "# Change the n value to 2:\n",
    "looking_glass_bigrams = ngrams(tokenized, 2)\n",
    "looking_glass_bigrams_frequency = Counter(looking_glass_bigrams)\n",
    "\n",
    "# Change the n value to 3:\n",
    "looking_glass_trigrams = ngrams(tokenized, 3)\n",
    "looking_glass_trigrams_frequency = Counter(looking_glass_trigrams)\n",
    "\n",
    "# Change the n value to a number greater than 3:\n",
    "looking_glass_ngrams = ngrams(tokenized, 1)\n",
    "looking_glass_ngrams_frequency = Counter(looking_glass_ngrams)\n",
    "\n",
    "print(\"Looking Glass Bigrams:\")\n",
    "print(looking_glass_bigrams_frequency.most_common(10))\n",
    "\n",
    "print(\"\\nLooking Glass Trigrams:\")\n",
    "print(looking_glass_trigrams_frequency.most_common(10))\n",
    "\n",
    "print(\"\\nLooking Glass n-grams:\")\n",
    "print(looking_glass_ngrams_frequency.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
