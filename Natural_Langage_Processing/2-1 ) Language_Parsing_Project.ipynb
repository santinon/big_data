{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project : \n",
    "We will perform a **natural language parsing** analysis to gain deeper insight into one of two famous and often discussed novels in the public domain:   \n",
    "- Oscar Wilde’s The Picture of Dorian Gray   \n",
    "- Homer’s The Iliad! \n",
    "- With Lawrence in Arabia, by Lowell Thomas\n",
    "\n",
    "[Interesting link for nltk libraries in other langages (french, german, ect)](https://stackoverflow.com/questions/35275001/use-of-punktsentencetokenizer-in-nltk)\n",
    "\n",
    "### NOTE :  there is no pre-processing provided here, we just do the parsing part in th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important : the cell just below needs to be runed one time because it contains the useful functions for later on (you can also implement this part in your code right at the beginning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before we go further we need first to create this specific tokenization function:------------\n",
    "\n",
    "# tokenizes text in sentences via specific library, then tokenizes each word in it.\n",
    "# \"PunktSentenceTokenizer\" = divides text into list of sentences (use unsupervised algorithm)\n",
    "from nltk.tokenize import PunktSentenceTokenizer, word_tokenize\n",
    "\n",
    "def word_sentence_tokenize(text):\n",
    "    # create a PunktSentenceTokenizer\n",
    "    sentence_tokenizer = PunktSentenceTokenizer(text)\n",
    "  \n",
    "    # sentence tokenize text\n",
    "    sentence_tokenized = sentence_tokenizer.tokenize(text)\n",
    "    \n",
    "    # create a list to hold word tokenized sentences\n",
    "    word_tokenized = list()\n",
    "  \n",
    "    # for-loop through each tokenized sentence in sentence_tokenized\n",
    "    for tokenized_sentence in sentence_tokenized:\n",
    "        # word tokenize \"word_tokenize\" each sentence and append to word_tokenized\n",
    "        word_tokenized.append(word_tokenize(tokenized_sentence))\n",
    "    \n",
    "    return word_tokenized\n",
    "# --------\n",
    "\n",
    "\n",
    "# A function np_chunk_counter() that returns the 30 most common NP-chunks\n",
    "# from a list of chunked sentences\n",
    "from collections import Counter\n",
    "\n",
    "# function that pulls chunks out of chunked sentence and finds the most common chunks\n",
    "def np_chunk_counter(chunked_sentences):\n",
    "\n",
    "    # create a list to hold chunks\n",
    "    chunks = list()\n",
    "\n",
    "    # for-loop through each chunked sentence to extract noun phrase chunks\n",
    "    for chunked_sentence in chunked_sentences:\n",
    "        for subtree in chunked_sentence.subtrees(filter=lambda t: t.label() == 'NP'):\n",
    "            chunks.append(tuple(subtree))\n",
    "\n",
    "    # create a Counter object\n",
    "    chunk_counter = Counter()\n",
    "\n",
    "    # for-loop through the list of chunks\n",
    "    for chunk in chunks:\n",
    "        # increase counter of specific chunk by 1\n",
    "        chunk_counter[chunk] += 1\n",
    "\n",
    "    # return 30 most frequent chunks\n",
    "    return chunk_counter.most_common(30)\n",
    "# ----------\n",
    "\n",
    "\n",
    "# vp_chunk_counter() that returns the 30 most common VP-chunks from a list of chunked sentences\n",
    "from collections import Counter\n",
    "\n",
    "# function that pulls chunks out of chunked sentence and finds the most common chunks\n",
    "def vp_chunk_counter(chunked_sentences):\n",
    "\n",
    "    # create a list to hold chunks\n",
    "    chunks = list()\n",
    "\n",
    "    # for-loop through each chunked sentence to extract verb phrase chunks\n",
    "    for chunked_sentence in chunked_sentences:\n",
    "        for subtree in chunked_sentence.subtrees(filter=lambda t: t.label() == 'VP'):\n",
    "            chunks.append(tuple(subtree))\n",
    "\n",
    "    # create a Counter object\n",
    "    chunk_counter = Counter()\n",
    "\n",
    "    # for-loop through the list of chunks\n",
    "    for chunk in chunks:\n",
    "        # increase counter of specific chunk by 1\n",
    "        chunk_counter[chunk] += 1\n",
    "\n",
    "    # return 30 most frequent chunks\n",
    "    return chunk_counter.most_common(30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE ILIAD OF HOMER\n",
      "\n",
      "- single_word_tokenized_sentence (the 1945th): \n",
      " ['on', 'the', 'bright', 'axle', 'turns', 'the', 'bidden', 'wheel', 'of', 'sounding', 'brass', ';', 'the', 'polished', 'axle', 'steel', '.']\n",
      "\n",
      "- single_pos_sentence  (the 1645th  ):\n",
      " [('on', 'IN'), ('the', 'DT'), ('bright', 'JJ'), ('axle', 'NN'), ('turns', 'VBZ'), ('the', 'DT'), ('bidden', 'JJ'), ('wheel', 'NN'), ('of', 'IN'), ('sounding', 'VBG'), ('brass', 'NN'), (';', ':'), ('the', 'DT'), ('polished', 'JJ'), ('axle', 'NN'), ('steel', 'NN'), ('.', '.')]\n",
      "\n",
      "- most_common_np_chunks: \n",
      " [((('hector', 'NN'),), 322), ((('i', 'NN'),), 277), ((('jove', 'NN'),), 257), ((('troy', 'NN'),), 208), ((('vain', 'NN'),), 195), ((('war', 'NN'),), 193), ((('son', 'NN'),), 170), ((('thou', 'NN'),), 158), ((('the', 'DT'), ('plain', 'NN')), 157), ((('the', 'DT'), ('field', 'NN')), 154), ((('the', 'DT'), ('ground', 'NN')), 138), ((('death', 'NN'),), 134), ((('hand', 'NN'),), 134), ((('greece', 'NN'),), 128), ((('heaven', 'NN'),), 127), ((('fate', 'NN'),), 127), ((('thee', 'NN'),), 122), ((('breast', 'NN'),), 121), ((('the', 'DT'), ('trojan', 'NN')), 120), ((('the', 'DT'), ('god', 'NN')), 119), ((('the', 'DT'), ('war', 'NN')), 117), ((('the', 'DT'), ('greeks', 'NN')), 116), ((('blood', 'NN'),), 115), ((('homer', 'NN'),), 112), ((('the', 'DT'), ('king', 'NN')), 105), ((('rage', 'NN'),), 103), ((('force', 'NN'),), 103), ((('care', 'NN'),), 99), ((('head', 'NN'),), 98), ((('man', 'NN'),), 97)]\n",
      "\n",
      "- most_common_vp_chunks_first_structure, {<VB.*><DT>?<JJ>*<NN><RB.?>?} : \n",
      " [(((\"'t\", 'NN'), ('is', 'VBZ')), 19), ((('i', 'NN'), ('am', 'VBP')), 11), (((\"'t\", 'NN'), ('was', 'VBD')), 11), ((('the', 'DT'), ('hero', 'NN'), ('said', 'VBD')), 9), ((('i', 'NN'), ('know', 'VBP')), 8), ((('i', 'NN'), ('saw', 'VBD')), 8), ((('the', 'DT'), ('scene', 'NN'), ('lies', 'VBZ')), 7), ((('i', 'NN'), ('was', 'VBD')), 6), ((('confess', 'NN'), (\"'d\", 'VBD')), 6), ((('the', 'DT'), ('scene', 'NN'), ('is', 'VBZ')), 6), ((('view', 'NN'), (\"'d\", 'VBD')), 5), ((('i', 'NN'), ('felt', 'VBD')), 5), ((('i', 'NN'), ('bear', 'VBP')), 5), ((('hector', 'NN'), ('is', 'VBZ')), 5), ((('vain', 'NN'), ('was', 'VBD')), 5), ((('homer', 'NN'), ('was', 'VBD')), 4), ((('i', 'NN'), ('have', 'VBP')), 4), ((('hunger', 'NN'), ('was', 'VBD')), 4), ((('glory', 'NN'), ('lost', 'VBN')), 4), ((('i', 'NN'), ('see', 'VBP')), 4), ((('war', 'NN'), ('be', 'VB')), 4), ((('the', 'DT'), ('weapon', 'NN'), ('stood', 'VBD')), 4), ((('i', 'NN'), ('go', 'VBP')), 4), ((('the', 'DT'), ('silence', 'NN'), ('broke', 'VBD')), 4), ((('the', 'DT'), ('trojan', 'NN'), ('bands', 'VBZ')), 4), ((('father', 'NN'), ('gave', 'VBD')), 4), ((('i', 'NN'), ('deem', 'VBP')), 4), ((('minerva', 'NN'), ('repressing', 'VBG')), 3), ((('thetis', 'NN'), ('calling', 'VBG')), 3), ((('thetis', 'NN'), ('entreating', 'VBG')), 3)]\n",
      "\n",
      "- most_common_vp_chunks_seconde_structure, {<DT>?<JJ>*<NN><VB.*><RB.?>?} : \n",
      " [((('erring', 'VBG'), ('fury', 'NN')), 5), ((('gave', 'VBD'), ('way', 'NN')), 5), ((('thundering', 'VBG'), ('sound', 'NN')), 5), ((('be', 'VB'), ('mine', 'NN')), 4), ((('let', 'VB'), ('greece', 'NN')), 4), ((('having', 'VBG'), ('spoke', 'NN')), 4), ((('gives', 'VBZ'), ('command', 'NN')), 4), ((('godlike', 'VB'), ('hector', 'NN')), 4), ((('be', 'VB'), ('thy', 'JJ'), ('care', 'NN')), 4), (((\"o'er\", 'VBD'), ('heaven', 'NN')), 4), ((('go', 'VB'), ('thou', 'NN')), 4), ((('invoking', 'VBG'), ('the', 'DT'), ('muse', 'NN')), 3), ((('repressing', 'VBG'), ('the', 'DT'), ('fury', 'NN')), 3), ((('calling', 'VBG'), ('briareus', 'NN')), 3), ((('entreating', 'VBG'), ('jupiter', 'NN')), 3), ((('sending', 'VBG'), ('the', 'DT'), ('evil', 'JJ'), ('dream', 'NN')), 3), ((('chiding', 'VBG'), ('paris', 'NN')), 3), ((('assist', 'VB'), ('the', 'DT'), ('greeks', 'NN')), 3), ((('advising', 'VBG'), ('hector', 'NN')), 3), ((('conveying', 'VBG'), ('the', 'DT'), ('body', 'NN')), 3), ((('commanding', 'VBG'), ('the', 'DT'), ('sun', 'NN')), 3), ((('receiving', 'VBG'), ('the', 'DT'), ('infant', 'JJ'), ('vulcan', 'NN')), 3), ((('bringing', 'VBG'), ('the', 'DT'), ('armour', 'NN')), 3), ((('obtain', 'VB'), ('the', 'DT'), ('body', 'NN')), 3), ((('see', 'VBP'), ('nothing', 'NN')), 3), ((('led', 'VBN'), ('the', 'DT'), ('way', 'NN')), 3), ((('is', 'VBZ'), ('time', 'NN')), 3), ((('o', 'VBZ'), ('king', 'NN')), 3), ((('are', 'VBP'), ('wise', 'NN')), 3), ((('devoted', 'VBN'), ('wall', 'NN')), 3)]\n"
     ]
    }
   ],
   "source": [
    "# THE ILIAD OF HOMER\n",
    "\n",
    "\n",
    "print(\"THE ILIAD OF HOMER\\n\")\n",
    "# 1 ) Import and Preprocess Text Data\n",
    "from nltk import pos_tag, RegexpParser\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# import chosen text and convert it to lowercase\n",
    "text = open(\"the_iliad.txt\",encoding='utf-8').read().lower()\n",
    "# sentence and word tokenized text (function \"word_sentence_tokenize\" above)\n",
    "word_tokenized_text = word_sentence_tokenize(text)\n",
    "\n",
    "# store and print any word tokenized sentence\n",
    "# let us see the word tokenized sentence 1945th\n",
    "single_word_tokenized_sentence = word_tokenized_text[1945]\n",
    "print(\"- single_word_tokenized_sentence (the 1945th): \\n\", single_word_tokenized_sentence) # check\n",
    "\n",
    "\n",
    "# 2 ) Part-of-speech (POS)  Tag Text\n",
    "# create a list to hold part-of-speech tagged sentences here\n",
    "pos_tagged_text = []\n",
    "\n",
    "# create a for loop through each word tokenized sentence here\n",
    "for word_tokenized_sentence in word_tokenized_text:\n",
    "  # part-of-speech tag each sentence and append to list of \"pos-tagged sentences\" with \n",
    "  # \"pos_tag()\" nltk built-in function\n",
    "    pos_tagged_text.append(pos_tag(word_tokenized_sentence))\n",
    "  \n",
    "\n",
    "# store and print any part-of-speech tagged sentence\n",
    "# let us see the pos-tagged sentence 1945th\n",
    "single_pos_sentence = pos_tagged_text[1945]\n",
    "print(\"\\n- single_pos_sentence  (the 1645th  ):\\n\", single_pos_sentence)\n",
    "\n",
    "\n",
    "# 3) Chunk Sentences (\"sens extraction\" based on noun or verb focus)\n",
    "# define noun phrase (np) chunk grammar\n",
    "np_chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "# create noun phrase RegexpParser object (RegexpParser object named \"np_chunk_parser\" \n",
    "# using the noun phrase np chunk grammar as argument)\n",
    "np_chunk_parser = RegexpParser(np_chunk_grammar)\n",
    "\n",
    "# define verb phrase chunk grammar\n",
    "vp_first_chunk_grammar = \"VP: {<VB.*><DT>?<JJ>*<NN><RB.?>?}\" # first structure\n",
    "vp_second_chunk_grammar = \"VP: {<DT>?<JJ>*<NN><VB.*><RB.?>?}\" # second structure\n",
    "\n",
    "# create verb phrase RegexpParser object (RegexpParser object named \"vp_chunk_parser\" \n",
    "#using the verb phrase vp chunk grammar as argument)\n",
    "vp_first_chunk_parser = RegexpParser(vp_first_chunk_grammar) # first structure\n",
    "vp_second_chunk_parser = RegexpParser(vp_second_chunk_grammar)  # second structure\n",
    "\n",
    "# create a list to hold noun phrase chunked sentences \n",
    "np_chunked_text = []\n",
    "# and a list to hold verb phrase chunked sentences\n",
    "vp_first_chunked_text = []  # first structure\n",
    "vp_second_chunked_text = [] # second structure\n",
    "\n",
    "# create a for loop through each pos-tagged sentence\n",
    "for pos_tagged_sentence in pos_tagged_text: # for every sentence where the words have been POSed\n",
    "  # chunk each sentence and append to lists (with np_chunk_parser‘s or \"vp_chunk_parser\" (RegexpParser object) \".parse()\" method and append to dedicated list)\n",
    "  np_chunked_text.append(np_chunk_parser.parse(pos_tagged_sentence)) # for noun focus\n",
    "  vp_first_chunked_text.append(vp_second_chunk_parser.parse(pos_tagged_sentence)) # for verb focus Second Structure\n",
    "  vp_second_chunked_text.append(vp_first_chunk_parser.parse(pos_tagged_sentence)) # for verb focus Second Structure\n",
    "\n",
    "    \n",
    "# 4 ) Analyze Chunks \n",
    "# store and print the most common NP-chunks (via \"np_chunk_counter function\" in the cell above)\n",
    "most_common_np_chunks = np_chunk_counter(np_chunked_text)\n",
    "print(\"\\n- most_common_np_chunks: \\n\", most_common_np_chunks)  # check\n",
    "\n",
    "# store and print the most common VP-chunks here\n",
    "most_common_vp_chunks_first_structure = vp_chunk_counter(vp_first_chunked_text)\n",
    "print(\"\\n- most_common_vp_chunks_first_structure, {<VB.*><DT>?<JJ>*<NN><RB.?>?} : \\n\", most_common_vp_chunks_first_structure)\n",
    "most_common_vp_chunks_seconde_structure = vp_chunk_counter(vp_second_chunked_text)\n",
    "print(\"\\n- most_common_vp_chunks_seconde_structure, {<DT>?<JJ>*<NN><VB.*><RB.?>?} : \\n\", most_common_vp_chunks_seconde_structure)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Picture of Dorian Gray\n",
      "\n",
      "- single_word_tokenized_sentence (the 1945th): \n",
      " ['he', 'had', 'something', 'on', 'his', 'mind', 'to', 'ask', 'of', 'her', ',', 'something', 'that', 'he', 'had', 'brooded', 'on', 'for', 'many', 'months', 'of', 'silence', '.']\n",
      "\n",
      "- single_pos_sentence  (the 1645th  ):\n",
      " [('he', 'PRP'), ('had', 'VBD'), ('something', 'NN'), ('on', 'IN'), ('his', 'PRP$'), ('mind', 'NN'), ('to', 'TO'), ('ask', 'VB'), ('of', 'IN'), ('her', 'PRP'), (',', ','), ('something', 'NN'), ('that', 'IN'), ('he', 'PRP'), ('had', 'VBD'), ('brooded', 'VBN'), ('on', 'IN'), ('for', 'IN'), ('many', 'JJ'), ('months', 'NNS'), ('of', 'IN'), ('silence', 'NN'), ('.', '.')]\n",
      "\n",
      "- most_common_np_chunks: \n",
      " [((('i', 'NN'),), 962), ((('henry', 'NN'),), 200), ((('lord', 'NN'),), 197), ((('life', 'NN'),), 170), ((('harry', 'NN'),), 136), ((('dorian', 'JJ'), ('gray', 'NN')), 127), ((('something', 'NN'),), 126), ((('nothing', 'NN'),), 93), ((('basil', 'NN'),), 85), ((('the', 'DT'), ('world', 'NN')), 70), ((('everything', 'NN'),), 69), ((('anything', 'NN'),), 68), ((('hallward', 'NN'),), 68), ((('the', 'DT'), ('man', 'NN')), 61), ((('the', 'DT'), ('room', 'NN')), 60), ((('face', 'NN'),), 57), ((('the', 'DT'), ('door', 'NN')), 56), ((('love', 'NN'),), 55), ((('art', 'NN'),), 52), ((('course', 'NN'),), 51), ((('the', 'DT'), ('picture', 'NN')), 46), ((('the', 'DT'), ('lad', 'NN')), 45), ((('head', 'NN'),), 44), ((('round', 'NN'),), 44), ((('hand', 'NN'),), 44), ((('sibyl', 'NN'),), 41), ((('the', 'DT'), ('table', 'NN')), 40), ((('the', 'DT'), ('painter', 'NN')), 38), ((('sir', 'NN'),), 38), ((('a', 'DT'), ('moment', 'NN')), 38)]\n",
      "\n",
      "- most_common_vp_chunks_first_structure, {<VB.*><DT>?<JJ>*<NN><RB.?>?} : \n",
      " [((('i', 'NN'), ('am', 'VBP')), 101), ((('i', 'NN'), ('was', 'VBD')), 40), ((('i', 'NN'), ('want', 'VBP')), 37), ((('i', 'NN'), ('know', 'VBP')), 33), ((('i', 'NN'), ('have', 'VBP')), 32), ((('i', 'NN'), ('do', 'VBP'), (\"n't\", 'RB')), 31), ((('i', 'NN'), ('had', 'VBD')), 31), ((('i', 'NN'), ('suppose', 'VBP')), 17), ((('i', 'NN'), ('think', 'VBP')), 16), ((('i', 'NN'), ('am', 'VBP'), ('not', 'RB')), 14), ((('i', 'NN'), ('thought', 'VBD')), 13), ((('i', 'NN'), ('believe', 'VBP')), 12), ((('dorian', 'JJ'), ('gray', 'NN'), ('was', 'VBD')), 11), ((('i', 'NN'), ('am', 'VBP'), ('so', 'RB')), 11), ((('henry', 'NN'), ('had', 'VBD')), 11), ((('i', 'NN'), ('did', 'VBD'), (\"n't\", 'RB')), 9), ((('i', 'NN'), ('met', 'VBD')), 9), ((('i', 'NN'), ('said', 'VBD')), 9), ((('i', 'NN'), ('am', 'VBP'), ('quite', 'RB')), 8), ((('i', 'NN'), ('see', 'VBP')), 8), ((('i', 'NN'), ('did', 'VBD'), ('not', 'RB')), 7), ((('i', 'NN'), ('have', 'VBP'), ('ever', 'RB')), 7), ((('life', 'NN'), ('has', 'VBZ')), 7), ((('i', 'NN'), ('did', 'VBD')), 6), ((('i', 'NN'), ('feel', 'VBP')), 6), ((('life', 'NN'), ('is', 'VBZ')), 6), ((('the', 'DT'), ('lad', 'NN'), ('was', 'VBD')), 6), ((('i', 'NN'), ('asked', 'VBD')), 6), ((('i', 'NN'), ('came', 'VBD')), 6), ((('i', 'NN'), ('felt', 'VBD')), 6)]\n",
      "\n",
      "- most_common_vp_chunks_seconde_structure, {<DT>?<JJ>*<NN><VB.*><RB.?>?} : \n",
      " [((('said', 'VBD'), ('lord', 'NN')), 46), ((('was', 'VBD'), ('something', 'NN')), 17), ((('is', 'VBZ'), ('nothing', 'NN')), 11), ((('answered', 'VBD'), ('lord', 'NN')), 10), ((('turned', 'VBD'), ('round', 'NN')), 9), ((('said', 'VBD'), ('hallward', 'NN')), 8), ((('made', 'VBD'), ('no', 'DT'), ('answer', 'NN')), 8), ((('am', 'VBP'), ('afraid', 'JJ'), ('i', 'NN')), 7), ((('said', 'VBD'), ('the', 'DT'), ('painter', 'NN')), 7), ((('murmured', 'VBD'), ('lord', 'NN')), 7), ((('is', 'VBZ'), ('something', 'NN')), 7), ((('was', 'VBD'), ('nothing', 'NN')), 7), ((('looked', 'VBD'), ('round', 'NN')), 7), ((('am', 'VBP'), ('sorry', 'NN')), 7), ((('cried', 'VBD'), ('lord', 'NN')), 6), ((('do', 'VB'), ('anything', 'NN')), 6), ((('have', 'VB'), ('something', 'NN')), 6), ((('is', 'VBZ'), ('the', 'DT'), ('reason', 'NN')), 6), ((('lit', 'VBD'), ('a', 'DT'), ('cigarette', 'NN')), 6), ((('said', 'VBD'), ('dorian', 'JJ'), ('gray', 'NN')), 6), ((('cried', 'VBD'), ('the', 'DT'), ('lad', 'NN')), 6), ((('is', 'VBZ'), ('no', 'DT'), ('such', 'JJ'), ('thing', 'NN')), 5), ((('believe', 'VB'), ('anything', 'NN')), 5), ((('has', 'VBZ'), ('nothing', 'NN')), 5), ((('cried', 'VBD'), ('dorian', 'JJ'), ('gray', 'NN')), 5), ((('lord', 'VB'), ('henry', 'NN')), 5), ((('is', 'VBZ'), ('the', 'DT'), ('only', 'JJ'), ('thing', 'NN')), 5), ((('wish', 'VBP'), ('i', 'NN')), 5), ((('left', 'VBD'), ('the', 'DT'), ('room', 'NN')), 5), ((('opened', 'VBD'), ('the', 'DT'), ('door', 'NN')), 5)]\n"
     ]
    }
   ],
   "source": [
    "# The Picture of Dorian Gray\n",
    "\n",
    "\n",
    "print(\"The Picture of Dorian Gray\\n\")\n",
    "# 1 ) Import and Preprocess Text Data\n",
    "from nltk import pos_tag, RegexpParser\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# import chosen text and convert it to lowercase\n",
    "text = open(\"dorian_gray.txt\",encoding='utf-8').read().lower()\n",
    "# sentence and word tokenized text (function \"word_sentence_tokenize\" above)\n",
    "word_tokenized_text = word_sentence_tokenize(text)\n",
    "\n",
    "# store and print any word tokenized sentence\n",
    "# let us see the word tokenized sentence 1945th\n",
    "single_word_tokenized_sentence = word_tokenized_text[1945]\n",
    "print(\"- single_word_tokenized_sentence (the 1945th): \\n\", single_word_tokenized_sentence) # check\n",
    "\n",
    "\n",
    "# 2 ) Part-of-speech (POS)  Tag Text\n",
    "# create a list to hold part-of-speech tagged sentences here\n",
    "pos_tagged_text = []\n",
    "\n",
    "# create a for loop through each word tokenized sentence here\n",
    "for word_tokenized_sentence in word_tokenized_text:\n",
    "  # part-of-speech tag each sentence and append to list of \"pos-tagged sentences\" with \n",
    "  # \"pos_tag()\" nltk built-in function\n",
    "    pos_tagged_text.append(pos_tag(word_tokenized_sentence))\n",
    "  \n",
    "\n",
    "# store and print any part-of-speech tagged sentence\n",
    "# let us see the pos-tagged sentence 1945th\n",
    "single_pos_sentence = pos_tagged_text[1945]\n",
    "print(\"\\n- single_pos_sentence  (the 1645th  ):\\n\", single_pos_sentence)\n",
    "\n",
    "\n",
    "# 3) Chunk Sentences (\"sens extraction\" based on noun or verb focus)\n",
    "# define noun phrase (np) chunk grammar\n",
    "np_chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "# create noun phrase RegexpParser object (RegexpParser object named \"np_chunk_parser\" \n",
    "# using the noun phrase np chunk grammar as argument)\n",
    "np_chunk_parser = RegexpParser(np_chunk_grammar)\n",
    "\n",
    "# define verb phrase chunk grammar\n",
    "vp_first_chunk_grammar = \"VP: {<VB.*><DT>?<JJ>*<NN><RB.?>?}\" # first structure\n",
    "vp_second_chunk_grammar = \"VP: {<DT>?<JJ>*<NN><VB.*><RB.?>?}\" # second structure\n",
    "\n",
    "# create verb phrase RegexpParser object (RegexpParser object named \"vp_chunk_parser\" \n",
    "#using the verb phrase vp chunk grammar as argument)\n",
    "vp_first_chunk_parser = RegexpParser(vp_first_chunk_grammar) # first structure\n",
    "vp_second_chunk_parser = RegexpParser(vp_second_chunk_grammar)  # second structure\n",
    "\n",
    "# create a list to hold noun phrase chunked sentences \n",
    "np_chunked_text = []\n",
    "# and a list to hold verb phrase chunked sentences\n",
    "vp_first_chunked_text = []  # first structure\n",
    "vp_second_chunked_text = [] # second structure\n",
    "\n",
    "# create a for loop through each pos-tagged sentence\n",
    "for pos_tagged_sentence in pos_tagged_text: # for every sentence where the words have been POSed\n",
    "  # chunk each sentence and append to lists (with np_chunk_parser‘s or \"vp_chunk_parser\" (RegexpParser object) \".parse()\" method and append to dedicated list)\n",
    "  np_chunked_text.append(np_chunk_parser.parse(pos_tagged_sentence)) # for noun focus\n",
    "  vp_first_chunked_text.append(vp_second_chunk_parser.parse(pos_tagged_sentence)) # for verb focus Second Structure\n",
    "  vp_second_chunked_text.append(vp_first_chunk_parser.parse(pos_tagged_sentence)) # for verb focus Second Structure\n",
    "\n",
    "    \n",
    "# 4 ) Analyze Chunks \n",
    "# store and print the most common NP-chunks (via \"np_chunk_counter function\" in the cell above)\n",
    "most_common_np_chunks = np_chunk_counter(np_chunked_text)\n",
    "print(\"\\n- most_common_np_chunks: \\n\", most_common_np_chunks)  # check\n",
    "\n",
    "# store and print the most common VP-chunks here\n",
    "most_common_vp_chunks_first_structure = vp_chunk_counter(vp_first_chunked_text)\n",
    "print(\"\\n- most_common_vp_chunks_first_structure, {<VB.*><DT>?<JJ>*<NN><RB.?>?} : \\n\", most_common_vp_chunks_first_structure)\n",
    "most_common_vp_chunks_seconde_structure = vp_chunk_counter(vp_second_chunked_text)\n",
    "print(\"\\n- most_common_vp_chunks_seconde_structure, {<DT>?<JJ>*<NN><VB.*><RB.?>?} : \\n\", most_common_vp_chunks_seconde_structure)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Lawrence in Arabia, by Lowell Thomas\n",
      "\n",
      "- single_word_tokenized_sentence (the 1945th): \n",
      " ['the', 'temple', 'was', 'carved', 'from', 'the', 'cliff', 'almost', 'two', 'thousand', 'years', 'ago', 'during', 'the', 'reign', 'of', 'the', 'roman', 'emperor', 'hadrian', ',', 'who', 'visited', 'petra', 'in', 'a.', 'd.', '131', '.']\n",
      "\n",
      "- single_pos_sentence  (the 1645th  ):\n",
      " [('the', 'DT'), ('temple', 'NN'), ('was', 'VBD'), ('carved', 'VBN'), ('from', 'IN'), ('the', 'DT'), ('cliff', 'NN'), ('almost', 'RB'), ('two', 'CD'), ('thousand', 'CD'), ('years', 'NNS'), ('ago', 'RB'), ('during', 'IN'), ('the', 'DT'), ('reign', 'NN'), ('of', 'IN'), ('the', 'DT'), ('roman', 'NN'), ('emperor', 'NN'), ('hadrian', 'JJ'), (',', ','), ('who', 'WP'), ('visited', 'VBD'), ('petra', 'NN'), ('in', 'IN'), ('a.', 'NN'), ('d.', 'NN'), ('131', 'CD'), ('.', '.')]\n",
      "\n",
      "- most_common_np_chunks: \n",
      " [((('lawrence', 'NN'),), 718), ((('s', 'NN'),), 299), ((('the', 'DT'), ('desert', 'NN')), 183), ((('i', 'NN'),), 180), ((('feisal', 'NN'),), 168), ((('arabia', 'NN'),), 145), ((('”', 'NN'),), 141), ((('“', 'NN'),), 134), ((('mecca', 'NN'),), 95), ((('hussein', 'NN'),), 94), ((('damascus', 'NN'),), 91), ((('the', 'DT'), ('war', 'NN')), 89), ((('akaba', 'NN'),), 81), ((('the', 'DT'), ('world', 'NN')), 75), ((('time', 'NN'),), 67), ((('syria', 'NN'),), 66), ((('the', 'DT'), ('hedjaz', 'NN')), 65), ((('palestine', 'NN'),), 63), ((('’', 'NN'),), 60), ((('medina', 'NN'),), 57), ((('allenby', 'NN'),), 54), ((('colonel', 'NN'),), 54), ((('way', 'NN'),), 48), ((('photograph', 'NN'),), 48), ((('order', 'NN'),), 46), ((('the', 'DT'), ('city', 'NN')), 45), ((('deraa', 'NN'),), 44), ((('war', 'NN'),), 42), ((('fact', 'NN'),), 42), ((('army', 'NN'),), 41)]\n",
      "\n",
      "- most_common_vp_chunks_first_structure, {<VB.*><DT>?<JJ>*<NN><RB.?>?} : \n",
      " [((('lawrence', 'NN'), ('had', 'VBD')), 41), ((('lawrence', 'NN'), ('was', 'VBD')), 31), ((('i', 'NN'), ('had', 'VBD')), 15), ((('i', 'NN'), ('was', 'VBD')), 11), ((('i', 'NN'), ('am', 'VBP')), 9), ((('feisal', 'NN'), ('had', 'VBD')), 8), ((('i', 'NN'), ('know', 'VBP')), 8), ((('the', 'DT'), ('“', 'NN'), ('lost', 'VBD')), 7), ((('feisal', 'NN'), ('was', 'VBD')), 7), ((('i', 'NN'), ('asked', 'VBD')), 7), ((('i', 'NN'), ('saw', 'VBD')), 7), ((('the', 'DT'), ('war', 'NN'), ('was', 'VBD')), 7), ((('i', 'NN'), ('found', 'VBD')), 6), ((('lawrence', 'NN'), ('has', 'VBZ')), 6), ((('’', 'NN'), ('s', 'VBD')), 6), ((('allenby', 'NN'), ('had', 'VBD')), 5), ((('feisal', 'NN'), ('is', 'VBZ')), 5), ((('”', 'NN'), ('was', 'VBD')), 5), ((('arabia', 'NN'), ('is', 'VBZ')), 5), ((('petra', 'NN'), ('was', 'VBD')), 5), ((('lawrence', 'NN'), ('conferring', 'VBG')), 4), ((('mother', 'NN'), ('was', 'VBD')), 4), ((('the', 'DT'), ('latter', 'NN'), ('were', 'VBD')), 4), ((('lawrence', 'NN'), ('decided', 'VBD')), 4), ((('the', 'DT'), ('near', 'JJ'), ('east', 'NN'), ('was', 'VBD')), 4), ((('the', 'DT'), ('desert', 'NN'), ('has', 'VBZ')), 4), ((('mecca', 'NN'), ('is', 'VBZ')), 4), ((('lawrence', 'NN'), ('wanted', 'VBD')), 4), ((('lawrence', 'NN'), ('knew', 'VBD')), 4), ((('the', 'DT'), ('turks', 'NN'), ('had', 'VBD')), 4)]\n",
      "\n",
      "- most_common_vp_chunks_seconde_structure, {<DT>?<JJ>*<NN><VB.*><RB.?>?} : \n",
      " [((('king', 'VBG'), ('hussein', 'NN')), 39), ((('lost', 'VBD'), ('city', 'NN')), 7), ((('king', 'VBG'), ('feisal', 'NN')), 7), ((('set', 'VBN'), ('forth', 'NN')), 7), ((('had', 'VBD'), ('time', 'NN')), 6), ((('emir', 'VB'), ('feisal', 'NN')), 5), ((('took', 'VBD'), ('part', 'NN')), 5), ((('is', 'VBZ'), ('a', 'DT'), ('man', 'NN')), 5), ((('sent', 'VBD'), ('word', 'NN')), 5), ((('king', 'VBG'), ('solomon', 'NN')), 5), ((('join', 'VB'), ('lawrence', 'NN')), 4), ((('said', 'VBD'), ('lawrence', 'NN')), 4), ((('replied', 'VBD'), ('lawrence', 'NN')), 3), ((('don', 'VBP'), ('’', 'JJ'), ('t', 'NN')), 3), ((('spend', 'VB'), ('the', 'DT'), ('night', 'NN')), 3), ((('join', 'VB'), ('feisal', 'NN')), 3), ((('was', 'VBD'), ('something', 'NN')), 3), ((('had', 'VBD'), ('an', 'DT'), ('opportunity', 'NN')), 3), ((('s', 'VBD'), ('palace', 'NN')), 3), ((('do', 'VB'), ('a', 'DT'), ('bit', 'NN')), 3), ((('had', 'VBD'), ('no', 'DT'), ('difficulty', 'NN')), 3), ((('resist', 'VB'), ('the', 'DT'), ('temptation', 'NN')), 3), ((('had', 'VBD'), ('the', 'DT'), ('opportunity', 'NN')), 3), ((('using', 'VBG'), ('this', 'DT'), ('ebook', 'NN')), 2), ((('gutenberg', 'VBZ'), ('ebook', 'NN')), 2), ((('t.', 'VBP'), ('e.', 'JJ'), ('lawrence', 'NN')), 2), ((('give', 'VB'), ('the', 'DT'), ('world', 'NN')), 2), ((('i', 'VBP'), ('a', 'DT'), ('modern', 'JJ'), ('arabian', 'JJ'), ('knight', 'NN')), 2), ((('xix', 'VBZ'), ('a', 'DT'), ('bedouin', 'JJ'), ('battle', 'NN')), 2), ((('xxiii', 'VBD'), ('a', 'DT'), ('cavalry', 'NN')), 2)]\n"
     ]
    }
   ],
   "source": [
    "# With Lawrence in Arabia, by Lowell Thomas\n",
    "\n",
    "\n",
    "print(\"With Lawrence in Arabia, by Lowell Thomas\\n\")\n",
    "# 1 ) Import and Preprocess Text Data\n",
    "from nltk import pos_tag, RegexpParser\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# import chosen text and convert it to lowercase\n",
    "text = open(\"With_lawrence_in_arabia.txt\",encoding='utf-8').read().lower()\n",
    "# sentence and word tokenized text (function \"word_sentence_tokenize\" above)\n",
    "word_tokenized_text = word_sentence_tokenize(text)\n",
    "\n",
    "# store and print any word tokenized sentence\n",
    "# let us see the word tokenized sentence 1945th\n",
    "single_word_tokenized_sentence = word_tokenized_text[1945]\n",
    "print(\"- single_word_tokenized_sentence (the 1945th): \\n\", single_word_tokenized_sentence) # check\n",
    "\n",
    "\n",
    "# 2 ) Part-of-speech (POS)  Tag Text\n",
    "# create a list to hold part-of-speech tagged sentences here\n",
    "pos_tagged_text = []\n",
    "\n",
    "# create a for loop through each word tokenized sentence here\n",
    "for word_tokenized_sentence in word_tokenized_text:\n",
    "  # part-of-speech tag each sentence and append to list of \"pos-tagged sentences\" with \n",
    "  # \"pos_tag()\" nltk built-in function\n",
    "    pos_tagged_text.append(pos_tag(word_tokenized_sentence))\n",
    "  \n",
    "\n",
    "# store and print any part-of-speech tagged sentence\n",
    "# let us see the pos-tagged sentence 1945th\n",
    "single_pos_sentence = pos_tagged_text[1945]\n",
    "print(\"\\n- single_pos_sentence  (the 1645th  ):\\n\", single_pos_sentence)\n",
    "\n",
    "\n",
    "# 3) Chunk Sentences (\"sens extraction\" based on noun or verb focus)\n",
    "# define noun phrase (np) chunk grammar\n",
    "np_chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "# create noun phrase RegexpParser object (RegexpParser object named \"np_chunk_parser\" \n",
    "# using the noun phrase np chunk grammar as argument)\n",
    "np_chunk_parser = RegexpParser(np_chunk_grammar)\n",
    "\n",
    "# define verb phrase chunk grammar\n",
    "vp_first_chunk_grammar = \"VP: {<VB.*><DT>?<JJ>*<NN><RB.?>?}\" # first structure\n",
    "vp_second_chunk_grammar = \"VP: {<DT>?<JJ>*<NN><VB.*><RB.?>?}\" # second structure\n",
    "\n",
    "# create verb phrase RegexpParser object (RegexpParser object named \"vp_chunk_parser\" \n",
    "#using the verb phrase vp chunk grammar as argument)\n",
    "vp_first_chunk_parser = RegexpParser(vp_first_chunk_grammar) # first structure\n",
    "vp_second_chunk_parser = RegexpParser(vp_second_chunk_grammar)  # second structure\n",
    "\n",
    "# create a list to hold noun phrase chunked sentences \n",
    "np_chunked_text = []\n",
    "# and a list to hold verb phrase chunked sentences\n",
    "vp_first_chunked_text = []  # first structure\n",
    "vp_second_chunked_text = [] # second structure\n",
    "\n",
    "# create a for loop through each pos-tagged sentence\n",
    "for pos_tagged_sentence in pos_tagged_text: # for every sentence where the words have been POSed\n",
    "  # chunk each sentence and append to lists (with np_chunk_parser‘s or \"vp_chunk_parser\" (RegexpParser object) \".parse()\" method and append to dedicated list)\n",
    "  np_chunked_text.append(np_chunk_parser.parse(pos_tagged_sentence)) # for noun focus\n",
    "  vp_first_chunked_text.append(vp_second_chunk_parser.parse(pos_tagged_sentence)) # for verb focus Second Structure\n",
    "  vp_second_chunked_text.append(vp_first_chunk_parser.parse(pos_tagged_sentence)) # for verb focus Second Structure\n",
    "\n",
    "    \n",
    "# 4 ) Analyze Chunks \n",
    "# store and print the most common NP-chunks (via \"np_chunk_counter function\" in the cell above)\n",
    "most_common_np_chunks = np_chunk_counter(np_chunked_text)\n",
    "print(\"\\n- most_common_np_chunks: \\n\", most_common_np_chunks)  # check\n",
    "\n",
    "# store and print the most common VP-chunks here\n",
    "most_common_vp_chunks_first_structure = vp_chunk_counter(vp_first_chunked_text)\n",
    "print(\"\\n- most_common_vp_chunks_first_structure, {<VB.*><DT>?<JJ>*<NN><RB.?>?} : \\n\", most_common_vp_chunks_first_structure)\n",
    "most_common_vp_chunks_seconde_structure = vp_chunk_counter(vp_second_chunked_text)\n",
    "print(\"\\n- most_common_vp_chunks_seconde_structure, {<DT>?<JJ>*<NN><VB.*><RB.?>?} : \\n\", most_common_vp_chunks_seconde_structure)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
