{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LANGUAGE PARSING TUTORIAL\n",
    "\n",
    "based on the courses of Codecademy\n",
    "\n",
    "\n",
    "### This part is only focusing on the parsing part, don't forget to do the pre-processing steps for real-text application. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing is a stage of NLP concerned with **breaking / segmenting** up text **based on syntax**\n",
    "\n",
    "By using Python’s _regular expression_ modulere and the _Natural Language Toolkit_, known as _NLTK_, you can find:    \n",
    "* keywords of interest\n",
    "* discover where and how often they are used \n",
    "* discern the parts-of-speech patterns in which they appear to understand the sometimes hidden meaning in a piece of writing\n",
    "\n",
    "> ex:  highlight the biases of its author or uncover additional insights that even a deep, rigorous reading of the text might not reveal\n",
    "\n",
    "### MENU: \n",
    "\n",
    "\n",
    "1) Compiling and Matching\n",
    "\n",
    "\n",
    "2) Searching and Finding\n",
    "\n",
    "3) Part-of-Speech Tagging\n",
    "\n",
    "4) Introduction to Chunking\n",
    "   - 4 - 1) Chunking Noun Phrases\n",
    "   - 4 - 2) Chunking Verb Phrases¶\n",
    "   - 4 - 3) Chunk filtering\n",
    "\n",
    "5) REVIEW "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Compiling and Matching\n",
    "\n",
    "* **.compile( )** :  \n",
    "This method takes a regular expression pattern as an argument and compiles the pattern into a regular expression object, which you can later use to find matching text\n",
    "\n",
    "* **.match( )** :   \n",
    "method that takes a string of text as an argument and looks for a single match to the regular expression that starts at the beginning of the string\n",
    "    * finds a match that starts at the beginning of the string, it will return a match object\n",
    "    * If there is no match, .match() will return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regular_expression:  re.compile('[A-Za-z]{7}')\n",
      "result_1:  <_sre.SRE_Match object; span=(0, 7), match='Dorothy'>\n",
      "match_1:  Dorothy\n",
      "result_2:  None\n",
      "result_2:  None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# characters are defined\n",
    "character_1 = \"Dorothy\"\n",
    "character_2 = \"Henry\"\n",
    "\n",
    "# regular expression object that match any 7 character string later usable to find matching text\n",
    "regular_expression = re.compile(\"[A-Za-z]{7}\") #  recognizes upper or lower case characters\n",
    "print(\"regular_expression: \", regular_expression)\n",
    "\n",
    "# check for a match to character_1 here\n",
    "# (check if regex matches string stored in character_1)\n",
    "result_1 = regular_expression.match(character_1)\n",
    "print(\"result_1: \", result_1)\n",
    "\n",
    "\n",
    "# store and print the matched text here \n",
    "match_1 = result_1.group(0)\n",
    "print(\"match_1: \", match_1)\n",
    "\n",
    "# compile a regular expression to match a 7 character string of word characters\n",
    "# and check for a match to character_2 here\n",
    "regular_expression = re.compile(\"[A-Za-z]{7}\") # search for something len = 7\n",
    "result_2 = regular_expression.match(character_2) # but is len = 5\n",
    "print(\"result_2: \", result_2) # so nothing is found => None\n",
    "# 2 last lines above can be written in one line also\n",
    "result_2 = re.match(\"[A-Za-z]{7}\", character_2)\n",
    "print(\"result_2: \", result_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Searching and Finding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **.search( )** :   \n",
    "Will look left to right through an entire piece of text and return a match object for the first match to the regular expression given.   \n",
    "If no match is found, _.search()_ will return None (unlike **.match( )** which will only find matches at the start of a string).\n",
    ">ex: **result = re.search(\"\\w{8}\",\"Are you a Munchkin?\")** search for a sequence of 8 word characters in the string \"Are you a Munchkin?\" (that will be \"Munchkin\")\n",
    "\n",
    "* **.findall( )** :  \n",
    "Finds all the occurrences of a word or keyword in a piece of text.  \n",
    "Will return a list of all non-overlapping matches of the regular expression in the string   \n",
    "(can be useful to find all the occurrences of a word or keyword in a piece of text to determine a frequency count).\n",
    ">ex:   \n",
    ". text  = *\"Everything is green here, while in the country of the Munchkins blue was the favorite color. But the people do not seem to be as friendly as the Munchkins, and I'm afraid we shall be unable to find a place to pass the night.\"*   \n",
    ". list_of_matches = re.findall(\"\\w{8}\",text)  \n",
    ". returns the list:  *['Everythi', 'Munchkin', 'favorite', 'friendly', 'Munchkin']*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found_wizard:  <_sre.SRE_Match object; span=(14, 20), match='wizard'>\n",
      "\n",
      " all_lions_list:  ['lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion']\n",
      "\n",
      " number_lions:  183\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# import L. Frank Baum's The Wonderful Wizard of Oz\n",
    "oz_text = open(\"the_wizard_of_oz_text.txt\",encoding='utf-8').read().lower()\n",
    "\n",
    "# search oz_text for an occurrence of 'wizard' here\n",
    "found_wizard = re.search(\"wizard\",oz_text)\n",
    "print(\"found_wizard: \", found_wizard)\n",
    "\n",
    "# find all the occurrences of 'lion' in oz_text here\n",
    "all_lions = re.findall(\"lion\", oz_text)\n",
    "print(\"\\n all_lions_list: \", all_lions)\n",
    "\n",
    "# store and print the length of all_lions here\n",
    "number_lions =  len(all_lions)\n",
    "print(\"\\n number_lions: \", number_lions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">ex:  \n",
    "\"Wow! Ramona and her class are happily studying the new textbook she has on NLP.\"\n",
    "\n",
    "\n",
    "* **Noun**: the name of a person (Ramona,class), place, thing (textbook), or idea (NLP) = **NN** \n",
    "\n",
    "\n",
    "* **Pronoun**: a word used in place of a noun (her,she) = **PRP** \n",
    "\n",
    "\n",
    "* **Determiner**: a word that introduces, or “determines”, a noun (the) = **DT** \n",
    "\n",
    "\n",
    "* **Verb**: expresses action (studying) or being (are,has) = **VBP**\n",
    "    * **VBD** is a past tense verb\n",
    "    * **VBN** is a past particle verb\n",
    "\n",
    "\n",
    "* **Adjective**: modifies or describes a noun or pronoun (new) = **JJ** \n",
    "\n",
    "\n",
    "* **Adverb**: modifies or describes a verb, an adjective, or another adverb (happily) = **RB** \n",
    "\n",
    "\n",
    "* **Preposition**: a word placed before a noun or pronoun to form a phrase modifying another word in the sentence (on) = **IN** \n",
    "\n",
    "\n",
    "* **Conjunction**: a word that joins words, phrases, or clauses (and) = **CC** \n",
    "\n",
    " \n",
    "* **Interjection**: a word used to express emotion (Wow) = **UH** \n",
    "\n",
    "\n",
    "* **pos_tag()** function:   \n",
    "Automates the part-of-speech tagging process.  \n",
    "The function takes one argument, a list of words in the order they appear in a sentence, and returns a list of tuples, where the first entry in the tuple is a word and the second is the part-of-speech tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100th tokenized sentence: \"the house must have fallen on her.\n",
      "\n",
      "sentence number  18 :  [('the', 'DT'), ('sun', 'NN'), ('and', 'CC'), ('wind', 'NN'), ('had', 'VBD'), ('changed', 'VBN'), ('her', 'PRP'), (',', ','), ('too', 'RB'), ('.', '.')]\n",
      "\n",
      "sentence number  19 :  [('they', 'PRP'), ('had', 'VBD'), ('taken', 'VBN'), ('the', 'DT'), ('sparkle', 'NN'), ('from', 'IN'), ('her', 'PRP$'), ('eyes', 'NNS'), ('and', 'CC'), ('left', 'VBD'), ('them', 'PRP'), ('a', 'DT'), ('sober', 'JJ'), ('gray', 'NN'), (';', ':'), ('they', 'PRP'), ('had', 'VBD'), ('taken', 'VBN'), ('the', 'DT'), ('red', 'JJ'), ('from', 'IN'), ('her', 'PRP$'), ('cheeks', 'NN'), ('and', 'CC'), ('lips', 'NNS'), (',', ','), ('and', 'CC'), ('they', 'PRP'), ('were', 'VBD'), ('gray', 'JJ'), ('also', 'RB'), ('.', '.')]\n",
      "\n",
      "Words_toke_100th_sentence:  [('``', '``'), ('the', 'DT'), ('house', 'NN'), ('must', 'MD'), ('have', 'VB'), ('fallen', 'VBN'), ('on', 'IN'), ('her', 'PRP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag # automates the part-of-speech tagging process\n",
    "from nltk.tokenize import word_tokenize # word tokenization\n",
    "from nltk.tokenize import sent_tokenize # sentence tokenization\n",
    "# import L. Frank Baum's The Wonderful Wizard of Oz\n",
    "oz_text = open(\"the_wizard_of_oz_text.txt\",encoding='utf-8').read().lower()\n",
    "\n",
    "# tokenization\n",
    "sentences_tokenized = sent_tokenize(oz_text) # sentence tokenization\n",
    "\n",
    "#100th sentence from the tokenized text above\n",
    "sentence_tokenized_100th = sentences_tokenized[100]\n",
    "print(\"100th tokenized sentence:\", sentence_tokenized_100th)\n",
    "\n",
    "# list bellow will contain all the token from the different sentences\n",
    "tokenized_words_text = [] # an empty list\n",
    "\n",
    "# for loop will token the words of every sentence separately\n",
    "for sentence in sentences_tokenized:\n",
    "    # tokenization of every sentence (so all words inside it)\n",
    "    word_sentence_toke = word_tokenize(sentence)\n",
    "    # POS tagging for every token (word) from a sentence \n",
    "    words_toke_for_all_sentences = pos_tag(word_sentence_toke)\n",
    "    # POS tagging for every token added to list all\n",
    "    tokenized_words_text.append(words_toke_for_all_sentences) # tokenized words + POS \n",
    "\n",
    "# print 18th to 19th sentence that is tokenized \n",
    "for n in range(18, 20): \n",
    "    print(\"\\nsentence number \", n, \": \", tokenized_words_text[n])\n",
    "\n",
    "words_toke_100th_sentence = tokenized_words_text[100]\n",
    "print(\"\\nWords_toke_100th_sentence: \",words_toke_100th_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Everything', 'NN'), ('was', 'VBD'), ('beautiful', 'JJ'), ('and', 'CC'), ('nothing', 'NN'), ('hurt', 'NN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag # automates the part-of-speech tagging process\n",
    "from nltk.tokenize import word_tokenize # word tokenization\n",
    "from nltk.tokenize import sent_tokenize # sentence tokenization\n",
    "\n",
    "sentence_to_POS = \"Everything was beautiful and nothing hurt.\"\n",
    "\n",
    "# list bellow will contain all the token from the different sentences\n",
    "POS_text = [] # an empty list\n",
    "\n",
    "# no for loop is needed because we only have one sentence to do POS recognition\n",
    "sentence_toke = word_tokenize(sentence_to_POS)\n",
    "# POS tagging for every token (word) from a sentence \n",
    "POS_of_a_word = pos_tag(sentence_toke)\n",
    "# POS tagging for every token added to list all\n",
    "POS_text.append(POS_of_a_word) # tokenized words + POS \n",
    "\n",
    "\n",
    "print(POS_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Introduction to Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **chunking** = technique of grouping words by their part-of-speech (POS) tag.  \n",
    "\n",
    "Given your part-of-speech tagged text, you can now use regular expressions to find patterns in sentence structure that give insight into the meaning of a text.\n",
    "\n",
    "### _______________\n",
    "\n",
    "The regular expression we build to find chunks is called chunk grammar. A piece of chunk grammar can be written as follows:\n",
    "\n",
    "chunk_grammar = \" AN : {< JJ >< NN >} \"\n",
    "\n",
    "    - AN is a user-defined name for the kind of chunk you are searching for. \n",
    "    We can use whatever name makes sense given our chunk grammar. In this case AN stands for adjective-noun\n",
    "    - A pair of curly braces {} surround the actual chunk grammar\n",
    "    - <JJ> operates similarly to a regex character class, matching any adjective\n",
    "    - <NN> matches any noun, singular or plural\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- chunk_parser looks like: \n",
      " chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<JJ><NN>'>\n",
      "\n",
      "- POS tagged sentence of interesst:  [('``', '``'), ('where', 'WRB'), ('is', 'VBZ'), ('the', 'DT'), ('emerald', 'JJ'), ('city', 'NN'), ('?', '.'), (\"''\", \"''\")]\n",
      "\n",
      "- Chunking-Parsed sentence ('scaredy_cat'):  (S ``/`` where/WRB is/VBZ the/DT (AN emerald/JJ city/NN) ?/. ''/'') \n",
      "\n",
      "- In form of a tree 'scaredy_cat' sentence : \n",
      "\n",
      "                         S                                    \n",
      "   ______________________|__________________________           \n",
      "  |       |       |      |     |    |               AN        \n",
      "  |       |       |      |     |    |        _______|_____     \n",
      "``/`` where/WRB is/VBZ the/DT ?/. ''/'' emerald/JJ     city/NN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------- this part is just to prepare the tokenized sentences ---------------\n",
    "from nltk.tokenize import word_tokenize # word tokenization\n",
    "from nltk.tokenize import sent_tokenize # sentence tokenization\n",
    "from nltk import pos_tag # automates the part-of-speech tagging process\n",
    "# import L. Frank Baum's The Wonderful Wizard of Oz\n",
    "oz_text = open(\"the_wizard_of_oz_text.txt\",encoding='utf-8').read().lower()\n",
    "# tokenization\n",
    "sentences_tokenized = sent_tokenize(oz_text) # sentence tokenization\n",
    "# list bellow will contain all the token from the different sentences\n",
    "tokenized_words_text = [] # an empty list\n",
    "# for loop will token the words of every sentence separately\n",
    "for sentence in sentences_tokenized:\n",
    "    # tokenization of every sentence (so all words inside it)\n",
    "    word_sentence_toke = word_tokenize(sentence)\n",
    "    # POS tagging for every token (word) from a sentence \n",
    "    words_toke_for_all_sentences = pos_tag(word_sentence_toke)\n",
    "    # POS tagging for every token added to list all\n",
    "    tokenized_words_text.append(words_toke_for_all_sentences) # tokenized words + POS \n",
    "# --------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# the code starts here bellow\n",
    "\n",
    "from nltk import RegexpParser, Tree\n",
    "\n",
    "# -- define adjective-noun chunk grammar -- \n",
    "# - AN is a user-defined name for the kind of chunk you are searching for. \n",
    "#   You can use whatever name makes sense given your chunk grammar. \n",
    "#   In this case AN stands for adjective-noun\n",
    "# - <JJ> matches any adjective (operates similarly to a regex character class, matching any adjective)\n",
    "# - <NN> matches any noun, singular or plural\n",
    "# - {} surrounds the chunk grammar\n",
    "chunk_grammar = \"AN: {<JJ><NN>}\"  # will match any adjective JJ that is followed by a noun NN\n",
    "\n",
    "# -- create \"RegexpParser\" object --\n",
    "# - To use the chunk grammar defined, we must create a nltk \"RegexpParser\" object and \n",
    "#   give it a piece of chunk grammar (\"chunk_grammar\") as an argument\n",
    "chunk_parser = RegexpParser(chunk_grammar) # chunk_grammar = argument\n",
    "print(\"- chunk_parser looks like: \\n\", chunk_parser)\n",
    "\n",
    "# chunk the POS tagged sentence at index 282 in \"tokenized_words_text\"\n",
    "print(\"\\n- POS tagged sentence of interesst: \", tokenized_words_text[282])\n",
    "scaredy_cat = chunk_parser.parse(tokenized_words_text[282])\n",
    "print(\"\\n- Chunking-Parsed sentence ('scaredy_cat'): \", scaredy_cat, \"\\n\")\n",
    "\n",
    "# pretty_print the chunked sentence\n",
    "print(\"- In form of a tree 'scaredy_cat' sentence : \\n\")\n",
    "Tree.fromstring(str(scaredy_cat)).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - 1) Chunking Noun Phrases\n",
    "\n",
    "\n",
    "**Types of chunking** that are linguistically helpful for determining *meaning* and *bias* in a piece of text.   \n",
    "A **noun phrase** is a phrase that contains _a noun_ and operates, as a unit, as a noun.\n",
    "\n",
    "\n",
    "\n",
    "* A popular form of noun phrase begins with a determiner **DT** (which specifies the noun being referenced),   followed by any number of adjectives **JJ** (which describe the noun) and ends with a noun **NN**.\n",
    "\n",
    "> ex : The POS-ed sentence :  \n",
    "[('we', 'PRP'), ('are', 'VBP'), ('so', 'RB'), ('grateful', 'JJ'), ('to', 'TO'), ('you', 'PRP'), ('for', 'IN'), ('having', 'VBG'), ('killed', 'VBN'), ('the', '**DT**'), ('wicked', '**JJ**'), ('witch', '**NN**'), ('of', 'IN'), ('the', '**DT**'), ('east', '**NN**'), (',', ','), ('and', 'CC'), ('for', 'IN'), ('setting', 'VBG'), ('our', 'PRP'), ('people', 'NNS'), ('free', 'VBP'), ('from', 'IN'), ('bondage', 'NN'), ('.', '.')]  \n",
    "Has 2 Chunking Noun Phrases:\n",
    " - (('the', 'DT'), ('wicked', 'JJ'), ('witch', 'NN'))\n",
    " - (('the', 'DT'), ('east', 'NN'))\n",
    "\n",
    "### ______\n",
    "\n",
    "\n",
    "We can easily find all the non-overlapping noun phrases in a piece of text.  \n",
    "Just like in normal regular expressions, we can use quantifiers to indicate how many of each POS we want to match.\n",
    "\n",
    "The chunk grammar for a noun phrase (**NP-chunks**) can be written as follows :\n",
    "\n",
    "chunk_grammar = \" NP : {< DT > ? < JJ > * < NN >} \"\n",
    "\n",
    "    - NP is the user-defined name of the chunk we are searching for. In this case NP stands for noun phrase\n",
    "    - <DT> matches any determiner\n",
    "    - ? is an optional quantifier, matching either 0 or 1 determiners\n",
    "    - <JJ> matches any adjective\n",
    "    - * is the Kleene star quantifier, matching 0 or more occurrences of an adjective\n",
    "    - <NN> matches any noun, singular or plural\n",
    "    \n",
    "\n",
    "**->** Via **NP-chunks** in a text, we can perform :\n",
    "\n",
    "* a frequency analysis and identify important, recurring noun phrases\n",
    "* identify pseudo-topics and tag articles and documents by their highest count NP-chunks\n",
    "* analyzing the adjective choices an author makes for different nouns\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here below we can see the distribution of words but also for small NP-Chunks sentences: \n",
      "\n",
      "[((('i', 'NN'),), 326), ((('dorothy', 'NN'),), 222), ((('the', 'DT'), ('scarecrow', 'NN')), 213), ((('the', 'DT'), ('lion', 'NN')), 148), ((('the', 'DT'), ('tin', 'NN')), 123), ((('woodman', 'NN'),), 112), ((('oz', 'NN'),), 86), ((('toto', 'NN'),), 73), ((('head', 'NN'),), 59), ((('the', 'DT'), ('woodman', 'NN')), 59), ((('the', 'DT'), ('wicked', 'JJ'), ('witch', 'NN')), 58), ((('the', 'DT'), ('emerald', 'JJ'), ('city', 'NN')), 51), ((('the', 'DT'), ('witch', 'NN')), 49), ((('the', 'DT'), ('girl', 'NN')), 46), ((('the', 'DT'), ('road', 'NN')), 41), ((('room', 'NN'),), 29), ((('nothing', 'NN'),), 29), ((('the', 'DT'), ('air', 'NN')), 29), ((('the', 'DT'), ('country', 'NN')), 26), ((('the', 'DT'), ('land', 'NN')), 24), ((('a', 'DT'), ('heart', 'NN')), 24), ((('the', 'DT'), ('west', 'NN')), 23), ((('axe', 'NN'),), 23), ((('the', 'DT'), ('sun', 'NN')), 22), ((('the', 'DT'), ('little', 'JJ'), ('girl', 'NN')), 22), ((('course', 'NN'),), 22), ((('the', 'DT'), ('cowardly', 'JJ'), ('lion', 'NN')), 21), ((('aunt', 'NN'),), 21), ((('the', 'DT'), ('house', 'NN')), 21), ((('the', 'DT'), ('door', 'NN')), 21)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Words alone : ((('i', 'NN'),), 326),   ((('dorothy', 'NN'),), 222)    Couple words : ((('the', 'DT'), ('scarecrow', 'NN')), 213),    ((('the', 'DT'), ('lion', 'NN')), 148),   \\n   ((('the', 'DT'), ('door', 'NN')), 21) \\\\ \\n    3 words coming together : ((('the', 'DT'), ('cowardly', 'JJ'), ('lion', 'NN')), 21)\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------- this part is just to prepare the tokenized sentences ---------------\n",
    "from nltk.tokenize import word_tokenize # word tokenization\n",
    "from nltk.tokenize import sent_tokenize # sentence tokenization\n",
    "from nltk import pos_tag # automates the part-of-speech tagging process\n",
    "# import L. Frank Baum's The Wonderful Wizard of Oz\n",
    "oz_text = open(\"the_wizard_of_oz_text.txt\",encoding='utf-8').read().lower()\n",
    "# tokenization\n",
    "sentences_tokenized = sent_tokenize(oz_text) # sentence tokenization\n",
    "# list bellow will contain all the token from the different sentences\n",
    "tokenized_words_text = [] # an empty list\n",
    "# for loop will token the words of every sentence separately\n",
    "for sentence in sentences_tokenized:\n",
    "    # tokenization of every sentence (so all words inside it)\n",
    "    word_sentence_toke = word_tokenize(sentence)\n",
    "    # POS tagging for every token (word) from a sentence \n",
    "    words_toke_for_all_sentences = pos_tag(word_sentence_toke)\n",
    "    # POS tagging for every token added to list all\n",
    "    tokenized_words_text.append(words_toke_for_all_sentences) # tokenized words + POS \n",
    "# --------------------------------\n",
    "\n",
    "\n",
    "# the code starts here\n",
    "from nltk import RegexpParser\n",
    "from np_chunk_counter import np_chunk_counter\n",
    "\n",
    "# define noun-phrase chunk grammar \n",
    "chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "# create RegexpParser object \n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    "\n",
    "# create a list to hold noun-phrase chunked sentences\n",
    "np_chunked_oz = list()\n",
    "\n",
    "# create a for loop through each pos-tagged sentence in pos_tagged_oz here\n",
    "for pos_tagged_sentence in tokenized_words_text:\n",
    "  # chunk each sentence and append to np_chunked_oz here\n",
    "  np_chunked_oz.append(chunk_parser.parse(pos_tagged_sentence))\n",
    "\n",
    "# store and print the 30 most common np-chunks from a list of chunked sentences\n",
    "most_common_np_chunks = np_chunk_counter(np_chunked_oz)\n",
    "print(\"Here below we can see the distribution of words but also for small NP-Chunks sentences: \\n\")\n",
    "print(most_common_np_chunks)\n",
    "\n",
    "'''Words alone : ((('i', 'NN'),), 326),   ((('dorothy', 'NN'),), 222) \\\n",
    "   Couple words : ((('the', 'DT'), ('scarecrow', 'NN')), 213),    ((('the', 'DT'), ('lion', 'NN')), 148),   \n",
    "   ((('the', 'DT'), ('door', 'NN')), 21) \\ \n",
    "    3 words coming together : ((('the', 'DT'), ('cowardly', 'JJ'), ('lion', 'NN')), 21)'''\n",
    "    \n",
    "# Analysis :\n",
    "# ((('the', 'DT'),('lion', 'NN')), 148) = lion mentioned more often \n",
    "# ((('the', 'DT'), ('tin', 'NN')), 123) = lin mentioned less often "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - 2) Chunking Verb Phrases\n",
    "\n",
    "Another popular type of chunking is **VP-chunking**, or verb phrase chunking. \n",
    "* A verb phrase is a phrase that contains a verb and its complements, objects, or modifiers.\n",
    "\n",
    "The *first structure* begins with a verb VB of any tense, followed by a noun phrase, and ends with an optional adverb RB of any form.   \n",
    "The *second structure* switches the order of the verb and the noun phrase, but also ends with an optional adverb.\n",
    "\n",
    ">ex : For example, consider the part-of-speech tagged verb phrases given below:  \n",
    "- (('said', 'VBD'), ('the', 'DT'), ('cowardly', 'JJ'), ('lion', 'NN')) -> *first structure*\n",
    "- ('the', 'DT'), ('cowardly', 'JJ'), ('lion', 'NN')), (('said', 'VBD') -> *second structure*\n",
    "\n",
    "###  __________\n",
    "\n",
    "* The chunk grammar to find the *first form* of verb phrase is given below:  \n",
    "  *  chunk_grammar = \" VP: {< VB. *> < DT > ? < JJ > * < NN > < RB.? > ? } \"\n",
    "  \n",
    "  \n",
    "* The chunk grammar for the *second form* of verb phrase is given below:\n",
    "   * chunk_grammar = \" VP: { < DT> ? < JJ > * < NN > < VB.* > < RB.? > ? } \"\n",
    "\n",
    "And we have:\n",
    "\n",
    "    - VP is the user-defined name of the chunk you are searching for. In this case VP stands for verb phrase\n",
    "    - <VB.*> matches any verb using the . as a wildcard and the * quantifier to match 0 or more occurrences  of any character. This ensures matching verbs of any tense (ex. VB for present tense, VBD for past tense,\n",
    "    or VBN for past participle)\n",
    "    (- * is the Kleene star quantifier, matching 0 or more occurrences of an adjective)\n",
    "    - <DT>?<JJ>*<NN> matches any noun phrase\n",
    "    - <RB.?> matches any adverb using the . as a wildcard and the optional quantifier to match 0 or\n",
    "    1 occurrence of any character. This ensures matching any form of adverb (regular RB, comparative RBR, or\n",
    "    superlative RBS)\n",
    "    - ? is an optional quantifier, matching either 0 or 1 adverbs\n",
    "\n",
    "\n",
    "\n",
    "* **VP-chunks** :  \n",
    "Just like with NP-chunks, we can find all them in a text and perform a frequency analysis to identify **important, recurring verb phrases**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First structure : \n",
      "[((('said', 'VBD'), ('the', 'DT'), ('scarecrow', 'NN')), 33), ((('said', 'VBD'), ('dorothy', 'NN')), 31), ((('asked', 'VBN'), ('dorothy', 'NN')), 20), ((('said', 'VBD'), ('the', 'DT'), ('tin', 'NN')), 19), ((('said', 'VBD'), ('the', 'DT'), ('lion', 'NN')), 15), ((('said', 'VBD'), ('the', 'DT'), ('girl', 'NN')), 10), ((('asked', 'VBN'), ('the', 'DT'), ('scarecrow', 'NN')), 10), ((('answered', 'VBD'), ('the', 'DT'), ('scarecrow', 'NN')), 8), ((('said', 'VBD'), ('the', 'DT'), ('cowardly', 'JJ'), ('lion', 'NN')), 8), ((('said', 'VBD'), ('oz', 'NN')), 8), ((('said', 'VBD'), ('the', 'DT'), ('woodman', 'NN')), 7), ((('pass', 'VB'), ('the', 'DT'), ('night', 'NN')), 6), ((('asked', 'VBN'), ('the', 'DT'), ('girl', 'NN')), 6), ((('see', 'VB'), ('the', 'DT'), ('great', 'JJ'), ('oz', 'NN')), 6), ((('answered', 'VBD'), ('oz', 'NN')), 6), ((('replied', 'VBD'), ('oz', 'NN')), 6), ((('cried', 'VBN'), ('dorothy', 'NN')), 5), ((('asked', 'VBN'), ('the', 'DT'), ('tin', 'NN')), 5), ((('asked', 'VBN'), ('the', 'DT'), ('lion', 'NN')), 5), ((('remarked', 'VBD'), ('the', 'DT'), ('lion', 'NN')), 5), ((('answered', 'VBD'), ('dorothy', 'NN')), 5), ((('replied', 'VBD'), ('the', 'DT'), ('lion', 'NN')), 5), ((('killed', 'VBN'), ('the', 'DT'), ('wicked', 'JJ'), ('witch', 'NN')), 4), ((('said', 'VBD'), ('the', 'DT'), ('witch', 'NN')), 4), ((('replied', 'VBD'), ('the', 'DT'), ('scarecrow', 'NN')), 4), ((('answered', 'VBD'), ('the', 'DT'), ('girl', 'NN')), 4), ((('said', 'VBD'), ('the', 'DT'), ('farmer', 'NN')), 4), ((('thought', 'VBD'), ('i', 'NN')), 4), ((('answered', 'VBD'), ('the', 'DT'), ('woodman', 'NN')), 4), ((('have', 'VBP'), ('no', 'DT'), ('heart', 'NN')), 4)]\n",
      "\n",
      "Second structure : \n",
      "[((('i', 'NN'), ('am', 'VBP')), 24), ((('i', 'NN'), ('was', 'VBD')), 17), ((('dorothy', 'NN'), ('was', 'VBD')), 13), ((('i', 'NN'), ('have', 'VBP')), 9), ((('i', 'NN'), ('know', 'VBP')), 8), ((('i', 'NN'), ('had', 'VBD')), 8), ((('dorothy', 'NN'), ('had', 'VBD')), 7), ((('oz', 'NN'), ('had', 'VBD')), 7), ((('the', 'DT'), ('scarecrow', 'NN'), ('said', 'VBD')), 6), ((('i', 'NN'), ('am', 'VBP'), ('oz', 'RB')), 6), ((('oz', 'NN'), ('was', 'VBD')), 6), ((('toto', 'NN'), ('did', 'VBD'), ('not', 'RB')), 5), ((('dorothy', 'NN'), ('looked', 'VBD')), 5), ((('i', 'NN'), ('have', 'VBP'), ('never', 'RB')), 5), ((('the', 'DT'), ('wicked', 'JJ'), ('witch', 'NN'), ('had', 'VBD')), 5), ((('i', 'NN'), (\"'m\", 'VBP')), 5), ((('i', 'NN'), ('do', 'VBP'), (\"n't\", 'RB')), 5), ((('i', 'NN'), ('want', 'VBP')), 5), ((('the', 'DT'), ('scarecrow', 'NN'), ('had', 'VBD')), 5), ((('the', 'DT'), ('balloon', 'NN'), ('was', 'VBD')), 4), ((('the', 'DT'), ('cyclone', 'NN'), ('had', 'VBD')), 4), ((('i', 'NN'), ('came', 'VBD')), 4), ((('the', 'DT'), ('wicked', 'JJ'), ('witch', 'NN'), ('was', 'VBD')), 4), ((('dorothy', 'NN'), ('did', 'VBD'), ('not', 'RB')), 4), ((('woodman', 'NN'), ('had', 'VBD')), 4), ((('i', 'NN'), ('suppose', 'VBP')), 4), ((('the', 'DT'), ('lion', 'NN'), ('was', 'VBD')), 4), ((('toto', 'NN'), ('had', 'VBD')), 4), ((('oz', 'NN'), ('has', 'VBZ')), 4), ((('dorothy', 'NN'), ('found', 'VBD')), 3)]\n"
     ]
    }
   ],
   "source": [
    "# -------------- this part is just to prepare the tokenized sentences ---------------\n",
    "from nltk.tokenize import word_tokenize # word tokenization\n",
    "from nltk.tokenize import sent_tokenize # sentence tokenization\n",
    "from nltk import pos_tag # automates the part-of-speech tagging process\n",
    "# import L. Frank Baum's The Wonderful Wizard of Oz\n",
    "oz_text = open(\"the_wizard_of_oz_text.txt\",encoding='utf-8').read().lower()\n",
    "# tokenization\n",
    "sentences_tokenized = sent_tokenize(oz_text) # sentence tokenization\n",
    "# list bellow will contain all the token from the different sentences\n",
    "tokenized_words_text = [] # an empty list\n",
    "# for loop will token the words of every sentence separately\n",
    "for sentence in sentences_tokenized:\n",
    "    # tokenization of every sentence (so all words inside it)\n",
    "    word_sentence_toke = word_tokenize(sentence)\n",
    "    # POS tagging for every token (word) from a sentence \n",
    "    words_toke_for_all_sentences = pos_tag(word_sentence_toke)\n",
    "    # POS tagging for every token added to list all\n",
    "    tokenized_words_text.append(words_toke_for_all_sentences) # tokenized words + POS \n",
    "# --------------------------------\n",
    "\n",
    "\n",
    "# code starts here\n",
    "# ------------  first structure  ------------------\n",
    "print(\"\\nFirst structure : \")\n",
    "from nltk import RegexpParser\n",
    "from vp_chunk_counter import vp_chunk_counter\n",
    "\n",
    "# define verb phrase chunk grammar (first form of verb phrase)\n",
    "chunk_grammar = \"VP: {<VB.*><DT>?<JJ>*<NN><RB.?>?} \" # first structure\n",
    "\n",
    "# create RegexpParser object here\n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    "\n",
    "# create a list to hold verb-phrase chunked sentences\n",
    "vp_chunked_oz = list()\n",
    "\n",
    "# create for loop through each pos-tagged sentence in pos_tagged_oz here\n",
    "for pos_tagged_sentence in tokenized_words_text:\n",
    "  # chunk each sentence and append to vp_chunked_oz here\n",
    "  vp_chunked_oz.append(chunk_parser.parse(pos_tagged_sentence))\n",
    "  \n",
    "# store and print the most common vp-chunks here\n",
    "most_common_vp_chunks = vp_chunk_counter(vp_chunked_oz)\n",
    "print(most_common_vp_chunks)\n",
    "\n",
    "# Analysis : \n",
    "# ((('said', 'VBD'), ('the', 'DT'), ('tin', 'NN')), 19) = tin speaks more \n",
    "# ((('said', 'VBD'), ('the', 'DT'), ('lion', 'NN')), 15) = lion speaks less\n",
    "\n",
    "\n",
    "# ------------  second structure  ------------------\n",
    "print(\"\\nSecond structure : \")\n",
    "# update the grammar to find a verb phrase of the following form: noun phrase, followed by a verb VB, followed by an optional adverb RB\n",
    "chunk_grammar = \"VP: {<DT>?<JJ>*<NN><VB.*><RB.?>?}\" # second structure\n",
    "# create RegexpParser object here\n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    "\n",
    "# create a list to hold verb-phrase chunked sentences\n",
    "vp_chunked_oz = list()\n",
    "\n",
    "# create for loop through each pos-tagged sentence in pos_tagged_oz here\n",
    "for pos_tagged_sentence in tokenized_words_text:\n",
    "  # chunk each sentence and append to vp_chunked_oz here\n",
    "  vp_chunked_oz.append(chunk_parser.parse(pos_tagged_sentence))\n",
    "  \n",
    "# store and print the most common vp-chunks here\n",
    "most_common_vp_chunks = vp_chunk_counter(vp_chunked_oz)\n",
    "print(most_common_vp_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - 3) Chunk filtering \n",
    "Chunk filtering lets you define what parts of speech you do not want in a chunk and remove them.\n",
    "\n",
    "A popular method for performing chunk filtering: chunk an entire sentence together and then indicate which parts of speech are to be filtered out.  \n",
    "If the filtered parts of speech are in the middle of a chunk, it will split the chunk into two separate chunks  \n",
    "\n",
    "\n",
    "The chunk grammar to perform chunk filtering is given just below:\n",
    "\n",
    ">ex: chunk_grammar = \"\"\" NP: {<.*>+}\n",
    "                       }<VB.?|IN>+{ \"\"\"\n",
    "\n",
    "    - NP is the user-defined name of the chunk you are searching for.   \n",
    "    In this case NP stands for noun phrase\n",
    "    - The brackets {} indicate what parts of speech we are chunking.   \n",
    "    <.*>+ matches every part of speech in the sentence\n",
    "    - The inverted brackets }{ indicate which parts of speech you want to filter from the chunk.   \n",
    "    <VB.?|IN>+ will filter out any verbs or prepositions\n",
    "    \n",
    "\n",
    "* **Chunk filtering**:  \n",
    "  provides an alternate way for you to search through a text and find the chunks of information useful for your   analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- chunk an entire sentence together: \n",
      "\n",
      "(S\n",
      "  (Chunk\n",
      "    then/RB\n",
      "    she/PRP\n",
      "    sat/VBD\n",
      "    upon/IN\n",
      "    a/DT\n",
      "    settee/NN\n",
      "    and/CC\n",
      "    watched/VBD\n",
      "    the/DT\n",
      "    people/NNS\n",
      "    dance/NN\n",
      "    ./.))\n",
      "\n",
      "- chunk a noun phrase using chunk filtering: \n",
      "\n",
      "(S\n",
      "  (NP then/RB she/PRP)\n",
      "  sat/VBD\n",
      "  upon/IN\n",
      "  (NP a/DT settee/NN and/CC)\n",
      "  watched/VBD\n",
      "  (NP the/DT people/NNS dance/NN ./.))\n",
      "\n",
      "In form of a tree : \n",
      "\n",
      "                                                 S                                                  \n",
      "    _____________________________________________|_______________________________                    \n",
      "   |       |         |               NP                  NP                      NP                 \n",
      "   |       |         |          _____|_____       _______|_______        ________|________________   \n",
      "sat/VBD upon/IN watched/VBD then/RB     she/PRP a/DT settee/NN and/CC the/DT people/NNS dance/NN ./.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------- this part is just to prepare the tokenized sentences ---------------\n",
    "from nltk.tokenize import word_tokenize # word tokenization\n",
    "from nltk.tokenize import sent_tokenize # sentence tokenization\n",
    "from nltk import pos_tag # automates the part-of-speech tagging process\n",
    "# import L. Frank Baum's The Wonderful Wizard of Oz\n",
    "oz_text = open(\"the_wizard_of_oz_text.txt\",encoding='utf-8').read().lower()\n",
    "# tokenization\n",
    "sentences_tokenized = sent_tokenize(oz_text) # sentence tokenization\n",
    "# list bellow will contain all the token from the different sentences\n",
    "tokenized_words_text = [] # an empty list\n",
    "# for loop will token the words of every sentence separately\n",
    "for sentence in sentences_tokenized:\n",
    "    # tokenization of every sentence (so all words inside it)\n",
    "    word_sentence_toke = word_tokenize(sentence)\n",
    "    # POS tagging for every token (word) from a sentence \n",
    "    words_toke_for_all_sentences = pos_tag(word_sentence_toke)\n",
    "    # POS tagging for every token added to list all\n",
    "    tokenized_words_text.append(words_toke_for_all_sentences) # tokenized words + POS \n",
    "# --------------------------------\n",
    "\n",
    "\n",
    "# code starts here\n",
    "from nltk import RegexpParser, Tree\n",
    "\n",
    "# part where : chunk an entire sentence together\n",
    "print(\"- chunk an entire sentence together: \\n\")\n",
    "# define chunk grammar to chunk an entire sentence together\n",
    "grammar = \"Chunk: {<.*>+}\" # # <.*>+ matches every POS in the sentence\n",
    "\n",
    "# create RegexpParser object\n",
    "parser = RegexpParser(grammar)\n",
    "\n",
    "# chunk the pos-tagged sentence at index 230 in \"tokenized_words_text\"\n",
    "chunked_dancers = parser.parse(tokenized_words_text[230])\n",
    "print(chunked_dancers)\n",
    "\n",
    "\n",
    "# ----------------\n",
    "# # part where : chunk a noun phrase using chunk filtering\n",
    "print(\"\\n- chunk a noun phrase using chunk filtering: \\n\")\n",
    "# define noun phrase chunk grammar using chunk filtering here\n",
    "chunk_grammar = \"\"\"NP: {<.*>+} \n",
    "                       }<VB.?|IN>+{\"\"\" \n",
    "# <.*>+ matches every POS in the sentence\n",
    "# }<VB.?|IN>+{ filter out (}{) any verbs or prepositions (<VB.?|IN>+)\n",
    "\n",
    "# create RegexpParser object \n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    "\n",
    "# chunk and filter the pos-tagged sentence at index 230 in \"tokenized_words_text\" \n",
    "filtered_dancers = chunk_parser.parse(tokenized_words_text[230])\n",
    "print(filtered_dancers)\n",
    "\n",
    "# pretty_print the chunked and filtered sentence here\n",
    "print(\"\\nIn form of a tree : \\n\")\n",
    "Tree.fromstring(str(filtered_dancers)).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REVIEW : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform some cool natural language parsing with regular expressions\n",
    "\n",
    "\n",
    "* The re module’s **.compile( )** and **.match( )** methods allow you to enter any regex pattern and look for a single match at the beginning of a piece of text\n",
    "\n",
    "\n",
    "* The re module’s **.search( )** method lets you find a single match to a regex pattern anywhere in a string, while the **.findall( )** method finds all the matches of a regex pattern in a string\n",
    "\n",
    "\n",
    "* Part-of-speech tagging identifies and labels the part of speech of words in a sentence, and can be performed in nltk using the **pos_tag( )** function\n",
    "\n",
    "\n",
    "* Chunking groups together patterns of words by their part-of-speech tag. Chunking can be performed in nltk by defining a piece of chunk grammar using regular expression syntax and calling a **RegexpParser**‘s **.parse( )** method on a word tokenized sentence\n",
    "\n",
    "\n",
    "* NP-chunking chunks together an optional determiner DT, any number of adjectives JJ, and a noun NN to form a noun phrase. The frequency of different NP-chunks can identify important topics in a text or demonstrate how an author describes different subjects\n",
    "\n",
    "\n",
    "* VP-chunking chunks together a verb VB, a noun phrase, and an optional adverb RB to form a verb phrase. The frequency of different VP-chunks can give insight into what kind of action different subjects take or how the actions that different subjects take are described by an author, potentially indicating bias\n",
    "\n",
    "\n",
    "* Chunk filtering provides an alternative means of chunking by specifying what parts of speech you do not want in a chunk and removing them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
