{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing is a stage of NLP concerned with **breaking / segmenting** up text **based on syntax**\n",
    "\n",
    "By using Python’s _regular expression_ modulere and the _Natural Language Toolkit_, known as _NLTK_, you can find:    \n",
    "* keywords of interest\n",
    "* discover where and how often they are used \n",
    "* discern the parts-of-speech patterns in which they appear to understand the sometimes hidden meaning in a piece of writing\n",
    "\n",
    "> ex:  highlight the biases of its author or uncover additional insights that even a deep, rigorous reading of the text might not reveal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Compiling and Matching\n",
    "\n",
    "* **.compile( )** :  \n",
    "This method takes a regular expression pattern as an argument and compiles the pattern into a regular expression object, which you can later use to find matching text\n",
    "\n",
    "* **.match( )** :   \n",
    "method that takes a string of text as an argument and looks for a single match to the regular expression that starts at the beginning of the string\n",
    "    * finds a match that starts at the beginning of the string, it will return a match object\n",
    "    * If there is no match, .match() will return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regular_expression:  re.compile('[A-Za-z]{7}')\n",
      "result_1:  <_sre.SRE_Match object; span=(0, 7), match='Dorothy'>\n",
      "match_1:  Dorothy\n",
      "result_2:  None\n",
      "result_2:  None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# characters are defined\n",
    "character_1 = \"Dorothy\"\n",
    "character_2 = \"Henry\"\n",
    "\n",
    "# regular expression object that match any 7 character string later usable to find matching text\n",
    "regular_expression = re.compile(\"[A-Za-z]{7}\") #  recognizes upper or lower case characters\n",
    "print(\"regular_expression: \", regular_expression)\n",
    "\n",
    "# check for a match to character_1 here\n",
    "# (check if regex matches string stored in character_1)\n",
    "result_1 = regular_expression.match(character_1)\n",
    "print(\"result_1: \", result_1)\n",
    "\n",
    "\n",
    "# store and print the matched text here \n",
    "match_1 = result_1.group(0)\n",
    "print(\"match_1: \", match_1)\n",
    "\n",
    "# compile a regular expression to match a 7 character string of word characters\n",
    "# and check for a match to character_2 here\n",
    "regular_expression = re.compile(\"[A-Za-z]{7}\") # search for something len = 7\n",
    "result_2 = regular_expression.match(character_2) # but is len = 5\n",
    "print(\"result_2: \", result_2) # so nothing is found => None\n",
    "# 2 last lines above can be written in one line also\n",
    "result_2 = re.match(\"[A-Za-z]{7}\", character_2)\n",
    "print(\"result_2: \", result_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Searching and Finding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **.search( )** :   \n",
    "Will look left to right through an entire piece of text and return a match object for the first match to the regular expression given.   \n",
    "If no match is found, _.search()_ will return None (unlike **.match( )** which will only find matches at the start of a string).\n",
    ">ex: **result = re.search(\"\\w{8}\",\"Are you a Munchkin?\")** search for a sequence of 8 word characters in the string \"Are you a Munchkin?\" (that will be \"Munchkin\")\n",
    "\n",
    "* **.findall( )** :  \n",
    "Finds all the occurrences of a word or keyword in a piece of text.  \n",
    "Will return a list of all non-overlapping matches of the regular expression in the string   \n",
    "(can be useful to find all the occurrences of a word or keyword in a piece of text to determine a frequency count).\n",
    ">ex:   \n",
    ". text  = *\"Everything is green here, while in the country of the Munchkins blue was the favorite color. But the people do not seem to be as friendly as the Munchkins, and I'm afraid we shall be unable to find a place to pass the night.\"*   \n",
    ". list_of_matches = re.findall(\"\\w{8}\",text)  \n",
    ". returns the list:  *['Everythi', 'Munchkin', 'favorite', 'friendly', 'Munchkin']*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found_wizard:  <_sre.SRE_Match object; span=(14, 20), match='wizard'>\n",
      "\n",
      " all_lions_list:  ['lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion']\n",
      "\n",
      " number_lions:  183\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# import L. Frank Baum's The Wonderful Wizard of Oz\n",
    "oz_text = open(\"the_wizard_of_oz_text.txt\",encoding='utf-8').read().lower()\n",
    "\n",
    "# search oz_text for an occurrence of 'wizard' here\n",
    "found_wizard = re.search(\"wizard\",oz_text)\n",
    "print(\"found_wizard: \", found_wizard)\n",
    "\n",
    "# find all the occurrences of 'lion' in oz_text here\n",
    "all_lions = re.findall(\"lion\", oz_text)\n",
    "print(\"\\n all_lions_list: \", all_lions)\n",
    "\n",
    "# store and print the length of all_lions here\n",
    "number_lions =  len(all_lions)\n",
    "print(\"\\n number_lions: \", number_lions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">ex:  \n",
    "\"Wow! Ramona and her class are happily studying the new textbook she has on NLP.\"\n",
    "\n",
    "\n",
    "* **Noun**: the name of a person (Ramona,class), place, thing (textbook), or idea (NLP) = **NN** \n",
    "\n",
    "\n",
    "* **Pronoun**: a word used in place of a noun (her,she) = **PRP** \n",
    "\n",
    "\n",
    "* **Determiner**: a word that introduces, or “determines”, a noun (the) = **DT** \n",
    "\n",
    "\n",
    "* **Verb**: expresses action (studying) or being (are,has) = **VBP** \n",
    "\n",
    "\n",
    "* **Adjective**: modifies or describes a noun or pronoun (new) = **JJ** \n",
    "\n",
    "\n",
    "* **Adverb**: modifies or describes a verb, an adjective, or another adverb (happily) = **RB** \n",
    "\n",
    "\n",
    "* **Preposition**: a word placed before a noun or pronoun to form a phrase modifying another word in the sentence (on) = **IN** \n",
    "\n",
    "\n",
    "* **Conjunction**: a word that joins words, phrases, or clauses (and) = **IN** \n",
    "\n",
    " \n",
    "* **Interjection**: a word used to express emotion (Wow) = **UH** \n",
    "\n",
    "\n",
    "* **pos_tag()** function:   \n",
    "Automates the part-of-speech tagging process.  \n",
    "The function takes one argument, a list of words in the order they appear in a sentence, and returns a list of tuples, where the first entry in the tuple is a word and the second is the part-of-speech tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100th tokenized sentence: \"the house must have fallen on her.\n",
      "\n",
      "sentence number  18 :  [('the', 'DT'), ('sun', 'NN'), ('and', 'CC'), ('wind', 'NN'), ('had', 'VBD'), ('changed', 'VBN'), ('her', 'PRP'), (',', ','), ('too', 'RB'), ('.', '.')]\n",
      "\n",
      "sentence number  19 :  [('they', 'PRP'), ('had', 'VBD'), ('taken', 'VBN'), ('the', 'DT'), ('sparkle', 'NN'), ('from', 'IN'), ('her', 'PRP$'), ('eyes', 'NNS'), ('and', 'CC'), ('left', 'VBD'), ('them', 'PRP'), ('a', 'DT'), ('sober', 'JJ'), ('gray', 'NN'), (';', ':'), ('they', 'PRP'), ('had', 'VBD'), ('taken', 'VBN'), ('the', 'DT'), ('red', 'JJ'), ('from', 'IN'), ('her', 'PRP$'), ('cheeks', 'NN'), ('and', 'CC'), ('lips', 'NNS'), (',', ','), ('and', 'CC'), ('they', 'PRP'), ('were', 'VBD'), ('gray', 'JJ'), ('also', 'RB'), ('.', '.')]\n",
      "\n",
      "Words_toke_100th_sentence:  [('``', '``'), ('the', 'DT'), ('house', 'NN'), ('must', 'MD'), ('have', 'VB'), ('fallen', 'VBN'), ('on', 'IN'), ('her', 'PRP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag # automates the part-of-speech tagging process\n",
    "from nltk.tokenize import word_tokenize # word tokenization\n",
    "from nltk.tokenize import sent_tokenize # sentence tokenization\n",
    "# import L. Frank Baum's The Wonderful Wizard of Oz\n",
    "oz_text = open(\"the_wizard_of_oz_text.txt\",encoding='utf-8').read().lower()\n",
    "\n",
    "# tokenization\n",
    "sentences_tokenized = sent_tokenize(oz_text) # sentence tokenization\n",
    "\n",
    "#100th sentence from the tokenized text above\n",
    "sentence_tokenized_100th = sentences_tokenized[100]\n",
    "print(\"100th tokenized sentence:\", sentence_tokenized_100th)\n",
    "\n",
    "# list bellow will contain all the token from the different sentences\n",
    "tokenized_words_text = [] # an empty list\n",
    "\n",
    "# for loop will token the words of every sentence separately\n",
    "for sentence in sentences_tokenized:\n",
    "    # tokenization of every sentence (so all words inside it)\n",
    "    word_sentence_toke = word_tokenize(sentence)\n",
    "    # POS tagging for every token (word) from a sentence \n",
    "    words_toke_for_all_sentences = pos_tag(word_sentence_toke)\n",
    "    # POS tagging for every token added to list all\n",
    "    tokenized_words_text.append(words_toke_for_all_sentences) # tokenized words + POS \n",
    "\n",
    "# print 18th to 19th sentence that is tokenized \n",
    "for n in range(18, 20): \n",
    "    print(\"\\nsentence number \", n, \": \", tokenized_words_text[n])\n",
    "\n",
    "words_toke_100th_sentence = tokenized_words_text[100]\n",
    "print(\"\\nWords_toke_100th_sentence: \",words_toke_100th_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Introduction to Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **chunking** = technique of grouping words by their part-of-speech (POS) tag.  \n",
    "\n",
    "Given your part-of-speech tagged text, you can now use regular expressions to find patterns in sentence structure that give insight into the meaning of a text.\n",
    "\n",
    "### _______________\n",
    "\n",
    "The regular expression we build to find chunks is called chunk grammar. A piece of chunk grammar can be written as follows:\n",
    "\n",
    "chunk_grammar = \" AN : {< JJ >< NN >} \"\n",
    "\n",
    "    - AN is a user-defined name for the kind of chunk you are searching for. \n",
    "    We can use whatever name makes sense given our chunk grammar. In this case AN stands for adjective-noun\n",
    "    - A pair of curly braces {} surround the actual chunk grammar\n",
    "    - <JJ> operates similarly to a regex character class, matching any adjective\n",
    "    - <NN> matches any noun, singular or plural\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- chunk_parser looks like: \n",
      " chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<JJ><NN>'>\n",
      "\n",
      "- POS tagged sentence of interesst:  [('``', '``'), ('where', 'WRB'), ('is', 'VBZ'), ('the', 'DT'), ('emerald', 'JJ'), ('city', 'NN'), ('?', '.'), (\"''\", \"''\")]\n",
      "\n",
      "- Chunking-Parsed sentence ('scaredy_cat'):  (S ``/`` where/WRB is/VBZ the/DT (AN emerald/JJ city/NN) ?/. ''/'') \n",
      "\n",
      "- In form of a tree 'scaredy_cat' sentence : \n",
      "\n",
      "                         S                                    \n",
      "   ______________________|__________________________           \n",
      "  |       |       |      |     |    |               AN        \n",
      "  |       |       |      |     |    |        _______|_____     \n",
      "``/`` where/WRB is/VBZ the/DT ?/. ''/'' emerald/JJ     city/NN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------- this part is just to prepare the tokenized sentences ---------------\n",
    "from nltk.tokenize import word_tokenize # word tokenization\n",
    "from nltk.tokenize import sent_tokenize # sentence tokenization\n",
    "from nltk import pos_tag # automates the part-of-speech tagging process\n",
    "# import L. Frank Baum's The Wonderful Wizard of Oz\n",
    "oz_text = open(\"the_wizard_of_oz_text.txt\",encoding='utf-8').read().lower()\n",
    "# tokenization\n",
    "sentences_tokenized = sent_tokenize(oz_text) # sentence tokenization\n",
    "# list bellow will contain all the token from the different sentences\n",
    "tokenized_words_text = [] # an empty list\n",
    "# for loop will token the words of every sentence separately\n",
    "for sentence in sentences_tokenized:\n",
    "    # tokenization of every sentence (so all words inside it)\n",
    "    word_sentence_toke = word_tokenize(sentence)\n",
    "    # POS tagging for every token (word) from a sentence \n",
    "    words_toke_for_all_sentences = pos_tag(word_sentence_toke)\n",
    "    # POS tagging for every token added to list all\n",
    "    tokenized_words_text.append(words_toke_for_all_sentences) # tokenized words + POS \n",
    "# --------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# the code starts here bellow\n",
    "\n",
    "from nltk import RegexpParser, Tree\n",
    "\n",
    "# -- define adjective-noun chunk grammar -- \n",
    "# - AN is a user-defined name for the kind of chunk you are searching for. \n",
    "#   You can use whatever name makes sense given your chunk grammar. \n",
    "#   In this case AN stands for adjective-noun\n",
    "# - <JJ> matches any adjective (operates similarly to a regex character class, matching any adjective)\n",
    "# - <NN> matches any noun, singular or plural\n",
    "# - {} surrounds the chunk grammar\n",
    "chunk_grammar = \"AN: {<JJ><NN>}\"  # will match any adjective JJ that is followed by a noun NN\n",
    "\n",
    "# -- create \"RegexpParser\" object --\n",
    "# - To use the chunk grammar defined, we must create a nltk \"RegexpParser\" object and \n",
    "#   give it a piece of chunk grammar (\"chunk_grammar\") as an argument\n",
    "chunk_parser = RegexpParser(chunk_grammar) # chunk_grammar = argument\n",
    "print(\"- chunk_parser looks like: \\n\", chunk_parser)\n",
    "\n",
    "# chunk the POS tagged sentence at index 282 in \"tokenized_words_text\"\n",
    "print(\"\\n- POS tagged sentence of interesst: \", tokenized_words_text[282])\n",
    "scaredy_cat = chunk_parser.parse(tokenized_words_text[282])\n",
    "print(\"\\n- Chunking-Parsed sentence ('scaredy_cat'): \", scaredy_cat, \"\\n\")\n",
    "\n",
    "# pretty_print the chunked sentence\n",
    "print(\"- In form of a tree 'scaredy_cat' sentence : \\n\")\n",
    "Tree.fromstring(str(scaredy_cat)).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - 2) Chunking Noun Phrases\n",
    "\n",
    "\n",
    "**Types of chunking** that are linguistically helpful for determining *meaning* and *bias* in a piece of text.   \n",
    "A **noun phrase** is a phrase that contains _a noun_ and operates, as a unit, as a noun.\n",
    "\n",
    "\n",
    "\n",
    "* A popular form of noun phrase begins with a determiner **DT** (which specifies the noun being referenced),   followed by any number of adjectives **JJ** (which describe the noun) and ends with a noun **NN**.\n",
    "\n",
    "> ex : The POS-ed sentence :  \n",
    "[('we', 'PRP'), ('are', 'VBP'), ('so', 'RB'), ('grateful', 'JJ'), ('to', 'TO'), ('you', 'PRP'), ('for', 'IN'), ('having', 'VBG'), ('killed', 'VBN'), ('the', '**DT**'), ('wicked', '**JJ**'), ('witch', '**NN**'), ('of', 'IN'), ('the', '**DT**'), ('east', '**NN**'), (',', ','), ('and', 'CC'), ('for', 'IN'), ('setting', 'VBG'), ('our', 'PRP'), ('people', 'NNS'), ('free', 'VBP'), ('from', 'IN'), ('bondage', 'NN'), ('.', '.')]  \n",
    "Has 2 Chunking Noun Phrases:\n",
    " - (('the', 'DT'), ('wicked', 'JJ'), ('witch', 'NN'))\n",
    " - (('the', 'DT'), ('east', 'NN'))\n",
    "\n",
    "### ______\n",
    "\n",
    "\n",
    "We can easily find all the non-overlapping noun phrases in a piece of text.  \n",
    "Just like in normal regular expressions, we can use quantifiers to indicate how many of each POS we want to match.\n",
    "\n",
    "The chunk grammar for a noun phrase (**NP-chunks**) can be written as follows :\n",
    "\n",
    "chunk_grammar = \" NP : {< DT > ? < JJ > * < NN >} \"\n",
    "\n",
    "    - NP is the user-defined name of the chunk we are searching for. In this case NP stands for noun phrase\n",
    "    - <DT> matches any determiner\n",
    "    - ? is an optional quantifier, matching either 0 or 1 determiners\n",
    "    - <JJ> matches any adjective\n",
    "    - * is the Kleene star quantifier, matching 0 or more occurrences of an adjective\n",
    "    - <NN> matches any noun, singular or plural\n",
    "    \n",
    "\n",
    "**->** Via NP-chunks in a text, we can perform :\n",
    "\n",
    "* a frequency analysis and identify important, recurring noun phrases\n",
    "* identify pseudo-topics and tag articles and documents by their highest count NP-chunks\n",
    "* analyzing the adjective choices an author makes for different nouns\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here below we can see the distribution of words but also for small NP-Chunks sentences: \n",
      "\n",
      "[((('i', 'NN'),), 326), ((('dorothy', 'NN'),), 222), ((('the', 'DT'), ('scarecrow', 'NN')), 213), ((('the', 'DT'), ('lion', 'NN')), 148), ((('the', 'DT'), ('tin', 'NN')), 123), ((('woodman', 'NN'),), 112), ((('oz', 'NN'),), 86), ((('toto', 'NN'),), 73), ((('head', 'NN'),), 59), ((('the', 'DT'), ('woodman', 'NN')), 59), ((('the', 'DT'), ('wicked', 'JJ'), ('witch', 'NN')), 58), ((('the', 'DT'), ('emerald', 'JJ'), ('city', 'NN')), 51), ((('the', 'DT'), ('witch', 'NN')), 49), ((('the', 'DT'), ('girl', 'NN')), 46), ((('the', 'DT'), ('road', 'NN')), 41), ((('room', 'NN'),), 29), ((('nothing', 'NN'),), 29), ((('the', 'DT'), ('air', 'NN')), 29), ((('the', 'DT'), ('country', 'NN')), 26), ((('the', 'DT'), ('land', 'NN')), 24), ((('a', 'DT'), ('heart', 'NN')), 24), ((('the', 'DT'), ('west', 'NN')), 23), ((('axe', 'NN'),), 23), ((('the', 'DT'), ('sun', 'NN')), 22), ((('the', 'DT'), ('little', 'JJ'), ('girl', 'NN')), 22), ((('course', 'NN'),), 22), ((('the', 'DT'), ('cowardly', 'JJ'), ('lion', 'NN')), 21), ((('aunt', 'NN'),), 21), ((('the', 'DT'), ('house', 'NN')), 21), ((('the', 'DT'), ('door', 'NN')), 21)]\n"
     ]
    }
   ],
   "source": [
    "# -------------- this part is just to prepare the tokenized sentences ---------------\n",
    "from nltk.tokenize import word_tokenize # word tokenization\n",
    "from nltk.tokenize import sent_tokenize # sentence tokenization\n",
    "from nltk import pos_tag # automates the part-of-speech tagging process\n",
    "# import L. Frank Baum's The Wonderful Wizard of Oz\n",
    "oz_text = open(\"the_wizard_of_oz_text.txt\",encoding='utf-8').read().lower()\n",
    "# tokenization\n",
    "sentences_tokenized = sent_tokenize(oz_text) # sentence tokenization\n",
    "# list bellow will contain all the token from the different sentences\n",
    "tokenized_words_text = [] # an empty list\n",
    "# for loop will token the words of every sentence separately\n",
    "for sentence in sentences_tokenized:\n",
    "    # tokenization of every sentence (so all words inside it)\n",
    "    word_sentence_toke = word_tokenize(sentence)\n",
    "    # POS tagging for every token (word) from a sentence \n",
    "    words_toke_for_all_sentences = pos_tag(word_sentence_toke)\n",
    "    # POS tagging for every token added to list all\n",
    "    tokenized_words_text.append(words_toke_for_all_sentences) # tokenized words + POS \n",
    "\n",
    "# the code starts here\n",
    "from nltk import RegexpParser\n",
    "from np_chunk_counter import np_chunk_counter\n",
    "\n",
    "# define noun-phrase chunk grammar \n",
    "chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "# create RegexpParser object \n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    "\n",
    "# create a list to hold noun-phrase chunked sentences\n",
    "np_chunked_oz = list()\n",
    "\n",
    "# create a for loop through each pos-tagged sentence in pos_tagged_oz here\n",
    "for pos_tagged_sentence in tokenized_words_text:\n",
    "  # chunk each sentence and append to np_chunked_oz here\n",
    "  np_chunked_oz.append(chunk_parser.parse(pos_tagged_sentence))\n",
    "\n",
    "# store and print the 30 most common np-chunks from a list of chunked sentences\n",
    "most_common_np_chunks = np_chunk_counter(np_chunked_oz)\n",
    "print(\" Here below we can see the distribution of words but also for small NP-Chunks sentences: \\n\")\n",
    "print(most_common_np_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
